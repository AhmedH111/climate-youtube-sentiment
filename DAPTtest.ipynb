{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5ad36-535d-4bcc-8bf5-be5fcc60d0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dapt Implementation\n",
    "import os, re, math, json, time, random, warnings\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    get_cosine_schedule_with_warmup\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"once\", category=UserWarning)\n",
    "\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    _HAS_TQDM = True\n",
    "except Exception:\n",
    "    _HAS_TQDM = False\n",
    "    def tqdm(*args, **kwargs):\n",
    "        class _Dummy:\n",
    "            def update(self, *a, **k): pass\n",
    "            def set_postfix(self, *a, **k): pass\n",
    "            def set_postfix_str(self, *a, **k): pass\n",
    "            def close(self): pass\n",
    "        return _Dummy()\n",
    "\n",
    "\n",
    "PARTS_DIR = \".\"\n",
    "PART_BASENAMES = [\"PartOne\", \"PartTwo\", \"PartThree\", \"PartFour\"]\n",
    "EXT_ORDER = [\".xlsx\", \".csv\", \".tsv\", \".xls\"]\n",
    "\n",
    "def resolve_parts(parts_dir: str, basenames: List[str]) -> List[str]:\n",
    "    paths = []\n",
    "    for base in basenames:\n",
    "        base_path = Path(parts_dir) / base\n",
    "        if base_path.suffix:\n",
    "            if base_path.exists():\n",
    "                paths.append(str(base_path))\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"[DAPT] File not found: {base_path}\")\n",
    "        else:\n",
    "            found = None\n",
    "            for ext in EXT_ORDER:\n",
    "                cand = base_path.with_suffix(ext)\n",
    "                if cand.exists():\n",
    "                    found = cand; break\n",
    "            if not found:\n",
    "                tried = \", \".join(str(base_path.with_suffix(ext)) for ext in EXT_ORDER)\n",
    "                raise FileNotFoundError(f\"[DAPT] Could not find a file for '{base}'. Tried: {tried}\")\n",
    "            paths.append(str(found))\n",
    "    return paths\n",
    "\n",
    "PART_PATHS: List[str] = resolve_parts(PARTS_DIR, PART_BASENAMES)\n",
    "print(f\"[DAPT] Using parts: {PART_PATHS}\")\n",
    "\n",
    "\n",
    "TASKB_LEAK_GUARD = \"taskB_youtube_raw.cleaned.csv\"\n",
    "\n",
    "# Training Budget\n",
    "TOTAL_STEPS       = 120_000\n",
    "VAL_EVERY_STEPS   = 5_000\n",
    "CKPT_EVERY_STEPS  = 20_000\n",
    "GRAD_ACCUM_STEPS  = 2\n",
    "\n",
    "\n",
    "MAX_LEN     = 160  \n",
    "BATCH_SIZE  = 16\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY  = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "WARMUP_RATIO = 0.06\n",
    "WEIGHT_DECAY = 0.01\n",
    "LR           = 1e-4  \n",
    "\n",
    "\n",
    "USE_LORA        = True\n",
    "MERGE_AND_SAVE  = True\n",
    "LORA_R          = 8\n",
    "LORA_ALPHA      = 16\n",
    "LORA_DROPOUT    = 0.10\n",
    "USE_GRAD_CHKPT  = False  \n",
    "\n",
    "\n",
    "_LORA_BASE_TARGETS  = [\"query\",\"key\",\"value\",\"dense\"]\n",
    "_LORA_EXTRA_TARGETS = [\"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\",\"intermediate.dense\",\"output.dense\"]\n",
    "LORA_TARGETS        = list(dict.fromkeys(_LORA_BASE_TARGETS + _LORA_EXTRA_TARGETS))\n",
    "\n",
    "\n",
    "VAL_FRAC, VAL_MIN, VAL_MAX = 0.01, 2_000, 20_000\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "SAVE_ADAPTER_DIR = \"dapt_lora_adapter\"\n",
    "SAVE_MERGED_DIR  = \"dapt_merged_backbone\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"[DAPT] device={DEVICE}, steps={TOTAL_STEPS}, batch={BATCH_SIZE}, grad_accum={GRAD_ACCUM_STEPS}\")\n",
    "\n",
    "def set_all_seeds(seed: int = 42):\n",
    "    import numpy as _np\n",
    "    random.seed(seed); _np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "URL_RE  = re.compile(r'https?://\\S+|www\\.\\S+', re.IGNORECASE)\n",
    "MENT_RE = re.compile(r'@\\w+')\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = str(s)\n",
    "    s = (s.replace(\"â€™\",\"’\").replace(\"â€œ\",\"“\").replace(\"â€\\x9d\",\"”\").replace(\"â€“\",\"–\"))\n",
    "    s = URL_RE.sub(\"<url>\", s); s = MENT_RE.sub(\"@user\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "TEXT_COLNAME_PREFS = [\"CommentText\",\"comment_text\",\"text\",\"raw_text\",\"Comment\",\"body\",\"content\"]\n",
    "\n",
    "def guess_text_column(df: pd.DataFrame) -> str:\n",
    "    for c in TEXT_COLNAME_PREFS:\n",
    "        if c in df.columns: return c\n",
    "    obj_cols = [c for c in df.columns if df[c].dtype == \"object\"]\n",
    "    if not obj_cols: raise ValueError(\"No text-like column found.\")\n",
    "    lens = {c: df[c].astype(str).str.len().mean() for c in obj_cols}\n",
    "    return max(lens, key=lens.get)\n",
    "\n",
    "def _read_excel(path: Path) -> pd.DataFrame:\n",
    "    try:    return pd.read_excel(path, engine=\"openpyxl\")\n",
    "    except Exception: return pd.read_excel(path)\n",
    "\n",
    "def load_table_texts(path: str) -> List[str]:\n",
    "    p = Path(path); ext = p.suffix.lower()\n",
    "    if ext in [\".xlsx\",\".xls\"]:\n",
    "        df = _read_excel(p)\n",
    "    elif ext == \".tsv\":\n",
    "        df = pd.read_csv(p, sep=\"\\t\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "    else:\n",
    "        df = pd.read_csv(p, encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "    col = guess_text_column(df)\n",
    "    print(f\"[DAPT] using text column '{col}' from {p.name} (columns={list(df.columns)})\")\n",
    "    series = df[col].dropna().astype(str)\n",
    "    return [normalize_text(x) for x in series if x.strip()]\n",
    "\n",
    "def as_mlm_checkpoint(backbone_name: str) -> str:\n",
    "    lower = backbone_name.lower()\n",
    "    if \"twitter-roberta-base-sentiment\" in lower: return \"cardiffnlp/twitter-roberta-base\"\n",
    "    if \"sentiment\" in lower and \"cardiffnlp\" in lower: return \"cardiffnlp/twitter-roberta-base\"\n",
    "    return backbone_name\n",
    "\n",
    "\n",
    "set_all_seeds(SEED)\n",
    "\n",
    "all_texts: List[str] = []\n",
    "for p in PART_PATHS:\n",
    "    all_texts.extend(load_table_texts(p))\n",
    "print(f\"[DAPT] loaded {len(all_texts):,} lines from {len(PART_PATHS)} part(s)\")\n",
    "\n",
    "if TASKB_LEAK_GUARD and Path(TASKB_LEAK_GUARD).exists():\n",
    "    tb = load_table_texts(TASKB_LEAK_GUARD)\n",
    "    leak_set = set(tb)\n",
    "    before = len(all_texts)\n",
    "    all_texts = [t for t in all_texts if t not in leak_set]\n",
    "    print(f\"[DAPT] leak-guard removed {before - len(all_texts):,} lines that matched TaskB\")\n",
    "else:\n",
    "    print(\"[DAPT] leak-guard skipped (TaskB file not found)\")\n",
    "\n",
    "random.shuffle(all_texts)\n",
    "n_total = len(all_texts)\n",
    "n_val   = min(max(int(n_total*VAL_FRAC), VAL_MIN), VAL_MAX)\n",
    "val_texts = all_texts[:n_val]; trn_texts = all_texts[n_val:]\n",
    "print(f\"[DAPT] split -> train={len(trn_texts):,}, val={len(val_texts):,}\")\n",
    "\n",
    "\n",
    "BACKBONE_NAME = \"cardiffnlp/twitter-roberta-base\"  \n",
    "TOKENIZER = AutoTokenizer.from_pretrained(BACKBONE_NAME, use_fast=True)\n",
    "\n",
    "added_tokens = 0\n",
    "for tok in [\"<url>\",\"@user\"]:\n",
    "    if TOKENIZER.convert_tokens_to_ids(tok) == TOKENIZER.unk_token_id:\n",
    "        TOKENIZER.add_tokens([tok]); added_tokens += 1\n",
    "if added_tokens: print(f\"[DAPT] tokenizer extended by {added_tokens} tokens\")\n",
    "\n",
    "mlm_name = as_mlm_checkpoint(BACKBONE_NAME)\n",
    "mlm = AutoModelForMaskedLM.from_pretrained(mlm_name)   \n",
    "if added_tokens: mlm.resize_token_embeddings(len(TOKENIZER))\n",
    "if USE_GRAD_CHKPT and hasattr(mlm, \"gradient_checkpointing_enable\"):\n",
    "    mlm.gradient_checkpointing_enable()\n",
    "\n",
    "\n",
    "peft_available = True\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model\n",
    "    try:\n",
    "        from peft import TaskType\n",
    "        _TASKTYPE = (\n",
    "            getattr(TaskType, \"MASKED_LM\", None)\n",
    "            or getattr(TaskType, \"TOKEN_CLS\", None)\n",
    "            or getattr(TaskType, \"FEATURE_EXTRACTION\", None)\n",
    "            or getattr(TaskType, \"SEQ_CLS\", None)\n",
    "            or getattr(TaskType, \"CAUSAL_LM\", None)\n",
    "        )\n",
    "    except Exception:\n",
    "        TaskType = None\n",
    "        _TASKTYPE = None\n",
    "except Exception as e:\n",
    "    peft_available = False\n",
    "    print(\"[DAPT] PEFT not available; falling back to full-model. Err:\", e)\n",
    "\n",
    "if USE_LORA and peft_available:\n",
    "    lora_kwargs = dict(\n",
    "        r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=LORA_TARGETS, bias=\"none\",\n",
    "    )\n",
    "    if _TASKTYPE is not None:\n",
    "        lora_kwargs[\"task_type\"] = _TASKTYPE\n",
    "    lcfg = LoraConfig(**lora_kwargs)\n",
    "    mlm = get_peft_model(mlm, lcfg)\n",
    "    if USE_GRAD_CHKPT and hasattr(mlm, \"gradient_checkpointing_enable\"):\n",
    "        mlm.gradient_checkpointing_enable()\n",
    "    print(\"[DAPT] LoRA attached for MLM:\", LORA_TARGETS)\n",
    "else:\n",
    "    for p in mlm.parameters(): p.requires_grad = True\n",
    "    if LR > 1e-5:\n",
    "        print(f\"[DAPT] Lowering LR for full-model DAPT from {LR:g} -> 5e-6\")\n",
    "        LR = 5e-6\n",
    "\n",
    "mlm.to(DEVICE)\n",
    "\n",
    "\n",
    "def count_params(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    train = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, train\n",
    "_tot, _tr = count_params(mlm)\n",
    "print(f\"[DAPT] params total={_tot/1e6:.1f}M | trainable={_tr/1e6:.2f}M\")\n",
    "\n",
    "\n",
    "class LineDataset(Dataset):\n",
    "    def __init__(self, lines: List[str]): self.lines = lines\n",
    "    def __len__(self): return len(self.lines)\n",
    "    def __getitem__(self, idx): return self.lines[idx]\n",
    "\n",
    "train_ds = LineDataset(trn_texts); val_ds = LineDataset(val_texts)\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=TOKENIZER, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "def collate_fn(batch_lines: List[str]):\n",
    "    enc = TOKENIZER(batch_lines, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    features = [{\"input_ids\": ids} for ids in enc[\"input_ids\"]]\n",
    "    masked = collator(features)\n",
    "    masked[\"attention_mask\"] = enc[\"attention_mask\"]\n",
    "    return masked\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE*2, shuffle=False,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "\n",
    "\n",
    "from torch.optim import AdamW\n",
    "optim = AdamW(mlm.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "warmup_steps = max(10, int(TOTAL_STEPS * WARMUP_RATIO))\n",
    "sched = get_cosine_schedule_with_warmup(optim, num_warmup_steps=warmup_steps, num_training_steps=TOTAL_STEPS)\n",
    "scaler = torch.amp.GradScaler(enabled=True)\n",
    "\n",
    "os.makedirs(SAVE_ADAPTER_DIR, exist_ok=True); os.makedirs(SAVE_MERGED_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\"); global_step = 0; last_log_t = time.time()\n",
    "\n",
    "def eval_mlm(model) -> float:\n",
    "    model.eval(); total = 0.0; count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            ids = batch[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
    "            msk = batch[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "            lab = batch[\"labels\"].to(DEVICE, non_blocking=True)\n",
    "            loss = model(input_ids=ids, attention_mask=msk, labels=lab).loss\n",
    "            total += loss.item() * ids.size(0); count += ids.size(0)\n",
    "    model.train(); return total / max(1, count)\n",
    "\n",
    "\n",
    "print(f\"[DAPT] Config: steps={TOTAL_STEPS}, val_every={VAL_EVERY_STEPS}, ckpt_every={CKPT_EVERY_STEPS}, \"\n",
    "      f\"batch={BATCH_SIZE}×accum{GRAD_ACCUM_STEPS}, max_len={MAX_LEN}, lr={LR:g}, warmup={WARMUP_RATIO}, \"\n",
    "      f\"lora={USE_LORA}, r={LORA_R}, alpha={LORA_ALPHA}, dropout={LORA_DROPOUT}, grad_ckpt={USE_GRAD_CHKPT}\")\n",
    "\n",
    "mlm.train(); torch.cuda.empty_cache()\n",
    "print(f\"[DAPT] START | TOTAL_STEPS={TOTAL_STEPS:,} | warmup_steps={warmup_steps:,} | LR={LR:g}\")\n",
    "\n",
    "bar = tqdm(total=TOTAL_STEPS, desc=\"[DAPT] steps\", unit=\"step\") if _HAS_TQDM else None\n",
    "accum = 0; done = False\n",
    "while not done:\n",
    "    for batch in train_loader:\n",
    "        ids = batch[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
    "        msk = batch[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "        lab = batch[\"labels\"].to(DEVICE, non_blocking=True)\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\", enabled=torch.cuda.is_available()):\n",
    "            out = mlm(input_ids=ids, attention_mask=msk, labels=lab)\n",
    "            loss = out.loss\n",
    "\n",
    "        scaler.scale(loss / GRAD_ACCUM_STEPS).backward(); accum += 1\n",
    "        if accum % GRAD_ACCUM_STEPS == 0:\n",
    "            scaler.unscale_(optim)\n",
    "            torch.nn.utils.clip_grad_norm_(mlm.parameters(), 1.0)\n",
    "            scaler.step(optim); scaler.update()\n",
    "            optim.zero_grad(set_to_none=True); sched.step()\n",
    "            global_step += 1\n",
    "\n",
    "            if bar is not None:\n",
    "                bar.update(1)\n",
    "                if global_step % 50 == 0:\n",
    "                    bar.set_postfix(loss=f\"{loss.item():.4f}\", lr=f\"{sched.get_last_lr()[0]:.2e}\")\n",
    "            elif global_step % 100 == 0:\n",
    "                dt = time.time() - last_log_t\n",
    "                print(f\"[DAPT] step {global_step:>6}/{TOTAL_STEPS} | loss={loss.item():.4f} | {dt:.1f}s/100 steps\")\n",
    "                last_log_t = time.time()\n",
    "\n",
    "            if global_step % VAL_EVERY_STEPS == 0 or global_step == TOTAL_STEPS:\n",
    "                val_loss = eval_mlm(mlm)\n",
    "                ppl = math.exp(min(20.0, val_loss))\n",
    "                improved = val_loss < best_val_loss - 1e-4\n",
    "                if improved: best_val_loss = val_loss\n",
    "                msg = f\"[DAPT]   VAL @ {global_step}: loss={val_loss:.4f} | ppl≈{ppl:.2f} | {'*BEST*' if improved else ''}\"\n",
    "                print(msg); \n",
    "                if bar is not None: bar.set_postfix_str(msg.replace(\"[DAPT]   \", \"\"))\n",
    "\n",
    "            if global_step % CKPT_EVERY_STEPS == 0 and USE_LORA and peft_available:\n",
    "                try:\n",
    "                    mlm.save_pretrained(SAVE_ADAPTER_DIR); TOKENIZER.save_pretrained(SAVE_ADAPTER_DIR)\n",
    "                    print(f\"[DAPT]   adapter snapshot saved -> {SAVE_ADAPTER_DIR}\")\n",
    "                except Exception as e:\n",
    "                    print(\"[DAPT]   snapshot save failed:\", repr(e))\n",
    "\n",
    "            if global_step >= TOTAL_STEPS:\n",
    "                done = True; break\n",
    "\n",
    "if bar is not None: bar.close()\n",
    "print(f\"[DAPT] DONE at step {global_step} | best_val_loss={best_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "if USE_LORA and peft_available:\n",
    "    try:\n",
    "        mlm.save_pretrained(SAVE_ADAPTER_DIR); TOKENIZER.save_pretrained(SAVE_ADAPTER_DIR)\n",
    "        print(f\"[DAPT] adapter saved -> {SAVE_ADAPTER_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(\"[DAPT] adapter save failed:\", repr(e))\n",
    "\n",
    "if MERGE_AND_SAVE and USE_LORA and peft_available:\n",
    "    try:\n",
    "        from peft import PeftModel\n",
    "        merged = mlm.merge_and_unload() if isinstance(mlm, PeftModel) else mlm\n",
    "        merged.save_pretrained(SAVE_MERGED_DIR); TOKENIZER.save_pretrained(SAVE_MERGED_DIR)\n",
    "        print(f\"[DAPT] merged backbone saved -> {SAVE_MERGED_DIR}\")\n",
    "    except Exception as e:\n",
    "        print(\"[DAPT] merge-and-save failed:\", repr(e))\n",
    "\n",
    "ADAPTATION_STATUS = {\n",
    "    \"used_lora\": bool(USE_LORA and peft_available),\n",
    "    \"save_adapter_dir\": os.path.abspath(SAVE_ADAPTER_DIR),\n",
    "    \"save_merged_dir\":  os.path.abspath(SAVE_MERGED_DIR),\n",
    "    \"total_steps\": int(TOTAL_STEPS),\n",
    "    \"best_val_loss\": float(best_val_loss)\n",
    "}\n",
    "print(\"[DAPT] Adaptation Status:\", json.dumps(ADAPTATION_STATUS, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a303dcc-d992-4937-a642-642651d1a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, torch, os\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "_DAPT_MERGED  = \"dapt_merged_backbone\"\n",
    "_DAPT_ADAPTER = \"dapt_lora_adapter\"\n",
    "_TAPT_MERGED  = \"tapt_merged_backbone\"\n",
    "_TAPT_ADAPTER = \"tapt_lora_adapter\"\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "    from peft import PeftModel\n",
    "    _PEFT_OK = True\n",
    "except Exception:\n",
    "    _PEFT_OK = False\n",
    "\n",
    "\n",
    "BACKBONE_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"  # base classifier-ish backbone\n",
    "_TOK_SRC      = BACKBONE_NAME\n",
    "_USE_ADAPTER  = False\n",
    "_ADAPTER_DIR  = None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if os.path.isdir(_DAPT_MERGED):\n",
    "    BACKBONE_NAME = _DAPT_MERGED\n",
    "    _TOK_SRC = _DAPT_MERGED\n",
    "    _USE_ADAPTER = False\n",
    "elif os.path.isdir(_DAPT_ADAPTER) and _PEFT_OK:\n",
    "    \n",
    "    BACKBONE_NAME = \"cardiffnlp/twitter-roberta-base\"\n",
    "    _TOK_SRC = _DAPT_ADAPTER\n",
    "    _USE_ADAPTER = True\n",
    "    _ADAPTER_DIR = _DAPT_ADAPTER\n",
    "elif os.path.isdir(_DAPT_ADAPTER) and not _PEFT_OK:\n",
    "    \n",
    "    BACKBONE_NAME = _DAPT_ADAPTER\n",
    "    _TOK_SRC = _DAPT_ADAPTER\n",
    "    _USE_ADAPTER = False\n",
    "elif os.path.isdir(_TAPT_MERGED):\n",
    "    BACKBONE_NAME = _TAPT_MERGED\n",
    "    _TOK_SRC = _TAPT_MERGED\n",
    "    _USE_ADAPTER = False\n",
    "elif os.path.isdir(_TAPT_ADAPTER) and _PEFT_OK:\n",
    "    BACKBONE_NAME = \"cardiffnlp/twitter-roberta-base\"\n",
    "    _TOK_SRC = _TAPT_ADAPTER\n",
    "    _USE_ADAPTER = True\n",
    "    _ADAPTER_DIR = _TAPT_ADAPTER\n",
    "elif os.path.isdir(_TAPT_ADAPTER) and not _PEFT_OK:\n",
    "    BACKBONE_NAME = _TAPT_ADAPTER\n",
    "    _TOK_SRC = _TAPT_ADAPTER\n",
    "    _USE_ADAPTER = False\n",
    "else:\n",
    "    BACKBONE_NAME = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "    _TOK_SRC = BACKBONE_NAME\n",
    "    _USE_ADAPTER = False\n",
    "\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(_TOK_SRC, use_fast=True)\n",
    "\n",
    "print(\"[ADAPT] merged dirs exist?  DAPT:\", os.path.isdir(_DAPT_MERGED), \"| TAPT:\", os.path.isdir(_TAPT_MERGED))\n",
    "print(\"[ADAPT] adapter dirs exist? DAPT:\", os.path.isdir(_DAPT_ADAPTER), \"| TAPT:\", os.path.isdir(_TAPT_ADAPTER))\n",
    "print(\"[ADAPT] PEFT available?:\", _PEFT_OK)\n",
    "print(\"[ADAPT] BACKBONE_NAME:\", BACKBONE_NAME, \"| TOKENIZER src:\", _TOK_SRC, \"| USE_ADAPTER:\", _USE_ADAPTER)\n",
    "\n",
    "\n",
    "def _gn_groups(C: int) -> int:\n",
    "    for g in (8, 4, 2, 1):\n",
    "        if C % g == 0:\n",
    "            return g\n",
    "    return 1\n",
    "\n",
    "def get_act(name: str):\n",
    "    if name == 'relu': return nn.ReLU()\n",
    "    if name == 'silu': return nn.SiLU()\n",
    "    if name == 'mish': return nn.Mish()\n",
    "    return nn.GELU()  \n",
    "\n",
    "def get_norm(name: str, C: int):\n",
    "    if name == 'bn':   return nn.BatchNorm1d(C)\n",
    "    if name == 'gn8':  return nn.GroupNorm(_gn_groups(C), C)\n",
    "    if name == 'ln':   return nn.GroupNorm(1, C)  \n",
    "    return nn.Identity()\n",
    "\n",
    "class SE1d(nn.Module):\n",
    "    def __init__(self, C: int, r: int):\n",
    "        super().__init__()\n",
    "        m = max(1, C // r)\n",
    "        self.fc1 = nn.Linear(C, m)\n",
    "        self.fc2 = nn.Linear(m, C)\n",
    "    def forward(self, x):                 \n",
    "        s = x.mean(dim=2)                 \n",
    "        s = F.silu(self.fc1(s))\n",
    "        s = torch.sigmoid(self.fc2(s)).unsqueeze(-1)\n",
    "        return x * s\n",
    "\n",
    "class ResidualBranch(nn.Module):\n",
    "    def __init__(self, core: nn.Module, skip: nn.Module):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "        self.skip = skip\n",
    "    def forward(self, x):\n",
    "        return self.core(x) + (x if isinstance(self.skip, nn.Identity) else self.skip(x))\n",
    "\n",
    "\n",
    "def _autocast_off():\n",
    "    try:\n",
    "        return torch.amp.autocast(device_type=('cuda' if torch.cuda.is_available() else 'cpu'), enabled=False)\n",
    "    except Exception:\n",
    "        return torch.cuda.amp.autocast(enabled=False)\n",
    "\n",
    "\n",
    "class CNNHead(nn.Module):\n",
    "    def __init__(self,\n",
    "                 hidden: int,\n",
    "                 num_classes: int,\n",
    "                 layers=2,\n",
    "                 filters=256,\n",
    "                 branches=3,\n",
    "                 kernels=(1,5,7),\n",
    "                 dilations=(2,8,8),\n",
    "                 dropout=0.2,\n",
    "                 pooling='attn',\n",
    "                 act='gelu',\n",
    "                 norm='bn',\n",
    "                 sep=False,\n",
    "                 groups=8,\n",
    "                 se_ratio=4,\n",
    "                 residual=True,\n",
    "                 kmax_k=1,\n",
    "                 gem_p=4.0):\n",
    "        super().__init__()\n",
    "        self.cfg = dict(layers=layers, filters=filters, branches=branches, kernels=tuple(kernels),\n",
    "                        dilations=tuple(dilations), dropout=dropout, pooling=pooling, act=act, norm=norm,\n",
    "                        sep=sep, groups=groups, se_ratio=se_ratio, residual=residual, kmax_k=kmax_k, gem_p=gem_p)\n",
    "        b = branches\n",
    "        f_total = filters\n",
    "        f_per = max(1, f_total // b)\n",
    "\n",
    "        self.branches = nn.ModuleList()\n",
    "        for bi, k in enumerate(kernels):\n",
    "            core = []\n",
    "            in_ch = hidden\n",
    "            d = dilations[bi] if bi < len(dilations) else 1\n",
    "            pad = ((k - 1) * d) // 2\n",
    "\n",
    "            use_res = residual\n",
    "            skip = None\n",
    "\n",
    "            for li in range(layers):\n",
    "                if sep:\n",
    "                    \n",
    "                    core.append(nn.Conv1d(in_ch, in_ch, kernel_size=k, padding=pad, dilation=d, groups=in_ch))\n",
    "                    core.append(get_act(act)); core.append(get_norm(norm, in_ch))\n",
    "                    core.append(nn.Conv1d(in_ch, f_per, kernel_size=1))\n",
    "                else:\n",
    "                    \n",
    "                    g_req = int(groups)\n",
    "                    if (in_ch % g_req == 0) and (f_per % g_req == 0):\n",
    "                        g_ok = g_req\n",
    "                    else:\n",
    "                        g_ok = 1\n",
    "                        for g in (8,4,2,1):\n",
    "                            if (in_ch % g == 0) and (f_per % g == 0):\n",
    "                                g_ok = g\n",
    "                                break\n",
    "                    core.append(nn.Conv1d(in_ch, f_per, kernel_size=k, padding=pad, dilation=d, groups=g_ok))\n",
    "                core.append(get_act(act))\n",
    "                core.append(get_norm(norm, f_per))\n",
    "                if se_ratio and se_ratio > 0:\n",
    "                    core.append(SE1d(f_per, se_ratio))\n",
    "\n",
    "                in_ch = f_per\n",
    "                if use_res and skip is None:\n",
    "                    skip = nn.Identity() if hidden == f_per else nn.Conv1d(hidden, f_per, 1)\n",
    "\n",
    "            branch_core = nn.Sequential(*core)\n",
    "            self.branches.append(ResidualBranch(branch_core, skip if (use_res and skip is not None) else nn.Identity()) if use_res else branch_core)\n",
    "\n",
    "        concat_ch = f_per * b\n",
    "        self.proj = nn.Conv1d(concat_ch, f_total, kernel_size=1) if concat_ch != f_total else None\n",
    "\n",
    "        self.pooling = pooling\n",
    "        if self.pooling == 'attn':\n",
    "            self.attn = nn.Linear(f_total, 1)\n",
    "        elif self.pooling == 'gem':\n",
    "            self.gem_p = nn.Parameter(torch.tensor(float(gem_p)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.out = nn.Linear(f_total, num_classes)\n",
    "\n",
    "    def _gem(self, x, eps=1e-6):  \n",
    "        p = self.gem_p\n",
    "        x = x.clamp(min=eps).pow(p)\n",
    "        x = x.mean(dim=2)\n",
    "        return x.pow(1.0/p)\n",
    "\n",
    "    def forward(self, last_hidden: torch.Tensor, attn_mask: torch.Tensor):\n",
    "        \n",
    "        with _autocast_off():\n",
    "            x = last_hidden.transpose(1, 2).contiguous().float()  \n",
    "\n",
    "            feats_list = [branch(x) for branch in self.branches]\n",
    "            feats = torch.cat(feats_list, dim=1)\n",
    "            if self.proj is not None:\n",
    "                feats = self.proj(feats)\n",
    "\n",
    "            if self.pooling == 'max':\n",
    "                x_out = feats.amax(dim=2)\n",
    "            elif self.pooling == 'avg':\n",
    "                x_out = feats.mean(dim=2)\n",
    "            elif self.pooling == 'gem':\n",
    "                x_out = self._gem(feats)\n",
    "            elif self.pooling == 'kmax':\n",
    "                mask = (attn_mask == 0)[:, None, :]\n",
    "                feats_m = feats.masked_fill(mask, torch.finfo(feats.dtype).min)\n",
    "                k = max(1, int(self.cfg.get('kmax_k', 1)))\n",
    "                vals, _ = torch.topk(feats_m, k, dim=2)\n",
    "                x_out = vals.mean(dim=2)\n",
    "            else:  # 'attn'\n",
    "                feats_T = feats.transpose(1, 2)                    \n",
    "                logits = self.attn(feats_T)                        \n",
    "                mask = (attn_mask == 0).unsqueeze(-1)              \n",
    "                logits = logits.masked_fill(mask, torch.finfo(logits.dtype).min)\n",
    "                w = torch.softmax(logits, dim=1)                   \n",
    "                x_out = (feats_T * w).sum(dim=1)                   \n",
    "\n",
    "            x_out = self.dropout(x_out)\n",
    "            return self.out(x_out)\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, name: str):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = AutoModel.from_pretrained(name)\n",
    "\n",
    "        \n",
    "        if _USE_ADAPTER and _PEFT_OK and (_ADAPTER_DIR is not None) and os.path.isdir(_ADAPTER_DIR):\n",
    "            try:\n",
    "                self.model = PeftModel.from_pretrained(self.model, _ADAPTER_DIR)\n",
    "                print(\"Loaded adapter from:\", _ADAPTER_DIR)\n",
    "            except Exception as _e:\n",
    "                print(\"PEFT adapter load skipped:\", _e)\n",
    "\n",
    "        \n",
    "        try:\n",
    "            vocab_model = self.model.get_input_embeddings().weight.shape[0]\n",
    "            vocab_tok   = len(TOKENIZER)\n",
    "            if vocab_tok != vocab_model and hasattr(self.model, \"resize_token_embeddings\"):\n",
    "                print(f\"[ADAPT] Resizing token embeddings: {vocab_model} -> {vocab_tok}\")\n",
    "                self.model.resize_token_embeddings(vocab_tok)\n",
    "        except Exception as _e:\n",
    "            print(\"Embedding resize check failed/skipped:\", _e)\n",
    "\n",
    "        self.hidden = self.model.config.hidden_size\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        return out.last_hidden_state\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        self.backbone = Backbone(BACKBONE_NAME)\n",
    "        self.head = CNNHead(\n",
    "            hidden=self.backbone.hidden,\n",
    "            num_classes=num_classes,\n",
    "            layers=2, filters=256, branches=3,\n",
    "            kernels=(1,5,7), dilations=(2,8,8),\n",
    "            dropout=0.2, pooling='attn',\n",
    "            act='gelu', norm='bn',\n",
    "            sep=False, groups=8, se_ratio=4, residual=True,\n",
    "            kmax_k=1, gem_p=4.0\n",
    "        )\n",
    "        self.backbone_frozen = False\n",
    "        \n",
    "        try:\n",
    "            self.backbone.model.gradient_checkpointing_enable()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    def freeze_backbone(self, freeze=True):\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = not freeze\n",
    "        self.backbone_frozen = freeze\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        if self.backbone_frozen:\n",
    "            with torch.no_grad():\n",
    "                last_hidden = self.backbone(input_ids, attention_mask)\n",
    "            last_hidden = last_hidden.detach()\n",
    "        else:\n",
    "            last_hidden = self.backbone(input_ids, attention_mask)\n",
    "        return self.head(last_hidden, attention_mask)\n",
    "\n",
    "\n",
    "NUM_CLASSES = 3\n",
    "model = SentimentModel(num_classes=NUM_CLASSES).to(DEVICE)\n",
    "\n",
    "\n",
    "print(\"Backbone:\", BACKBONE_NAME, \"| hidden:\", model.backbone.hidden)\n",
    "cfg = model.head.cfg\n",
    "print(\"Head cfg:\", cfg)\n",
    "for i, br in enumerate(model.head.branches):\n",
    "    core = br.core if isinstance(br, ResidualBranch) else br\n",
    "    groups_seq = [m.groups for m in core if isinstance(m, nn.Conv1d)]\n",
    "    print(f\"branch {i} kernel={cfg['kernels'][i]} dil={cfg['dilations'][i]} groups(seq)={groups_seq}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc053968-60ad-4c1c-9caa-aec6ab3ba9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, hashlib\n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "VAL_FRAC = 0.10\n",
    "MAX_LEN  = 160   \n",
    "BATCH    = 64\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "\n",
    "assert 'TOKENIZER' in globals(), \"Please run Cell 1 first so TOKENIZER is defined.\"\n",
    "\n",
    "\n",
    "DATASET = 'B'   \n",
    "CSV_A = \"taskA_youtube_raw.csv\"\n",
    "CSV_B = \"taskB_youtube_raw.csv\"\n",
    "CSV_PATH = CSV_A if DATASET=='A' else CSV_B\n",
    "\n",
    "TEXT_COL  = \"text\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "class YouTubeCommentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=160):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        enc = self.tok(\n",
    "            str(self.texts[i]),\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=self.max_len\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': enc['input_ids'],\n",
    "            'attention_mask': enc['attention_mask'],\n",
    "            'labels': int(self.labels[i])\n",
    "        }\n",
    "\n",
    "\n",
    "COLLATE = DataCollatorWithPadding(tokenizer=TOKENIZER, pad_to_multiple_of=8, return_tensors=\"pt\")\n",
    "\n",
    "def build_loader(texts, labels, batch=BATCH, shuffle=False):\n",
    "    ds = YouTubeCommentDataset(texts, labels, TOKENIZER, MAX_LEN)\n",
    "    if shuffle:\n",
    "        \n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(SEED)\n",
    "        sampler = RandomSampler(ds, generator=g)\n",
    "        return DataLoader(\n",
    "            ds, batch_size=batch, sampler=sampler,\n",
    "            num_workers=NUM_WORKERS, pin_memory=True, collate_fn=COLLATE\n",
    "        )\n",
    "    else:\n",
    "        return DataLoader(\n",
    "            ds, batch_size=batch, shuffle=False,\n",
    "            num_workers=NUM_WORKERS, pin_memory=True, collate_fn=COLLATE\n",
    "        )\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, encoding=\"utf-8\", engine=\"python\").copy()\n",
    "df[LABEL_COL] = df[LABEL_COL].astype(int)\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    df, test_size=VAL_FRAC, stratify=df[LABEL_COL], random_state=SEED\n",
    ")\n",
    "\n",
    "def _checksum_index(idx):\n",
    "    a = np.asarray(idx, dtype=np.int64)\n",
    "    return hashlib.md5(a.tobytes()).hexdigest()[:10]\n",
    "\n",
    "NUM_CLASSES = int(df[LABEL_COL].nunique())  # for Cell 3\n",
    "\n",
    "print(f\"Dataset: {CSV_PATH}\")\n",
    "print(\"Counts -> train/val:\", len(train_df), len(val_df))\n",
    "print(\"Index checksums -> train:\", _checksum_index(train_df.index),\n",
    "      \" val:\", _checksum_index(val_df.index))\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "\n",
    "Xtr, ytr = train_df[TEXT_COL].astype(str).tolist(), train_df[LABEL_COL].astype(int).tolist()\n",
    "Xva, yva = val_df[TEXT_COL].astype(str).tolist(),   val_df[LABEL_COL].astype(int).tolist()\n",
    "\n",
    "train_loader = build_loader(Xtr, ytr, shuffle=True)\n",
    "val_loader   = build_loader(Xva, yva, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5d7f29-f350-4cd2-9171-6d926dcc4eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TAPT (This doesn't get used)\n",
    "import os, re, shutil\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForMaskedLM,\n",
    "    DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model  \n",
    "\n",
    "\n",
    "\n",
    "BASE_FOR_TAPT = \"dapt_merged_backbone\" if os.path.isdir(\"dapt_merged_backbone\") else \"cardiffnlp/twitter-roberta-base\"\n",
    "\n",
    "try:\n",
    "    MAX_LEN\n",
    "except NameError:\n",
    "    MAX_LEN = 160\n",
    "\n",
    "\n",
    "try:\n",
    "    VAL_FRAC\n",
    "except NameError:\n",
    "    VAL_FRAC = 0.10\n",
    "try:\n",
    "    SEED\n",
    "except NameError:\n",
    "    SEED = 42\n",
    "\n",
    "\n",
    "try:\n",
    "    CSV_PATH\n",
    "except NameError:\n",
    "    \n",
    "    CSV_PATH = \"taskB_youtube_raw.csv\"\n",
    "\n",
    "\n",
    "try:\n",
    "    TEXT_COL\n",
    "except NameError:\n",
    "    TEXT_COL = \"text\"\n",
    "try:\n",
    "    LABEL_COL\n",
    "except NameError:\n",
    "    LABEL_COL = \"label\"\n",
    "\n",
    "TOTAL_STEPS_TAPT     = 12_000     \n",
    "WARMUP_RATIO_TAPT    = 0.06\n",
    "LR_TAPT              = 1e-4\n",
    "BATCH_TRAIN_PER_DEV  = 16\n",
    "GRAD_ACCUM_STEPS     = 2\n",
    "MLM_PROB             = 0.15\n",
    "\n",
    "LORA_R               = 8\n",
    "LORA_ALPHA           = 16\n",
    "LORA_DROPOUT         = 0.10\n",
    "OUT_ADAPTER_DIR      = \"tapt_lora_adapter\"\n",
    "OUT_MERGED_DIR       = \"tapt_merged_backbone\"\n",
    "\n",
    "\n",
    "assert os.path.exists(CSV_PATH), f\"TAPT needs your labeled dataset CSV (got {CSV_PATH!r} not found).\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "if TEXT_COL not in df.columns:\n",
    "    \n",
    "    for c in [\"clean_text\", \"comment\", \"body\", \"content\"]:\n",
    "        if c in df.columns:\n",
    "            TEXT_COL = c; break\n",
    "if LABEL_COL not in df.columns:\n",
    "    for c in [\"label\", \"labels\", \"target\"]:\n",
    "        if c in df.columns:\n",
    "            LABEL_COL = c; break\n",
    "\n",
    "assert TEXT_COL in df.columns, f\"Could not find a text column in {CSV_PATH}\"\n",
    "assert LABEL_COL in df.columns, f\"Could not find a label column in {CSV_PATH}\"\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, _ = train_test_split(\n",
    "    df[[TEXT_COL, LABEL_COL]],\n",
    "    test_size=VAL_FRAC,\n",
    "    stratify=df[LABEL_COL] if df[LABEL_COL].nunique() > 1 else None,\n",
    "    random_state=SEED,\n",
    ")\n",
    "\n",
    "train_texts = [str(x) for x in train_df[TEXT_COL].dropna().tolist()]\n",
    "\n",
    "def _basic_clean(s: str) -> str:\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "train_texts = [_basic_clean(s) for s in train_texts if s.strip()]\n",
    "train_texts = list(dict.fromkeys(train_texts))  \n",
    "\n",
    "print(f\"[TAPT] Source: {CSV_PATH} | train_texts={len(train_texts)}\")\n",
    "\n",
    "class _MLMDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], tokenizer, max_len: int):\n",
    "        self.enc = tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            padding=False,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "    def __len__(self): return len(self.enc[\"input_ids\"])\n",
    "    def __getitem__(self, i):\n",
    "        return {k: torch.tensor(v[i]) for k, v in self.enc.items()}\n",
    "\n",
    "\n",
    "print(f\"[TAPT] Base backbone: {BASE_FOR_TAPT}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_FOR_TAPT, use_fast=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(BASE_FOR_TAPT)\n",
    "\n",
    "\n",
    "LORA_TARGETS = [\n",
    "    \"query\",\"key\",\"value\",\"dense\",\n",
    "    \"q_proj\",\"k_proj\",\"v_proj\",\"out_proj\",\n",
    "    \"intermediate.dense\",\"output.dense\"\n",
    "]\n",
    "\n",
    "\n",
    "try:\n",
    "    from peft import TaskType\n",
    "    _TASKTYPE = (\n",
    "        getattr(TaskType, \"MASKED_LM\", None)\n",
    "        or getattr(TaskType, \"TOKEN_CLS\", None)\n",
    "        or getattr(TaskType, \"FEATURE_EXTRACTION\", None)\n",
    "        or getattr(TaskType, \"SEQ_CLS\", None)\n",
    "        or getattr(TaskType, \"CAUSAL_LM\", None)\n",
    "    )\n",
    "except Exception:\n",
    "    TaskType = None\n",
    "    _TASKTYPE = None\n",
    "\n",
    "lora_kwargs = dict(\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    target_modules=LORA_TARGETS, bias=\"none\",\n",
    ")\n",
    "if _TASKTYPE is not None:\n",
    "    lora_kwargs[\"task_type\"] = _TASKTYPE\n",
    "\n",
    "lcfg = LoraConfig(**lora_kwargs)\n",
    "model = get_peft_model(model, lcfg)\n",
    "\n",
    "try:\n",
    "    model.print_trainable_parameters()\n",
    "except Exception as e:\n",
    "    print(\"[TAPT] print_trainable_parameters() unavailable:\", e)\n",
    "\n",
    "dataset = _MLMDataset(train_texts, tokenizer, MAX_LEN)\n",
    "collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=MLM_PROB)\n",
    "\n",
    "\n",
    "fp16 = torch.cuda.is_available()\n",
    "args = TrainingArguments(\n",
    "    output_dir=OUT_ADAPTER_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=TOTAL_STEPS_TAPT,\n",
    "    per_device_train_batch_size=BATCH_TRAIN_PER_DEV,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM_STEPS,\n",
    "    learning_rate=LR_TAPT,\n",
    "    warmup_ratio=WARMUP_RATIO_TAPT,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_steps=2_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    dataloader_drop_last=False,\n",
    "    fp16=fp16,\n",
    "    report_to=[],\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "print(\"[TAPT] Starting training…\")\n",
    "trainer.train()\n",
    "print(\"[TAPT] Done.\")\n",
    "\n",
    "\n",
    "for d in [OUT_ADAPTER_DIR, OUT_MERGED_DIR]:\n",
    "    if os.path.isdir(d):\n",
    "        shutil.rmtree(d)\n",
    "\n",
    "trainer.save_model(OUT_ADAPTER_DIR)\n",
    "tokenizer.save_pretrained(OUT_ADAPTER_DIR)\n",
    "\n",
    "print(\"[TAPT] Merging LoRA into base weights…\")\n",
    "merged = trainer.model.merge_and_unload()\n",
    "os.makedirs(OUT_MERGED_DIR, exist_ok=True)\n",
    "merged.save_pretrained(OUT_MERGED_DIR)\n",
    "tokenizer.save_pretrained(OUT_MERGED_DIR)\n",
    "\n",
    "print(f\"[TAPT] Saved adapter -> {OUT_ADAPTER_DIR}\")\n",
    "print(f\"[TAPT] Saved merged backbone -> {OUT_MERGED_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538740e-74d3-444f-bd9b-f077d5391b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Training loop\n",
    "import os, re, math, torch, numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.amp import GradScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "EPOCHS                  = 26\n",
    "PATIENCE                = 8\n",
    "FREEZE_BACKBONE_EPOCHS  = 4\n",
    "\n",
    "HEAD_LR        = 1e-4\n",
    "BACKBONE_LR    = 1e-5\n",
    "LR_LAYER_DECAY = 1      \n",
    "WEIGHT_DECAY   = 0.01\n",
    "MIXED_PREC     = True\n",
    "LABEL_SMOOTH   = 0.06\n",
    "CLASS_WEIGHT_EXP = 0.35\n",
    "WARMUP_RATIO   = 0.08\n",
    "EMA_DECAY      = 0.999\n",
    "R_DROP_ALPHA   = 1.00   \n",
    "    \n",
    "\n",
    "try:\n",
    "    IS_TASK_A = ('taskA' in CSV_PATH)\n",
    "except NameError:\n",
    "    IS_TASK_A = False\n",
    "CKPT_OUT = f\"task{'A' if IS_TASK_A else 'B'}_from_scratch_mtnas_parity.pt\"\n",
    "\n",
    "\n",
    "_DAPT_MERGED  = \"dapt_merged_backbone\"\n",
    "_DAPT_ADAPTER = \"dapt_lora_adapter\"\n",
    "_TAPT_MERGED  = \"tapt_merged_backbone\"\n",
    "_TAPT_ADAPTER = \"tapt_lora_adapter\"\n",
    "\n",
    "def _safe_name_or_path(m):\n",
    "    try:\n",
    "        return getattr(m, \"name_or_path\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _is_peft_model(m):\n",
    "    try:\n",
    "        return hasattr(m, \"peft_config\") or m.__class__.__name__.lower().startswith(\"peft\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def _detect_adapt_status(model, tokenizer):\n",
    "    has_dm = os.path.isdir(_DAPT_MERGED)\n",
    "    has_da = os.path.isdir(_DAPT_ADAPTER)\n",
    "    has_tm = os.path.isdir(_TAPT_MERGED)\n",
    "    has_ta = os.path.isdir(_TAPT_ADAPTER)\n",
    "\n",
    "    core = getattr(model.backbone, \"model\", model.backbone)\n",
    "    name_path = _safe_name_or_path(core) or \"<unknown>\"\n",
    "    is_peft   = _is_peft_model(core)\n",
    "\n",
    "    try: tok_vocab = len(tokenizer)\n",
    "    except Exception: tok_vocab = -1\n",
    "    try: emb_rows = core.get_input_embeddings().weight.size(0)\n",
    "    except Exception: emb_rows = -2\n",
    "\n",
    "    specials = {}\n",
    "    try:\n",
    "        vv = tokenizer.get_vocab()\n",
    "        specials[\"<url>\"] = \"<url>\" in vv\n",
    "        specials[\"@user\"] = \"@user\" in vv\n",
    "    except Exception:\n",
    "        specials[\"<url>\"] = None\n",
    "        specials[\"@user\"] = None\n",
    "\n",
    "    if name_path.endswith(_DAPT_MERGED):\n",
    "        verdict = \"DAPT merged\"\n",
    "    elif is_peft and has_da:\n",
    "        verdict = \"DAPT adapter (PEFT)\"\n",
    "    elif name_path.endswith(_DAPT_ADAPTER):\n",
    "        verdict = \"DAPT direct-load (no PEFT)\"\n",
    "    elif name_path.endswith(_TAPT_MERGED):\n",
    "        verdict = \"TAPT merged\"\n",
    "    elif is_peft and has_ta:\n",
    "        verdict = \"TAPT adapter (PEFT)\"\n",
    "    elif name_path.endswith(_TAPT_ADAPTER):\n",
    "        verdict = \"TAPT direct-load (no PEFT)\"\n",
    "    else:\n",
    "        verdict = \"Base only\"\n",
    "\n",
    "    warnings = []\n",
    "    if tok_vocab != emb_rows and emb_rows > 0:\n",
    "        warnings.append(f\"Tokenizer vocab ({tok_vocab}) != embed rows ({emb_rows}) — embeddings NOT resized.\")\n",
    "    if any([has_dm, has_da]) and (\"DAPT\" not in verdict):\n",
    "        warnings.append(\"DAPT artifacts exist, but training is not using them.\")\n",
    "    if any([has_tm, has_ta]) and (\"TAPT\" not in verdict) and not any([has_dm, has_da]):\n",
    "        warnings.append(\"TAPT artifacts exist, but training is not using them.\")\n",
    "    if specials.get(\"<url>\") is False or specials.get(\"@user\") is False:\n",
    "        warnings.append(\"Tokenizer missing <url> and/or @user special tokens.\")\n",
    "\n",
    "    return {\n",
    "        \"verdict\": verdict,\n",
    "        \"name_or_path\": name_path,\n",
    "        \"is_peft\": bool(is_peft),\n",
    "        \"tok_vocab\": int(tok_vocab),\n",
    "        \"embed_rows\": int(emb_rows),\n",
    "        \"specials\": specials,\n",
    "        \"has_dirs\": {\n",
    "            \"dapt_merged\": has_dm, \"dapt_adapter\": has_da,\n",
    "            \"tapt_merged\": has_tm, \"tapt_adapter\": has_ta\n",
    "        },\n",
    "        \"warnings\": warnings\n",
    "    }\n",
    "\n",
    "_status = _detect_adapt_status(model, TOKENIZER)\n",
    "print(\"\\n==== ADAPTATION STATUS ====\")\n",
    "print(\" verdict         :\", _status[\"verdict\"])\n",
    "print(\" backbone path   :\", _status[\"name_or_path\"])\n",
    "print(\" PEFT attached?  :\", _status[\"is_peft\"])\n",
    "print(\" vocab vs embeds :\", _status[\"tok_vocab\"], \"vs\", _status[\"embed_rows\"])\n",
    "print(\" specials        :\", _status[\"specials\"])\n",
    "print(\" dirs present    :\", _status[\"has_dirs\"])\n",
    "if _status[\"warnings\"]:\n",
    "    for w in _status[\"warnings\"]:\n",
    "        print(\" Warning\", w)\n",
    "else:\n",
    "    print(\" All adaptation checks look consistent\")\n",
    "print(\"========================================\\n\")\n",
    "\n",
    "\n",
    "def _emb_vocab_ok_verify(core_model, tok):\n",
    "    try:\n",
    "        return core_model.get_input_embeddings().weight.shape[0] == len(tok)\n",
    "    except Exception:\n",
    "        return True  \n",
    "\n",
    "_core_model = getattr(getattr(model, \"backbone\", model), \"model\", getattr(model, \"backbone\", model))\n",
    "_model_path = getattr(_core_model, \"name_or_path\", str(type(_core_model)))\n",
    "_tok_path   = getattr(TOKENIZER, \"name_or_path\", \"?\")\n",
    "\n",
    "print(f\"[Verify] model path:     {_model_path}\")\n",
    "print(f\"[Verify] tokenizer path: {_tok_path}\")\n",
    "print(f\"[Verify] vocab~emb match: {_emb_vocab_ok_verify(_core_model, TOKENIZER)}\")\n",
    "\n",
    "_lora_active = any(\"lora_\" in n for n, p in model.named_parameters())\n",
    "print(f\"[Verify] PEFT/LoRA active? {_lora_active} (False is expected when using merged backbones)\")\n",
    "\n",
    "\n",
    "assert _emb_vocab_ok_verify(_core_model, TOKENIZER), \\\n",
    "    \"Tokenizer vocab size and model embedding size mismatch — likely loaded the wrong tokenizer/backbone.\"\n",
    "\n",
    "\n",
    "def _set_gradient_checkpointing(model, enabled: bool):\n",
    "    try:\n",
    "        core = getattr(model.backbone, \"model\", model.backbone)\n",
    "        if enabled and hasattr(core, \"gradient_checkpointing_enable\"):\n",
    "            core.gradient_checkpointing_enable()\n",
    "        if (not enabled) and hasattr(core, \"gradient_checkpointing_disable\"):\n",
    "            core.gradient_checkpointing_disable()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "\n",
    "def _is_nodecay(name: str, p: nn.Parameter) -> bool:\n",
    "    lname = name.lower()\n",
    "    return (p.ndim < 2) or ('layernorm' in lname) or ('layer_norm' in lname) or lname.endswith('.bias')\n",
    "\n",
    "NUM_LAYERS = getattr(getattr(model.backbone, \"model\", model.backbone).config, \"num_hidden_layers\", 12)\n",
    "_layer_pat = re.compile(r'encoder\\.layer\\.(\\d+)\\.')\n",
    "\n",
    "def _layer_id_from_name(n: str) -> int:\n",
    "    if \"embeddings\" in n:\n",
    "        return -1\n",
    "    m = _layer_pat.search(n)\n",
    "    if m: \n",
    "        return int(m.group(1))      \n",
    "    return NUM_LAYERS               \n",
    "\n",
    "def _lr_for_layer(layer_id: int) -> float:\n",
    "    \n",
    "    if layer_id == -1:\n",
    "        depth = NUM_LAYERS + 1\n",
    "    elif layer_id >= NUM_LAYERS:\n",
    "        depth = 0\n",
    "    else:\n",
    "        depth = (NUM_LAYERS - 1 - layer_id)\n",
    "    return BACKBONE_LR * (LR_LAYER_DECAY ** depth)\n",
    "\n",
    "\n",
    "layer_groups = {}\n",
    "for n, p in model.backbone.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    lid = _layer_id_from_name(n)\n",
    "    key = \"nodecay\" if _is_nodecay(n, p) else \"decay\"\n",
    "    layer_groups.setdefault(lid, {\"decay\": [], \"nodecay\": []})\n",
    "    layer_groups[lid][key].append(p)\n",
    "\n",
    "bb_param_groups = []\n",
    "for lid in sorted(layer_groups.keys()):\n",
    "    lr = _lr_for_layer(lid)\n",
    "    if layer_groups[lid][\"decay\"]:\n",
    "        bb_param_groups.append({\"params\": layer_groups[lid][\"decay\"],   \"lr\": lr, \"weight_decay\": WEIGHT_DECAY})\n",
    "    if layer_groups[lid][\"nodecay\"]:\n",
    "        bb_param_groups.append({\"params\": layer_groups[lid][\"nodecay\"], \"lr\": lr, \"weight_decay\": 0.0})\n",
    "\n",
    "\n",
    "hd_decay, hd_nodecay = [], []\n",
    "for n, p in model.head.named_parameters():\n",
    "    if not p.requires_grad: \n",
    "        continue\n",
    "    (hd_nodecay if _is_nodecay(n, p) else hd_decay).append(p)\n",
    "\n",
    "param_groups = bb_param_groups + [\n",
    "    {\"params\": hd_decay,   \"lr\": HEAD_LR, \"weight_decay\": WEIGHT_DECAY},\n",
    "    {\"params\": hd_nodecay, \"lr\": HEAD_LR, \"weight_decay\": 0.0},\n",
    "]\n",
    "\n",
    "optimizer = AdamW(param_groups)\n",
    "\n",
    "\n",
    "def _layer_lr_table():\n",
    "    order = [-1] + list(range(NUM_LAYERS)) + [NUM_LAYERS]\n",
    "    return [(lid, _lr_for_layer(lid)) for lid in order]\n",
    "\n",
    "print(f\"LLRD: BASE_LR={BACKBONE_LR}, DECAY={LR_LAYER_DECAY}, layers={NUM_LAYERS}\")\n",
    "print(\"Backbone per-layer LRs:\", [(lid, f\"{lr:.8f}\") for lid, lr in _layer_lr_table()])\n",
    "print(\"Total param groups:\", len(optimizer.param_groups))\n",
    "\n",
    "\n",
    "scaler = GradScaler(enabled=(MIXED_PREC and DEVICE.type == 'cuda'))\n",
    "\n",
    "\n",
    "counts = np.bincount(np.array(ytr, dtype=np.int64), minlength=NUM_CLASSES).astype(float)\n",
    "w = 1.0 / np.maximum(counts, 1.0) ** CLASS_WEIGHT_EXP\n",
    "w = w / w.mean()\n",
    "class_weights = torch.tensor(w, dtype=torch.float32, device=DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=LABEL_SMOOTH)\n",
    "\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = max(1, steps_per_epoch * EPOCHS)\n",
    "warmup_steps = max(1, int(WARMUP_RATIO * total_steps))\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "def init_ema(m: nn.Module):\n",
    "    ema = {}\n",
    "    for n, p in m.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            ema[n] = p.detach().clone()\n",
    "    return ema\n",
    "\n",
    "@torch.no_grad()\n",
    "def update_ema(m: nn.Module, ema: dict, decay: float = EMA_DECAY):\n",
    "    for n, p in m.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            \n",
    "            if n not in ema:\n",
    "                ema[n] = p.detach().clone()\n",
    "            ema[n].mul_(decay).add_(p.detach(), alpha=1.0 - decay)\n",
    "\n",
    "@torch.no_grad()\n",
    "def swap_in_ema(m: nn.Module, ema: dict):\n",
    "    backup = {}\n",
    "    for n, p in m.named_parameters():\n",
    "        if p.requires_grad and n in ema:\n",
    "            backup[n] = p.detach().clone()\n",
    "            p.copy_(ema[n])\n",
    "    return backup\n",
    "\n",
    "@torch.no_grad()\n",
    "def restore_from_backup(m: nn.Module, backup: dict):\n",
    "    for n, p in m.named_parameters():\n",
    "        if p.requires_grad and n in backup:\n",
    "            p.copy_(backup[n])\n",
    "\n",
    "def build_ema_state_dict(m: nn.Module, ema: dict):\n",
    "    sd = m.state_dict()\n",
    "    for n, p in m.named_parameters():\n",
    "        if p.requires_grad and n in ema:\n",
    "            sd[n] = ema[n].detach().clone().to(sd[n].device)\n",
    "    return sd\n",
    "\n",
    "ema_state = init_ema(model)\n",
    "\n",
    "print(\"Trainable params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(\"Backbone trainable?:\", any(p.requires_grad for p in model.backbone.parameters()))\n",
    "print(f\"Class counts: {counts.tolist()} | weights(exp={CLASS_WEIGHT_EXP}): {np.round(w, 4).tolist()} | label_smooth={LABEL_SMOOTH}\")\n",
    "\n",
    "best_acc = -1.0\n",
    "best_f1  = -1.0\n",
    "best_state = None\n",
    "epochs_no_improve = 0\n",
    "TARGET_NAMES = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {ep}/{EPOCHS}\")\n",
    "\n",
    "    \n",
    "    frozen = (ep <= FREEZE_BACKBONE_EPOCHS)\n",
    "    model.freeze_backbone(frozen)\n",
    "    _set_gradient_checkpointing(model, enabled=not frozen)\n",
    "    print(\"  Backbone frozen?\" , frozen)\n",
    "\n",
    "    \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        ids = batch['input_ids'].to(DEVICE, non_blocking=True)\n",
    "        msk = batch['attention_mask'].to(DEVICE, non_blocking=True)\n",
    "        lab = batch['labels'].to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.amp.autocast(\"cuda\", enabled=(MIXED_PREC and DEVICE.type == 'cuda')):\n",
    "            if R_DROP_ALPHA > 0.0:\n",
    "                \n",
    "                logits1 = model(ids, msk)\n",
    "                logits2 = model(ids, msk)\n",
    "                ce = 0.5 * (criterion(logits1, lab) + criterion(logits2, lab))\n",
    "\n",
    "                p = F.log_softmax(logits1, dim=1); q = F.log_softmax(logits2, dim=1)\n",
    "                kl = 0.5 * (F.kl_div(p, q.exp(), reduction=\"batchmean\") +\n",
    "                            F.kl_div(q, p.exp(), reduction=\"batchmean\"))\n",
    "                loss = ce + R_DROP_ALPHA * kl\n",
    "                logits_for_metrics = (logits1 + logits2) / 2.0\n",
    "            else:\n",
    "                logits_for_metrics = model(ids, msk)\n",
    "                loss = criterion(logits_for_metrics, lab)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "        update_ema(model, ema_state, decay=EMA_DECAY)\n",
    "\n",
    "        bs = lab.size(0)\n",
    "        total_loss += loss.item() * bs\n",
    "        y_true.extend(lab.detach().cpu().numpy().tolist())\n",
    "        y_pred.extend(logits_for_metrics.argmax(dim=1).detach().cpu().numpy().tolist())\n",
    "\n",
    "    train_acc = accuracy_score(y_true, y_pred)\n",
    "    train_loss = total_loss / max(1, len(train_loader.dataset))\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    backup_params = swap_in_ema(model, ema_state)\n",
    "\n",
    "    vy_true, vy_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            ids = batch[\"input_ids\"].to(DEVICE, non_blocking=True)\n",
    "            msk = batch[\"attention_mask\"].to(DEVICE, non_blocking=True)\n",
    "            lab = batch[\"labels\"].to(DEVICE, non_blocking=True)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(MIXED_PREC and DEVICE.type == 'cuda')):\n",
    "                logits = model(ids, msk)\n",
    "            vy_true.extend(lab.detach().cpu().tolist())\n",
    "            vy_pred.extend(logits.argmax(dim=1).detach().cpu().tolist())\n",
    "\n",
    "    val_acc = accuracy_score(vy_true, vy_pred)\n",
    "    val_f1  = f1_score(vy_true, vy_pred, average=\"macro\")\n",
    "    print(f\"Validation Accuracy: {val_acc:.4f} | Macro-F1: {val_f1:.4f}\")\n",
    "\n",
    "    present = sorted(np.unique(vy_true).tolist())\n",
    "    print(classification_report(\n",
    "        vy_true, vy_pred,\n",
    "        labels=present,\n",
    "        target_names=[{0:\"negative\",1:\"neutral\",2:\"positive\"}.get(i, f\"class_{i}\") for i in present],\n",
    "        digits=4, zero_division=0\n",
    "    ))\n",
    "\n",
    "    \n",
    "    improved = (val_acc > best_acc) or (abs(val_acc - best_acc) < 1e-6 and val_f1 > best_f1)\n",
    "    if improved:\n",
    "        best_acc, best_f1 = val_acc, val_f1\n",
    "        epochs_no_improve = 0\n",
    "        best_state = {k: v.cpu() for k, v in build_ema_state_dict(model, ema_state).items()}\n",
    "        torch.save({\n",
    "            \"arch\": getattr(model.head, \"cfg\", None),\n",
    "            \"state_dict\": best_state,\n",
    "            \"backbone\": _safe_name_or_path(getattr(model, \"backbone\", None).model) if hasattr(getattr(model, \"backbone\", None), \"model\") else None,\n",
    "            \"num_classes\": NUM_CLASSES,\n",
    "            \"val_acc\": float(val_acc),\n",
    "            \"val_f1\": float(val_f1),\n",
    "            \"ema_decay\": EMA_DECAY,\n",
    "            \"label_smoothing\": LABEL_SMOOTH,\n",
    "            \"class_weights\": w.tolist(),\n",
    "            \"class_weight_exp\": CLASS_WEIGHT_EXP,\n",
    "            \"adapt_meta\": _status,\n",
    "            \"r_drop_alpha\": R_DROP_ALPHA,\n",
    "            \"llrd\": {\"base_lr\": BACKBONE_LR, \"layer_decay\": LR_LAYER_DECAY},\n",
    "        }, CKPT_OUT)\n",
    "        print(f\" New best (EMA) saved (acc={val_acc:.4f}, f1={val_f1:.4f}) → {CKPT_OUT}\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epoch(s).\")\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(\"\\n Early stopping triggered.\")\n",
    "\n",
    "    restore_from_backup(model, backup_params)\n",
    "    if epochs_no_improve >= PATIENCE:\n",
    "        break\n",
    "\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state, strict=True)\n",
    "model.eval()\n",
    "print(\"\\n Best (EMA) model reloaded. Best Validation Accuracy:\", f\"{best_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
