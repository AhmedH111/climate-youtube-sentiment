{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e390d4-9596-4484-815c-86f7df6ca71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"YOUTUBE_API_KEY\"] = #I can't post this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81327431-949a-4203-bc3f-d2e5a1abee29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, time, math, json, random, pathlib, datetime as dt\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "USE_PARQUET = True\n",
    "\n",
    "\n",
    "API_KEY = os.environ.get(\"YOUTUBE_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Missing YOUTUBE_API_KEY env var — please set it before running.\")\n",
    "\n",
    "try:\n",
    "    from googleapiclient.discovery import build\n",
    "    from googleapiclient.errors import HttpError\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"Please install google-api-python-client (pip install google-api-python-client).\") from e\n",
    "\n",
    "YOUTUBE = build(\"youtube\", \"v3\", developerKey=API_KEY)\n",
    "\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"./yt_climate_data\")\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "PROGRESS_PATH = DATA_DIR / \"progress.json\"\n",
    "\n",
    "\n",
    "def _sleep_backoff(attempt: int):\n",
    "    \n",
    "    delay = min(0.5 * (2 ** attempt) + random.random() * 0.25, 8.0)\n",
    "    time.sleep(delay)\n",
    "\n",
    "def _yt_call(method, **kwargs):\n",
    "    \n",
    "    attempts = 0\n",
    "    while True:\n",
    "        try:\n",
    "            return method(**kwargs).execute()\n",
    "        except HttpError as e:\n",
    "            status = getattr(e, \"status_code\", None)\n",
    "            \n",
    "            if e.resp.status in (403, 429, 500, 503):\n",
    "                attempts += 1\n",
    "                if attempts > 6:\n",
    "                    raise\n",
    "                _sleep_backoff(attempts)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "def load_progress() -> Dict[str, str]:\n",
    "    if PROGRESS_PATH.exists():\n",
    "        try:\n",
    "            return json.loads(PROGRESS_PATH.read_text(encoding=\"utf-8\"))\n",
    "        except Exception:\n",
    "            return {}\n",
    "    return {}\n",
    "\n",
    "def save_progress(prog: Dict[str, str]):\n",
    "    tmp = PROGRESS_PATH.with_suffix(\".json.tmp\")\n",
    "    tmp.write_text(json.dumps(prog, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "    tmp.replace(PROGRESS_PATH)\n",
    "\n",
    "\n",
    "def save_df(df: pd.DataFrame, path: pathlib.Path):\n",
    "    if USE_PARQUET:\n",
    "        try:\n",
    "            df.to_parquet(path, index=False)\n",
    "            return\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] Parquet failed ({e}). Falling back to CSV.\")\n",
    "    df.to_csv(path.with_suffix(\".csv\"), index=False, encoding=\"utf-8\")\n",
    "\n",
    "def load_existing_ids(path: pathlib.Path) -> set:\n",
    "    if not path.exists():\n",
    "        alt = path.with_suffix(\".csv\")\n",
    "        if not alt.exists():\n",
    "            return set()\n",
    "        df = pd.read_csv(alt, usecols=[\"comment_id\"])\n",
    "        return set(df[\"comment_id\"].astype(str))\n",
    "    df = pd.read_parquet(path, columns=[\"comment_id\"])\n",
    "    return set(df[\"comment_id\"].astype(str))\n",
    "\n",
    "\n",
    "def to_dt(s: Optional[str]) -> Optional[dt.datetime]:\n",
    "    if not s: return None\n",
    "    try:\n",
    "        return dt.datetime.fromisoformat(s.replace(\"Z\", \"+00:00\"))\n",
    "    except Exception:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61f44ad-1920-4cac-beb7-c6925815d776",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "\n",
    "CHANNELS_US: Dict[str, str] = {\n",
    "    \"CNN\": \"UCupvZG-5ko_eiXAupbDfxWw\",\n",
    "    \"Fox News\": \"UCXIJgqnII2ZOINSWNOGFThA\",\n",
    "    \"MSNBC\": \"UCaXkIU1QidjPwiAYu6GcHjg\",\n",
    "    \"PBS NewsHour\": \"UC6ZFN9Tx6xh-skXCuRHCDpQ\",\n",
    "    \"ABC News\": \"UCBi2mrWuNuyYy4gbM6fU18Q\",   \n",
    "    \"CBS News\": \"UC8p1vwvWtl6T73JiExfWs1g\",    \n",
    "    \"NBC News\": \"UCeY0bbntWzzVIaj2z3QigXg\",\n",
    "    \"Bloomberg\": \"UCIALMKvObZNtJ6AmdCLP7Lg\",\n",
    "}\n",
    "\n",
    "CHANNELS_EU: Dict[str, str] = {\n",
    "    \"BBC News\": \"UC16niRr50-MSBwiO3YDb3RA\",\n",
    "    \"Sky News\": \"UCoMdktPbSTixAyNGwb-UYkQ\",\n",
    "    \"DW News\": \"UCknLrEdhRCp1aegoMqRaCZg\",\n",
    "    \"euronews (English)\": \"UCSrZ3UV4jOidv8ppoVuvW9Q\",\n",
    "    \"France 24 English\": \"UCQfwfsi5VrQ8yKZ-UWmAEFg\",\n",
    "    \"The Guardian\": \"UCIRYBXDze5krPDzAEOxFGVA\",\n",
    "    \"Financial Times\": \"UCoUxsWakJucWg46KW5RsvPw\",  \n",
    "    \"Channel 4 News\": \"UCTrQ7HXWRRxr7OsOtodr2_w\",   \n",
    "}\n",
    "\n",
    "\n",
    "REGION_LABELS = {\n",
    "    **{cid: \"US\" for cid in CHANNELS_US.values()},\n",
    "    **{cid: \"EU\" for cid in CHANNELS_EU.values()},\n",
    "}\n",
    "\n",
    "\n",
    "ALL_CHANNELS: Dict[str, str] = {**CHANNELS_US, **CHANNELS_EU}\n",
    "\n",
    "\n",
    "def get_channel_countries(channel_ids: List[str]) -> Dict[str, Optional[str]]:\n",
    "    countries: Dict[str, Optional[str]] = {}\n",
    "    for i in range(0, len(channel_ids), 50):\n",
    "        resp = _yt_call(\n",
    "            YOUTUBE.channels().list,\n",
    "            part=\"snippet,brandingSettings\",\n",
    "            id=\",\".join(channel_ids[i:i+50])\n",
    "        )\n",
    "        for it in resp.get(\"items\", []):\n",
    "            cid = it[\"id\"]\n",
    "            country = (it.get(\"snippet\", {}) or {}).get(\"country\") \\\n",
    "                      or (it.get(\"brandingSettings\", {}).get(\"channel\", {}) or {}).get(\"country\")\n",
    "            countries[cid] = country\n",
    "    return countries\n",
    "\n",
    "CHANNEL_COUNTRIES = get_channel_countries(list(REGION_LABELS.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f81b37-0049-42d8-85c7-6c806eb6bd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "print(f\"US channels: {len(CHANNELS_US)} | EU channels: {len(CHANNELS_EU)} | Total: {len(ALL_CHANNELS)}\")\n",
    "\n",
    "\n",
    "meta_rows = []\n",
    "for name, cid in ALL_CHANNELS.items():\n",
    "    meta_rows.append({\n",
    "        \"channel_name\": name,\n",
    "        \"channel_id\": cid,\n",
    "        \"region_group\": REGION_LABELS.get(cid, \"UNK\"),\n",
    "        \"api_country\": CHANNEL_COUNTRIES.get(cid)  \n",
    "    })\n",
    "df_channels = pd.DataFrame(meta_rows).sort_values([\"region_group\",\"channel_name\"])\n",
    "display(df_channels)\n",
    "\n",
    "\n",
    "unknown_regions = [r for r in meta_rows if r[\"region_group\"] == \"UNK\"]\n",
    "if unknown_regions:\n",
    "    print(\"[warn] Some channels missing region labels:\", [r[\"channel_name\"] for r in unknown_regions])\n",
    "\n",
    "missing_country = [r[\"channel_name\"] for r in meta_rows if r[\"api_country\"] in (None, \"\")]\n",
    "if missing_country:\n",
    "    print(\"[note] These channels don’t expose a country in the API (normal):\", missing_country)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd4e12-d9c5-4adc-8209-7a84932fc229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "DATE_FROM = \"2010-01-01\"\n",
    "DATE_TO   = dt.datetime.now(dt.timezone.utc).date().isoformat()\n",
    "\n",
    "\n",
    "KEYWORD_PAGES = 1   \n",
    "\n",
    "CLIMATE_KEYWORDS = [\n",
    "    \"global warming\",\n",
    "    \"climate change\",\n",
    "    \"climate crisis\",\n",
    "    \"climate emergency\",\n",
    "    \"global heating\",\n",
    "    \"climate heating\",\n",
    "    \"environmental warming\",\n",
    "    \"global temperature\",\n",
    "    \"greenhouse effect\",\n",
    "    \"greenhouse impact\",\n",
    "    \"paris agreement\",\n",
    "    \"paris climate accord\",\n",
    "    \"carbon neutrality\",\n",
    "    \"carbon emissions\",\n",
    "    \"paris climate agreement\"\n",
    "]\n",
    "\n",
    "\n",
    "_KEYWORDS_LC = [k.lower() for k in CLIMATE_KEYWORDS]\n",
    "def is_climate_text(text: str | None) -> bool:\n",
    "    if not text:\n",
    "        return False\n",
    "    t = text.lower()\n",
    "    return any(k in t for k in _KEYWORDS_LC)\n",
    "\n",
    "\n",
    "def _rfc3339_start(date_str: str) -> str:\n",
    "    return f\"{date_str}T00:00:00Z\"\n",
    "\n",
    "def _rfc3339_end(date_str: str) -> str:\n",
    "    \n",
    "    return f\"{date_str}T23:59:59Z\"\n",
    "\n",
    "\n",
    "def get_uploads_playlist_id(channel_id: str) -> str | None:\n",
    "    resp = _yt_call(YOUTUBE.channels().list, part=\"contentDetails\", id=channel_id)\n",
    "    items = resp.get(\"items\", [])\n",
    "    if not items:\n",
    "        return None\n",
    "    return items[0][\"contentDetails\"][\"relatedPlaylists\"][\"uploads\"]\n",
    "\n",
    "def list_channel_video_ids_with_dates(channel_id: str) -> list[tuple[str, str | None]]:\n",
    "    \"\"\"\n",
    "    Enumerate (video_id, publishedAt) for all uploads via playlistItems.\n",
    "    Uses contentDetails.videoPublishedAt when present, else snippet.publishedAt.\n",
    "    \"\"\"\n",
    "    uploads = get_uploads_playlist_id(channel_id)\n",
    "    if not uploads:\n",
    "        return []\n",
    "    out, token = [], None\n",
    "    while True:\n",
    "        resp = _yt_call(\n",
    "            YOUTUBE.playlistItems().list,\n",
    "            part=\"contentDetails,snippet\",   \n",
    "            playlistId=uploads,\n",
    "            maxResults=50,\n",
    "            pageToken=token\n",
    "        )\n",
    "        for it in resp.get(\"items\", []):\n",
    "            cd = it.get(\"contentDetails\", {}) or {}\n",
    "            sn = it.get(\"snippet\", {}) or {}\n",
    "            vid = cd.get(\"videoId\")\n",
    "            vdate = cd.get(\"videoPublishedAt\") or sn.get(\"publishedAt\")\n",
    "            if vid:\n",
    "                out.append((vid, vdate))\n",
    "        token = resp.get(\"nextPageToken\")\n",
    "        if not token:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def batch_get_video_meta(video_ids: list[str]) -> list[dict]:\n",
    "    out = []\n",
    "    for i in range(0, len(video_ids), 50):\n",
    "        resp = _yt_call(\n",
    "            YOUTUBE.videos().list,\n",
    "            part=\"snippet,statistics\",\n",
    "            id=\",\".join(video_ids[i:i+50])\n",
    "        )\n",
    "        out.extend(resp.get(\"items\", []))\n",
    "    return out\n",
    "\n",
    "\n",
    "def _search_channel_for_keyword(\n",
    "    channel_id: str,\n",
    "    keyword: str,\n",
    "    date_from: str,\n",
    "    date_to: str,\n",
    "    max_pages: int = 1,\n",
    "    order: str = \"date\"\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Run a channel-scoped keyword search in a date window.\n",
    "    Returns a list of {'videoId','publishedAt','title'} (snippet-level).\n",
    "    \"\"\"\n",
    "    out, token, pages = [], None, 0\n",
    "    while True:\n",
    "        resp = _yt_call(\n",
    "            YOUTUBE.search().list,\n",
    "            part=\"snippet\",\n",
    "            channelId=channel_id,\n",
    "            q=keyword,\n",
    "            type=\"video\",\n",
    "            maxResults=50,\n",
    "            order=order,  \n",
    "            publishedAfter=_rfc3339_start(date_from),\n",
    "            publishedBefore=_rfc3339_end(date_to),\n",
    "            pageToken=token\n",
    "        )\n",
    "        pages += 1\n",
    "        for it in resp.get(\"items\", []):\n",
    "            sid = it.get(\"id\", {}) or {}\n",
    "            vid = sid.get(\"videoId\")\n",
    "            sn  = it.get(\"snippet\", {}) or {}\n",
    "            if vid:\n",
    "                out.append({\n",
    "                    \"videoId\": vid,\n",
    "                    \"publishedAt\": sn.get(\"publishedAt\",\"\"),\n",
    "                    \"title\": sn.get(\"title\",\"\")\n",
    "                })\n",
    "        token = resp.get(\"nextPageToken\")\n",
    "        if not token or pages >= max_pages:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def discover_climate_videos_for_channel(\n",
    "    channel_id: str,\n",
    "    cap_per_channel: int = 60,\n",
    "    date_from: str | None = DATE_FROM,\n",
    "    date_to: str | None = DATE_TO,\n",
    "    verbose: bool | None = False,\n",
    "    per_keyword_pages: int = KEYWORD_PAGES\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Simpler discovery:\n",
    "      - For each keyword, run channel-scoped search.list in [date_from, date_to]\n",
    "      - Merge & dedupe video IDs\n",
    "      - Batch fetch snippet+statistics\n",
    "      - Keep items whose title/desc/tags contain ANY keyword\n",
    "      - Sort by publish date (desc) then comment count; cap to `cap_per_channel`\n",
    "    \"\"\"\n",
    "    if not date_from or not date_to:\n",
    "        \n",
    "        date_from, date_to = \"2000-01-01\", DATE_TO\n",
    "\n",
    "    \n",
    "    cand = {}\n",
    "    for kw in CLIMATE_KEYWORDS:\n",
    "        hits = _search_channel_for_keyword(channel_id, kw, date_from, date_to, max_pages=per_keyword_pages)\n",
    "        for h in hits:\n",
    "            vid = h[\"videoId\"]\n",
    "            \n",
    "            if vid not in cand:\n",
    "                cand[vid] = h\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[discover-simple] channel={channel_id}: candidates_from_search={len(cand)} \"\n",
    "              f\"(keywords={len(CLIMATE_KEYWORDS)}, pages/kw={per_keyword_pages})\")\n",
    "\n",
    "    if not cand:\n",
    "        return []\n",
    "\n",
    "    ids = list(cand.keys())\n",
    "\n",
    "    \n",
    "    metas = batch_get_video_meta(ids)\n",
    "\n",
    "    \n",
    "    selected = []\n",
    "    for it in metas:\n",
    "        sn = it.get(\"snippet\", {}) or {}\n",
    "        title = sn.get(\"title\",\"\")\n",
    "        desc  = sn.get(\"description\",\"\")\n",
    "        tags  = \" \".join(sn.get(\"tags\", []) or [])\n",
    "        if is_climate_text(title) or is_climate_text(desc) or is_climate_text(tags):\n",
    "            stats = it.get(\"statistics\", {}) or {}\n",
    "            selected.append({\n",
    "                \"video_id\": it[\"id\"],\n",
    "                \"video_title\": title,\n",
    "                \"video_publish_date\": sn.get(\"publishedAt\",\"\"),\n",
    "                \"video_comment_count\": int(stats.get(\"commentCount\", 0) or 0),\n",
    "                \"video_view_count\": int(stats.get(\"viewCount\", 0) or 0),\n",
    "            })\n",
    "\n",
    "    \n",
    "    selected.sort(key=lambda r: (r[\"video_publish_date\"], r[\"video_comment_count\"]), reverse=True)\n",
    "    out = selected[:cap_per_channel]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[discover-simple] channel={channel_id}: kept={len(out)} (from {len(selected)} filtered, \"\n",
    "              f\"{len(cand)} raw), window={date_from}→{date_to}\")\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7571651-b363-4708-9aff-1b56e4bb2dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_DISCOVERY = True\n",
    "print(\"DATE_FROM → DATE_TO:\", DATE_FROM, \"→\", DATE_TO)\n",
    "\n",
    "\n",
    "tests = [\n",
    "    \"Climate change is accelerating; IPCC report 2023\",\n",
    "    \"Global Warming hoax? debate\",\n",
    "    \"We installed solar and heat pumps at home\",\n",
    "    \"Sports highlights from last night\"\n",
    "]\n",
    "for t in tests:\n",
    "    print(f\"{t!r} -> {is_climate_text(t)}\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def list_channel_video_ids_with_dates_head(channel_id: str, max_pages: int = 2):\n",
    "    uploads = get_uploads_playlist_id(channel_id)\n",
    "    if not uploads:\n",
    "        return []\n",
    "    out, token, pages = [], None, 0\n",
    "    while True:\n",
    "        resp = _yt_call(\n",
    "            YOUTUBE.playlistItems().list,\n",
    "            part=\"contentDetails\",\n",
    "            playlistId=uploads,\n",
    "            maxResults=50,\n",
    "            pageToken=token\n",
    "        )\n",
    "        pages += 1\n",
    "        for it in resp.get(\"items\", []):\n",
    "            cd = it.get(\"contentDetails\", {}) or {}\n",
    "            out.append((cd.get(\"videoId\"), cd.get(\"videoPublishedAt\") or cd.get(\"publishedAt\")))\n",
    "        token = resp.get(\"nextPageToken\")\n",
    "        if not token or pages >= max_pages:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def quick_preview_one_channel(channel_name: str, head_pages: int = 2, cap_per_channel: int = 8):\n",
    "    cid = ALL_CHANNELS[channel_name]\n",
    "    pairs = list_channel_video_ids_with_dates_head(cid, max_pages=head_pages)\n",
    "    dfp = pd.DataFrame(pairs, columns=[\"video_id\",\"video_publishedAt_raw\"])\n",
    "    dfp[\"video_publishedAt\"] = pd.to_datetime(dfp[\"video_publishedAt_raw\"], utc=True, errors=\"coerce\")\n",
    "\n",
    "    # date-window filter\n",
    "    dfp = dfp[\n",
    "        (dfp[\"video_publishedAt\"] >= pd.to_datetime(DATE_FROM, utc=True))\n",
    "        & (dfp[\"video_publishedAt\"] <= pd.to_datetime(DATE_TO,   utc=True))\n",
    "    ]\n",
    "    ids = dfp[\"video_id\"].dropna().astype(str).tolist()\n",
    "    metas = batch_get_video_meta(ids)\n",
    "\n",
    "    rows = []\n",
    "    for it in metas:\n",
    "        sn = it.get(\"snippet\", {}) or {}\n",
    "        title = sn.get(\"title\",\"\"); desc = sn.get(\"description\",\"\"); tags = \" \".join(sn.get(\"tags\", []) or [])\n",
    "        if is_climate_text(title) or is_climate_text(desc) or is_climate_text(tags):\n",
    "            rows.append({\n",
    "                \"video_id\": it[\"id\"],\n",
    "                \"video_title\": title,\n",
    "                \"video_publish_date\": sn.get(\"publishedAt\",\"\"),\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(f\"[{channel_name}] pages_scanned={head_pages} | uploads_seen={len(dfp)} | climate_hits={len(df)}\")\n",
    "    display(df.head(cap_per_channel))\n",
    "    return df\n",
    "\n",
    "\n",
    "_ = quick_preview_one_channel(\"CNN\", head_pages=8, cap_per_channel=15)\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def _get_channel_titles(channel_ids):\n",
    "    titles = {}\n",
    "    for i in range(0, len(channel_ids), 50):\n",
    "        resp = _yt_call(YOUTUBE.channels().list, part=\"snippet\", id=\",\".join(channel_ids[i:i+50]))\n",
    "        for it in resp.get(\"items\", []):\n",
    "            titles[it[\"id\"]] = it.get(\"snippet\", {}).get(\"title\", \"\")\n",
    "    return titles\n",
    "\n",
    "def _list_upload_ids_head(channel_id: str, head_pages: int = 1):\n",
    "    uploads = get_uploads_playlist_id(channel_id)\n",
    "    if not uploads:\n",
    "        return False, []  \n",
    "    ids, token, pages = [], None, 0\n",
    "    while True:\n",
    "        resp = _yt_call(\n",
    "            YOUTUBE.playlistItems().list,\n",
    "            part=\"contentDetails\",\n",
    "            playlistId=uploads,\n",
    "            maxResults=50,\n",
    "            pageToken=token\n",
    "        )\n",
    "        pages += 1\n",
    "        for it in resp.get(\"items\", []):\n",
    "            cd = it.get(\"contentDetails\", {}) or {}\n",
    "            vid = cd.get(\"videoId\")\n",
    "            if vid:\n",
    "                ids.append(vid)\n",
    "        token = resp.get(\"nextPageToken\")\n",
    "        if not token or pages >= head_pages:\n",
    "            break\n",
    "    return True, ids\n",
    "\n",
    "def quick_all_channel_check(\n",
    "    channels_map,\n",
    "    region_labels,\n",
    "    head_pages: int = 1,\n",
    "    check_climate_head: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    cid_list = list(channels_map.values())\n",
    "    api_titles = _get_channel_titles(cid_list)\n",
    "\n",
    "    rows = []\n",
    "    for name, cid in channels_map.items():\n",
    "        uploads_ok, ids = _list_upload_ids_head(cid, head_pages=head_pages)\n",
    "        uploads_seen = len(ids)\n",
    "\n",
    "        climate_hits_head = None\n",
    "        if check_climate_head and ids:\n",
    "            \n",
    "            resp = _yt_call(YOUTUBE.videos().list, part=\"snippet\", id=\",\".join(ids[:50]))\n",
    "            hits = 0\n",
    "            for it in resp.get(\"items\", []):\n",
    "                sn = it.get(\"snippet\", {}) or {}\n",
    "                title = sn.get(\"title\",\"\")\n",
    "                desc  = sn.get(\"description\",\"\")\n",
    "                tags  = \" \".join(sn.get(\"tags\", []) or [])\n",
    "                if is_climate_text(title) or is_climate_text(desc) or is_climate_text(tags):\n",
    "                    hits += 1\n",
    "            climate_hits_head = hits\n",
    "\n",
    "        rows.append({\n",
    "            \"channel_name_cohort\": name,\n",
    "            \"channel_id\": cid,\n",
    "            \"api_title\": api_titles.get(cid, \"\"),\n",
    "            \"region_group\": region_labels.get(cid, \"UNK\"),\n",
    "            \"uploads_playlist_ok\": uploads_ok,\n",
    "            \"uploads_seen_head\": uploads_seen,\n",
    "            \"climate_hits_head\": climate_hits_head,\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(rows).sort_values([\"region_group\",\"channel_name_cohort\"]).reset_index(drop=True)\n",
    "\n",
    "    \n",
    "    bad_uploads = df[~df[\"uploads_playlist_ok\"]]\n",
    "    print(f\"Channels checked: {len(df)} | with uploads playlist: {len(df) - len(bad_uploads)} | missing uploads: {len(bad_uploads)}\")\n",
    "    if bad_uploads.shape[0]:\n",
    "        print(\"!! These channels returned no uploads playlist (ID likely wrong):\")\n",
    "        display(bad_uploads[[\"channel_name_cohort\",\"channel_id\",\"region_group\",\"api_title\"]])\n",
    "\n",
    "    if check_climate_head:\n",
    "        total_hits = int(df[\"climate_hits_head\"].fillna(0).sum())\n",
    "        by_region = (df.groupby(\"region_group\", as_index=False)[\"climate_hits_head\"]\n",
    "                       .sum()\n",
    "                       .rename(columns={\"climate_hits_head\":\"climate_hits_head_sum\"}))\n",
    "        print(f\"\\nHead-sample climate hits (approx, first {head_pages*50} uploads/channel): {total_hits}\")\n",
    "        display(by_region)\n",
    "\n",
    "    print(\"\\nPer-channel snapshot (head sample only):\")\n",
    "    display(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "print(\"== ALL CHANNELS — head_pages=1, climate check on ==\")\n",
    "df_s4_lite_all = quick_all_channel_check(ALL_CHANNELS, REGION_LABELS, head_pages=1, check_climate_head=True)\n",
    "\n",
    "print(\"\\n== US ONLY — head_pages=1 ==\")\n",
    "df_s4_lite_us = quick_all_channel_check(CHANNELS_US, REGION_LABELS, head_pages=1, check_climate_head=True)\n",
    "\n",
    "print(\"\\n== EU ONLY — head_pages=1 ==\")\n",
    "df_s4_lite_eu = quick_all_channel_check(CHANNELS_EU, REGION_LABELS, head_pages=1, check_climate_head=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45780387-8dad-472b-a93d-5a3243a60561",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "from googleapiclient.errors import HttpError\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "\n",
    "def _skip_reason_from_http_error(e: Exception) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Extract a simple reason tag from HttpError content, if recognizable.\n",
    "    \"\"\"\n",
    "    s = \"\"\n",
    "    if hasattr(e, \"content\") and e.content:\n",
    "        try:\n",
    "            s = e.content.decode(\"utf-8\", \"ignore\")\n",
    "        except Exception:\n",
    "            s = str(e)\n",
    "    else:\n",
    "        s = str(e)\n",
    "    for key in (\n",
    "        \"commentsDisabled\",\n",
    "        \"forbidden\",\n",
    "        \"videoNotFound\",\n",
    "        \"processingFailure\",\n",
    "        \"insufficientPermissions\",\n",
    "        \"quotaExceeded\",\n",
    "    ):\n",
    "        if key in s:\n",
    "            return key\n",
    "    return None\n",
    "\n",
    "def get_video_commentability(video_id: str, skip_if_zero: bool = False) -> Tuple[bool, str]:\n",
    "    \"\"\"\n",
    "    Quick preflight: check if a video is likely to accept comment fetches.\n",
    "    Uses videos.list(part='status,statistics').\n",
    "\n",
    "    Heuristics:\n",
    "      - madeForKids -> skip\n",
    "      - statistics.commentCount missing -> likely disabled/hidden -> skip\n",
    "      - if skip_if_zero=True and commentCount == 0 -> skip (otherwise allow)\n",
    "\n",
    "    Returns: (is_commentable, reason)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = _yt_call(\n",
    "            YOUTUBE.videos().list,\n",
    "            part=\"status,statistics\",\n",
    "            id=video_id\n",
    "        )\n",
    "    except HttpError as e:\n",
    "        reason = _skip_reason_from_http_error(e) or \"video_api_error\"\n",
    "        return False, reason\n",
    "\n",
    "    items = resp.get(\"items\", [])\n",
    "    if not items:\n",
    "        return False, \"video_not_found\"\n",
    "\n",
    "    v = items[0]\n",
    "    status = v.get(\"status\", {}) or {}\n",
    "    stats  = v.get(\"statistics\", {}) or {}\n",
    "\n",
    "    if status.get(\"madeForKids\") or status.get(\"selfDeclaredMadeForKids\"):\n",
    "        return False, \"made_for_kids\"\n",
    "\n",
    "    if \"commentCount\" not in stats:\n",
    "        \n",
    "        return False, \"comments_disabled_or_hidden\"\n",
    "\n",
    "    if skip_if_zero and int(stats.get(\"commentCount\", 0) or 0) == 0:\n",
    "        return False, \"no_comments\"\n",
    "\n",
    "    return True, \"ok\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fetch_comments_for_video(\n",
    "    video_id: str,\n",
    "    since_iso: Optional[str] = None,\n",
    "    include_replies: bool = False,\n",
    "    max_top_level: Optional[int] = None,\n",
    "    order: str = \"time\",\n",
    "    precheck: bool = True,\n",
    "    skip_if_zero: bool = False,\n",
    ") -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Fetch top-level comments (and optionally replies) for a video (newest-first by default).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    video_id : str\n",
    "        Target video ID.\n",
    "    since_iso : ISO8601 or None\n",
    "        If provided, keep comments with publishedAt > since_iso (used for incremental runs).\n",
    "    include_replies : bool\n",
    "        If True, fetch all replies for each top-level comment (adds quota).\n",
    "    max_top_level : int or None\n",
    "        Cap on number of top-level comments to fetch per video (replies not capped).\n",
    "    order : str\n",
    "        'time' (newest first) or 'relevance' (YouTube sorting). Defaults to 'time'.\n",
    "    precheck : bool\n",
    "        If True, preflight with videos.list(status,statistics) to skip known-bad videos early.\n",
    "    skip_if_zero : bool\n",
    "        With precheck, if True and commentCount==0, skip the video.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict]\n",
    "        Each dict includes:\n",
    "            comment_id, parent_id (None for top-level), comment_text, like_count,\n",
    "            comment_published_at, author_channel_id\n",
    "    \"\"\"\n",
    "    \n",
    "    if precheck:\n",
    "        ok, why = get_video_commentability(video_id, skip_if_zero=skip_if_zero)\n",
    "        if not ok:\n",
    "            print(f\"[skip-pre] {video_id}: {why}\")\n",
    "            return []\n",
    "\n",
    "    rows: List[Dict] = []\n",
    "    page_token: Optional[str] = None\n",
    "    fetched_top = 0\n",
    "    cutoff_dt = to_dt(since_iso) if since_iso else None\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            resp = _yt_call(\n",
    "                YOUTUBE.commentThreads().list,\n",
    "                part=\"snippet\",\n",
    "                videoId=video_id,\n",
    "                maxResults=100,\n",
    "                order=order,\n",
    "                textFormat=\"plainText\",\n",
    "                pageToken=page_token\n",
    "            )\n",
    "        except HttpError as e:\n",
    "            reason = _skip_reason_from_http_error(e)\n",
    "            if reason:\n",
    "                print(f\"[skip] threads {video_id}: {reason}\")\n",
    "                return rows\n",
    "            raise\n",
    "\n",
    "        items = resp.get(\"items\", [])\n",
    "        if not items:\n",
    "            break\n",
    "\n",
    "        for th in items:\n",
    "            top = th[\"snippet\"][\"topLevelComment\"]\n",
    "            csn = top[\"snippet\"]\n",
    "            top_id = top[\"id\"]\n",
    "            published = csn.get(\"publishedAt\", \"\")\n",
    "            p_dt = to_dt(published)\n",
    "\n",
    "            \n",
    "            if cutoff_dt and p_dt and p_dt <= cutoff_dt:\n",
    "                continue\n",
    "\n",
    "            rows.append({\n",
    "                \"comment_id\": top_id,\n",
    "                \"parent_id\": None,\n",
    "                \"comment_text\": csn.get(\"textDisplay\") or csn.get(\"textOriginal\", \"\"),\n",
    "                \"like_count\": int(csn.get(\"likeCount\", 0) or 0),\n",
    "                \"comment_published_at\": published,\n",
    "                \"author_channel_id\": (csn.get(\"authorChannelId\") or {}).get(\"value\"),\n",
    "            })\n",
    "            fetched_top += 1\n",
    "\n",
    "            \n",
    "            if include_replies and th[\"snippet\"].get(\"totalReplyCount\", 0) > 0:\n",
    "                rp_token = None\n",
    "                while True:\n",
    "                    try:\n",
    "                        rresp = _yt_call(\n",
    "                            YOUTUBE.comments().list,\n",
    "                            part=\"snippet\",\n",
    "                            parentId=top_id,\n",
    "                            maxResults=100,\n",
    "                            textFormat=\"plainText\",\n",
    "                            pageToken=rp_token\n",
    "                        )\n",
    "                    except HttpError as e:\n",
    "                        reason = _skip_reason_from_http_error(e)\n",
    "                        if reason:\n",
    "                            print(f\"[skip] replies {video_id}/{top_id}: {reason}\")\n",
    "                            break\n",
    "                        raise\n",
    "\n",
    "                    for r in rresp.get(\"items\", []):\n",
    "                        rsn = r[\"snippet\"]\n",
    "                        r_published = rsn.get(\"publishedAt\", \"\")\n",
    "                        r_dt = to_dt(r_published)\n",
    "                        if cutoff_dt and r_dt and r_dt <= cutoff_dt:\n",
    "                            continue\n",
    "                        rows.append({\n",
    "                            \"comment_id\": r[\"id\"],\n",
    "                            \"parent_id\": top_id,\n",
    "                            \"comment_text\": rsn.get(\"textDisplay\") or rsn.get(\"textOriginal\", \"\"),\n",
    "                            \"like_count\": int(rsn.get(\"likeCount\", 0) or 0),\n",
    "                            \"comment_published_at\": r_published,\n",
    "                            \"author_channel_id\": (rsn.get(\"authorChannelId\") or {}).get(\"value\"),\n",
    "                        })\n",
    "\n",
    "                    rp_token = rresp.get(\"nextPageToken\")\n",
    "                    if not rp_token:\n",
    "                        break\n",
    "\n",
    "            if max_top_level and fetched_top >= max_top_level:\n",
    "                break\n",
    "\n",
    "        if max_top_level and fetched_top >= max_top_level:\n",
    "            break\n",
    "\n",
    "        page_token = resp.get(\"nextPageToken\")\n",
    "        if not page_token:\n",
    "            break\n",
    "\n",
    "    return rows\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def quota_estimate_comments(num_videos: int, avg_top_level_per_video: int = 500, include_replies: bool = False) -> int:\n",
    "    \"\"\"\n",
    "    Very rough units estimate for comment crawling:\n",
    "      ~1 call per 100 top-level comments page, plus ~30% extra if replies on (highly variable).\n",
    "    \"\"\"\n",
    "    pages = max(1, (avg_top_level_per_video + 99) // 100)\n",
    "    total = num_videos * pages\n",
    "    if include_replies:\n",
    "        total = int(total * 1.3)\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061e347d-2ee8-4a65-abad-a1cce40f6706",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_channel = \"CNN\"  \n",
    "test_cid = ALL_CHANNELS[test_channel]\n",
    "\n",
    "\n",
    "one = discover_climate_videos_for_channel(test_cid, cap_per_channel=1, date_from=DATE_FROM, date_to=DATE_TO, verbose=True)\n",
    "assert len(one) >= 1, \"No climate video found in window for test channel.\"\n",
    "vid = one[0][\"video_id\"]\n",
    "print(f\"Testing video: {vid} | {one[0]['video_title']}\")\n",
    "\n",
    "comments = fetch_comments_for_video(vid, since_iso=None, include_replies=False, max_top_level=100)\n",
    "print(f\"Fetched {len(comments)} comments (top-level)\")\n",
    "\n",
    "dfc = pd.DataFrame(comments)\n",
    "if not dfc.empty:\n",
    "    \n",
    "    dfc[\"comment_published_at\"] = pd.to_datetime(dfc[\"comment_published_at\"], errors=\"coerce\", utc=True)\n",
    "    display(dfc.head(10)[[\"comment_id\",\"comment_published_at\",\"like_count\",\"comment_text\"]])\n",
    "else:\n",
    "    print(\"No comments returned (possible if comments are disabled or locked).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5842d4-b5af-4dab-8eb7-1742c2c53656",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Est (units) for 500 top-level comments across 200 videos, replies off:\",\n",
    "      quota_estimate_comments(num_videos=200, avg_top_level_per_video=500, include_replies=False))\n",
    "print(\"Est (units) with replies on (~30% overhead):\",\n",
    "      quota_estimate_comments(num_videos=200, avg_top_level_per_video=500, include_replies=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68fc70b-2894-4a03-9393-7fd548b8fc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def discover_climate_by_year(channel_id: str, start_year=2010, end_year=2025, per_year_cap=6, verbose=False) -> List[dict]:\n",
    "    \"\"\"\n",
    "    Time-balanced discovery: pick up to per_year_cap climate videos per year.\n",
    "    Uses playlistItems -> videos (batch) and Cell C's is_climate_text.\n",
    "    \"\"\"\n",
    "    pairs = list_channel_video_ids_with_dates(channel_id)\n",
    "    if not pairs:\n",
    "        return []\n",
    "    df = pd.DataFrame(pairs, columns=[\"video_id\",\"video_publishedAt_raw\"])\n",
    "    df[\"dt\"] = pd.to_datetime(df[\"video_publishedAt_raw\"], utc=True, errors=\"coerce\")\n",
    "    df[\"year\"] = df[\"dt\"].dt.year\n",
    "\n",
    "    results: List[dict] = []\n",
    "    for y in range(start_year, end_year + 1):\n",
    "        ids = df.loc[df[\"year\"] == y, \"video_id\"].dropna().astype(str).tolist()\n",
    "        if not ids:\n",
    "            continue\n",
    "        metas = batch_get_video_meta(ids)\n",
    "        year_hits = []\n",
    "        for it in metas:\n",
    "            sn = it.get(\"snippet\", {}) or {}\n",
    "            title = sn.get(\"title\",\"\"); desc = sn.get(\"description\",\"\"); tags = \" \".join(sn.get(\"tags\", []) or [])\n",
    "            if is_climate_text(title) or is_climate_text(desc) or is_climate_text(tags):\n",
    "                stats = it.get(\"statistics\", {}) or {}\n",
    "                year_hits.append({\n",
    "                    \"video_id\": it[\"id\"],\n",
    "                    \"video_title\": title,\n",
    "                    \"video_publish_date\": sn.get(\"publishedAt\",\"\"),\n",
    "                    \"video_comment_count\": int(stats.get(\"commentCount\", 0) or 0),\n",
    "                    \"video_view_count\": int(stats.get(\"viewCount\", 0) or 0),\n",
    "                    \"year\": y,\n",
    "                })\n",
    "        \n",
    "        year_hits.sort(key=lambda r: r[\"video_comment_count\"], reverse=True)\n",
    "        kept = year_hits[:per_year_cap]\n",
    "        results.extend(kept)\n",
    "        if verbose and year_hits:\n",
    "            print(f\"[per-year] channel={channel_id} year={y} matches={len(year_hits)} kept={len(kept)}\")\n",
    "    \n",
    "    results.sort(key=lambda r: r[\"video_publish_date\"])\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def build_video_manifest(\n",
    "    channels_map: Dict[str, str],\n",
    "    region_labels: Dict[str, str],\n",
    "    mode: str = \"recent\",                \n",
    "    cap_per_channel: int = 60,           \n",
    "    date_from: Optional[str] = DATE_FROM,\n",
    "    date_to: Optional[str] = DATE_TO,\n",
    "    # per-year options:\n",
    "    start_year: int = 2010,\n",
    "    end_year: int = 2025,\n",
    "    per_year_cap: int = 6,\n",
    "    verbose: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame of videos to crawl with channel + region metadata.\n",
    "    Columns: [video_id, video_title, video_publish_date, video_comment_count, video_view_count,\n",
    "              channel_name, channel_id, region_group]\n",
    "    \"\"\"\n",
    "    discovered: List[dict] = []\n",
    "\n",
    "    for name, cid in channels_map.items():\n",
    "        if mode == \"recent\":\n",
    "            vids = discover_climate_videos_for_channel(\n",
    "                channel_id=cid,\n",
    "                cap_per_channel=cap_per_channel,\n",
    "                date_from=date_from,\n",
    "                date_to=date_to,\n",
    "                verbose=verbose\n",
    "            )\n",
    "        elif mode == \"per_year\":\n",
    "            vids = discover_climate_by_year(\n",
    "                channel_id=cid,\n",
    "                start_year=start_year,\n",
    "                end_year=end_year,\n",
    "                per_year_cap=per_year_cap,\n",
    "                verbose=verbose\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"mode must be 'recent' or 'per_year'.\")\n",
    "\n",
    "        for v in vids:\n",
    "            discovered.append({\n",
    "                \"video_id\": v[\"video_id\"],\n",
    "                \"video_title\": v[\"video_title\"],\n",
    "                \"video_publish_date\": v.get(\"video_publish_date\",\"\"),\n",
    "                \"video_comment_count\": v.get(\"video_comment_count\", 0),\n",
    "                \"video_view_count\": v.get(\"video_view_count\", 0),\n",
    "                \"channel_name\": name,\n",
    "                \"channel_id\": cid,\n",
    "                \"region_group\": region_labels.get(cid, \"UNK\"),\n",
    "            })\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[manifest] {name} ({cid}) -> {len(vids)} videos\")\n",
    "\n",
    "    df = pd.DataFrame(discovered)\n",
    "    \n",
    "    if not df.empty:\n",
    "        save_df(df, DATA_DIR / f\"manifest_{mode}_{date_from}_{date_to}.parquet\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def run_crawl_from_manifest(\n",
    "    manifest_df: pd.DataFrame,\n",
    "    include_replies: bool = False,\n",
    "    max_top_level_per_video: Optional[int] = None,\n",
    "    out_basename: str = \"climate_comments\",\n",
    "    ignore_progress: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Iterates over the manifest, fetches comments, writes to Parquet/CSV, and updates progress.json\n",
    "    \"\"\"\n",
    "    if manifest_df.empty:\n",
    "        print(\"Manifest is empty — nothing to crawl.\")\n",
    "        return\n",
    "\n",
    "    progress = {} if ignore_progress else load_progress()\n",
    "    all_rows: List[dict] = []\n",
    "\n",
    "    for _, row in manifest_df.iterrows():\n",
    "        vid = row[\"video_id\"]\n",
    "        since_iso = None if ignore_progress else progress.get(vid)\n",
    "        rows = fetch_comments_for_video(\n",
    "            video_id=vid,\n",
    "            since_iso=since_iso,\n",
    "            include_replies=include_replies,\n",
    "            max_top_level=max_top_level_per_video\n",
    "        )\n",
    "        if not rows:\n",
    "            continue\n",
    "\n",
    "        \n",
    "        for r in rows:\n",
    "            r.update({\n",
    "                \"video_id\": vid,\n",
    "                \"video_title\": row[\"video_title\"],\n",
    "                \"video_publish_date\": row[\"video_publish_date\"],\n",
    "                \"channel_id\": row[\"channel_id\"],\n",
    "                \"channel_name\": row[\"channel_name\"],\n",
    "                \"region_group\": row[\"region_group\"],\n",
    "            })\n",
    "\n",
    "        \n",
    "        newest = max((to_dt(r[\"comment_published_at\"]) for r in rows if r.get(\"comment_published_at\")), default=None)\n",
    "        if newest:\n",
    "            progress[vid] = newest.astimezone(dt.timezone.utc).isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "        all_rows.extend(rows)\n",
    "\n",
    "    \n",
    "    if not ignore_progress:\n",
    "        save_progress(progress)\n",
    "\n",
    "    if not all_rows:\n",
    "        print(\"No new comments fetched (up to date or empty).\")\n",
    "        return\n",
    "\n",
    "    \n",
    "    out_path = DATA_DIR / f\"{out_basename}.parquet\"\n",
    "    df = pd.DataFrame(all_rows)\n",
    "    df.drop_duplicates(subset=[\"comment_id\"], inplace=True)\n",
    "    for col in (\"comment_published_at\", \"video_publish_date\"):\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\", utc=True)\n",
    "\n",
    "    existing_ids = load_existing_ids(out_path)\n",
    "    if existing_ids:\n",
    "        before = len(df)\n",
    "        df = df[~df[\"comment_id\"].astype(str).isin(existing_ids)].copy()\n",
    "        print(f\"Deduped vs existing: {before} -> {len(df)} new rows\")\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No new rows to write after dedupe.\")\n",
    "        return\n",
    "\n",
    "    save_df(df, out_path)\n",
    "    print(f\"Wrote {len(df)} rows to {out_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def build_monthly_aggregates(out_basename: str = \"climate_comments\"):\n",
    "    path = DATA_DIR / f\"{out_basename}.parquet\"\n",
    "    alt  = path.with_suffix(\".csv\")\n",
    "    if path.exists():\n",
    "        base = pd.read_parquet(path)\n",
    "    elif alt.exists():\n",
    "        base = pd.read_csv(alt, parse_dates=[\"comment_published_at\",\"video_publish_date\"])\n",
    "    else:\n",
    "        print(\"No comment data found.\")\n",
    "        return None\n",
    "\n",
    "    base[\"comment_published_at\"] = pd.to_datetime(base[\"comment_published_at\"], utc=True, errors=\"coerce\")\n",
    "    base[\"month\"] = base[\"comment_published_at\"].dt.to_period(\"M\")\n",
    "    agg = (base\n",
    "           .dropna(subset=[\"month\"])\n",
    "           .groupby([\"region_group\", \"month\"], as_index=False)\n",
    "           .agg(\n",
    "               comments=(\"comment_id\",\"count\"),\n",
    "               unique_videos=(\"video_id\",\"nunique\"),\n",
    "               unique_channels=(\"channel_id\",\"nunique\"),\n",
    "               total_likes=(\"like_count\",\"sum\"),\n",
    "           ))\n",
    "    agg[\"month_start\"] = agg[\"month\"].dt.to_timestamp()\n",
    "    save_df(agg, DATA_DIR / f\"{out_basename}_monthly.parquet\")\n",
    "    print(\"Monthly aggregate preview:\")\n",
    "    display(agg.sort_values([\"region_group\",\"month_start\"]).head(12))\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fe5dde-1901-4d25-8e97-787b10faa2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"EU\"          \n",
    "MODE   = \"recent\"      \n",
    "\n",
    "CAP_PER_CHANNEL = 3    \n",
    "DATE_FROM_TEST  = DATE_FROM\n",
    "DATE_TO_TEST    = DATE_TO\n",
    "START_YEAR      = 2010\n",
    "END_YEAR        = 2025\n",
    "\n",
    "# pick channels by region\n",
    "if REGION.upper() == \"US\":\n",
    "    CHMAP = CHANNELS_US\n",
    "elif REGION.upper() == \"EU\":\n",
    "    CHMAP = CHANNELS_EU\n",
    "else:\n",
    "    CHMAP = ALL_CHANNELS\n",
    "\n",
    "\n",
    "if MODE == \"recent\":\n",
    "    manifest = build_video_manifest(\n",
    "        channels_map=CHMAP,\n",
    "        region_labels=REGION_LABELS,\n",
    "        mode=\"recent\",\n",
    "        cap_per_channel=CAP_PER_CHANNEL,\n",
    "        date_from=DATE_FROM_TEST, date_to=DATE_TO_TEST,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    manifest = build_video_manifest(\n",
    "        channels_map=CHMAP,\n",
    "        region_labels=REGION_LABELS,\n",
    "        mode=\"per_year\",\n",
    "        start_year=START_YEAR, end_year=END_YEAR,\n",
    "        per_year_cap=2,                 \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "print(f\"Manifest size: {len(manifest)} videos\")\n",
    "if not manifest.empty:\n",
    "    display(manifest.head())\n",
    "    print(\"Videos by region:\")\n",
    "    display(manifest.groupby(\"region_group\")[\"video_id\"].count().rename(\"videos\").to_frame())\n",
    "\n",
    "\n",
    "manifest = manifest[manifest[\"video_comment_count\"].fillna(0).astype(int) > 0].copy()\n",
    "print(\"After filtering zero-comment videos:\", len(manifest))\n",
    "\n",
    "\n",
    "def _slug(s): \n",
    "    return str(s).replace(\" \", \"\").replace(\":\", \"\").replace(\"-\", \"\")\n",
    "OUT_BASENAME = f\"climate_comments_{REGION.upper()}_{MODE}_{_slug(DATE_FROM_TEST)}_{_slug(DATE_TO_TEST)}\" \\\n",
    "               if MODE==\"recent\" else f\"climate_comments_{REGION.upper()}_{MODE}_{START_YEAR}_{END_YEAR}\"\n",
    "\n",
    "\n",
    "run_crawl_from_manifest(\n",
    "    manifest_df=manifest,\n",
    "    include_replies=False,\n",
    "    max_top_level_per_video=150,   \n",
    "    out_basename=OUT_BASENAME,\n",
    "    ignore_progress=False\n",
    ")\n",
    "\n",
    "_ = build_monthly_aggregates(out_basename=OUT_BASENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3165d5-2592-493f-954e-ce7858364e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"US\"          \n",
    "MODE   = \"recent\"      \n",
    "\n",
    "CAP_PER_CHANNEL = 3    \n",
    "DATE_FROM_TEST  = DATE_FROM\n",
    "DATE_TO_TEST    = DATE_TO\n",
    "START_YEAR      = 2010\n",
    "END_YEAR        = 2025\n",
    "\n",
    "\n",
    "if REGION.upper() == \"US\":\n",
    "    CHMAP = CHANNELS_US\n",
    "elif REGION.upper() == \"EU\":\n",
    "    CHMAP = CHANNELS_EU\n",
    "else:\n",
    "    CHMAP = ALL_CHANNELS\n",
    "\n",
    "\n",
    "if MODE == \"recent\":\n",
    "    manifest = build_video_manifest(\n",
    "        channels_map=CHMAP,\n",
    "        region_labels=REGION_LABELS,\n",
    "        mode=\"recent\",\n",
    "        cap_per_channel=CAP_PER_CHANNEL,\n",
    "        date_from=DATE_FROM_TEST, date_to=DATE_TO_TEST,\n",
    "        verbose=True\n",
    "    )\n",
    "else:\n",
    "    manifest = build_video_manifest(\n",
    "        channels_map=CHMAP,\n",
    "        region_labels=REGION_LABELS,\n",
    "        mode=\"per_year\",\n",
    "        start_year=START_YEAR, end_year=END_YEAR,\n",
    "        per_year_cap=2,                 \n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "print(f\"Manifest size: {len(manifest)} videos\")\n",
    "if not manifest.empty:\n",
    "    display(manifest.head())\n",
    "    print(\"Videos by region:\")\n",
    "    display(manifest.groupby(\"region_group\")[\"video_id\"].count().rename(\"videos\").to_frame())\n",
    "\n",
    "\n",
    "manifest = manifest[manifest[\"video_comment_count\"].fillna(0).astype(int) > 0].copy()\n",
    "print(\"After filtering zero-comment videos:\", len(manifest))\n",
    "\n",
    "\n",
    "def _slug(s): \n",
    "    return str(s).replace(\" \", \"\").replace(\":\", \"\").replace(\"-\", \"\")\n",
    "OUT_BASENAME = f\"climate_comments_{REGION.upper()}_{MODE}_{_slug(DATE_FROM_TEST)}_{_slug(DATE_TO_TEST)}\" \\\n",
    "               if MODE==\"recent\" else f\"climate_comments_{REGION.upper()}_{MODE}_{START_YEAR}_{END_YEAR}\"\n",
    "\n",
    "\n",
    "run_crawl_from_manifest(\n",
    "    manifest_df=manifest,\n",
    "    include_replies=False,\n",
    "    max_top_level_per_video=150,    \n",
    "    out_basename=OUT_BASENAME,\n",
    "    ignore_progress=False\n",
    ")\n",
    "\n",
    "\n",
    "_ = build_monthly_aggregates(out_basename=OUT_BASENAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17231a49-ae86-420e-be80-2e15ced908f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comment collection by year\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import calendar\n",
    "\n",
    "YEAR = 2024\n",
    "YEAR_FROM = f\"{YEAR}-01-01\"\n",
    "YEAR_TO   = f\"{YEAR}-12-31\"\n",
    "\n",
    "\n",
    "PER_KEYWORD_PAGES = None  \n",
    "\n",
    "def _rfc3339(dt: datetime) -> str:\n",
    "    \n",
    "    if dt.tzinfo is None:\n",
    "        dt = dt.replace(tzinfo=timezone.utc)\n",
    "    return dt.isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "def _quarter_slices(date_from: str, date_to: str):\n",
    "    \"\"\"Generate [start,end] RFC3339 strings per quarter covering [date_from, date_to].\"\"\"\n",
    "    y1, m1, _ = map(int, date_from.split(\"-\"))\n",
    "    y2, m2, _ = map(int, date_to.split(\"-\"))\n",
    "    q_start_m = ((m1 - 1) // 3) * 3 + 1\n",
    "    q_end_m   = ((m2 - 1) // 3) * 3 + 1\n",
    "    y, m = y1, q_start_m\n",
    "    while (y < y2) or (y == y2 and m <= q_end_m):\n",
    "        m_end = m + 2\n",
    "        last_day = calendar.monthrange(y, m_end)[1]\n",
    "        start = datetime(y, m, 1, 0, 0, 0, tzinfo=timezone.utc)\n",
    "        end   = datetime(y, m_end, last_day, 23, 59, 59, tzinfo=timezone.utc)\n",
    "        \n",
    "        start = max(start, datetime.fromisoformat(YEAR_FROM + \"T00:00:00+00:00\"))\n",
    "        end   = min(end,   datetime.fromisoformat(YEAR_TO   + \"T23:59:59+00:00\"))\n",
    "        yield _rfc3339(start), _rfc3339(end)\n",
    "        m += 3\n",
    "        if m > 12:\n",
    "            m = 1\n",
    "            y += 1\n",
    "\n",
    "def _search_channel_window(channel_id: str, keyword: str, start_rfc: str, end_rfc: str,\n",
    "                           per_keyword_pages: int | None) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Channel-scoped search in [start_rfc, end_rfc].\n",
    "    Walks pages up to per_keyword_pages (or all pages if None). Returns [{'videoId','publishedAt','title'}...].\n",
    "    \"\"\"\n",
    "    out, token, pages = [], None, 0\n",
    "    while True:\n",
    "        resp = _yt_call(\n",
    "            YOUTUBE.search().list,\n",
    "            part=\"snippet\",\n",
    "            channelId=channel_id,\n",
    "            q=keyword,\n",
    "            type=\"video\",\n",
    "            maxResults=50,\n",
    "            order=\"date\",\n",
    "            publishedAfter=start_rfc,\n",
    "            publishedBefore=end_rfc,\n",
    "            pageToken=token\n",
    "        )\n",
    "        for it in resp.get(\"items\", []):\n",
    "            sid = it.get(\"id\", {}) or {}\n",
    "            vid = sid.get(\"videoId\")\n",
    "            sn  = it.get(\"snippet\", {}) or {}\n",
    "            if vid:\n",
    "                out.append({\n",
    "                    \"videoId\": vid,\n",
    "                    \"publishedAt\": sn.get(\"publishedAt\",\"\"),\n",
    "                    \"title\": sn.get(\"title\",\"\")\n",
    "                })\n",
    "        token = resp.get(\"nextPageToken\"); pages += 1\n",
    "        if not token:\n",
    "            break\n",
    "        if per_keyword_pages is not None and pages >= per_keyword_pages:\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def discover_climate_videos_for_channel_SEARCH_range(\n",
    "    channel_id: str,\n",
    "    date_from: str = YEAR_FROM,\n",
    "    date_to: str   = YEAR_TO,\n",
    "    verbose: bool = True\n",
    ") -> list[dict]:\n",
    "    \"\"\"\n",
    "    Robust discovery using search.list over quarterly slices to bypass uploads depth issues.\n",
    "    Dedupes across keywords+slices, then verifies via videos.list and your is_climate_text().\n",
    "    \"\"\"\n",
    "    candidates = {}\n",
    "    slices = list(_quarter_slices(date_from, date_to))\n",
    "    for start_rfc, end_rfc in slices:\n",
    "        for kw in CLIMATE_KEYWORDS:\n",
    "            hits = _search_channel_window(channel_id, kw, start_rfc, end_rfc, PER_KEYWORD_PAGES)\n",
    "            for h in hits:\n",
    "                vid = h[\"videoId\"]\n",
    "                if vid not in candidates:\n",
    "                    candidates[vid] = h\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[discover-search] channel={channel_id}: raw_candidates={len(candidates)} \"\n",
    "              f\"(slices={len(slices)}, keywords={len(CLIMATE_KEYWORDS)})\")\n",
    "\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    \n",
    "    metas = batch_get_video_meta(list(candidates.keys()))\n",
    "    start = pd.to_datetime(date_from, utc=True)\n",
    "    end   = pd.to_datetime(date_to,   utc=True)\n",
    "\n",
    "    rows = []\n",
    "    for it in metas:\n",
    "        sn = it.get(\"snippet\", {}) or {}\n",
    "        vid_dt = pd.to_datetime(sn.get(\"publishedAt\", \"\"), utc=True, errors=\"coerce\")\n",
    "        if pd.isna(vid_dt) or not (start <= vid_dt <= end):\n",
    "            continue\n",
    "        title = sn.get(\"title\",\"\")\n",
    "        desc  = sn.get(\"description\",\"\")\n",
    "        tags  = \" \".join(sn.get(\"tags\", []) or [])\n",
    "        if is_climate_text(title) or is_climate_text(desc) or is_climate_text(tags):\n",
    "            stats = it.get(\"statistics\", {}) or {}\n",
    "            rows.append({\n",
    "                \"video_id\": it[\"id\"],\n",
    "                \"video_title\": title,\n",
    "                \"video_publish_date\": sn.get(\"publishedAt\",\"\"),\n",
    "                \"video_comment_count\": int(stats.get(\"commentCount\", 0) or 0),\n",
    "                \"video_view_count\": int(stats.get(\"viewCount\", 0) or 0),\n",
    "            })\n",
    "\n",
    "    rows.sort(key=lambda r: (r[\"video_publish_date\"], r[\"video_comment_count\"]), reverse=True)\n",
    "    if verbose:\n",
    "        print(f\"[discover-search] channel={channel_id}: kept={len(rows)} within window {date_from}→{date_to}\")\n",
    "    return rows\n",
    "\n",
    "def build_manifest_one_year_searchonly(channels_map, region_labels,\n",
    "                                       date_from=YEAR_FROM, date_to=YEAR_TO) -> pd.DataFrame:\n",
    "    discovered = []\n",
    "    for name, cid in channels_map.items():\n",
    "        vids = discover_climate_videos_for_channel_SEARCH_range(cid, date_from, date_to, verbose=True)\n",
    "        for v in vids:\n",
    "            v.update({\"channel_name\": name, \"channel_id\": cid, \"region_group\": region_labels.get(cid, \"UNK\")})\n",
    "            discovered.append(v)\n",
    "    df = pd.DataFrame(discovered)\n",
    "    if not df.empty:\n",
    "        save_df(df, DATA_DIR / f\"manifest_{YEAR}_searchonly.parquet\")\n",
    "    return df\n",
    "\n",
    "manifest_year = build_manifest_one_year_searchonly(ALL_CHANNELS, REGION_LABELS, YEAR_FROM, YEAR_TO)\n",
    "print(f\"\\n=== {YEAR} Manifest summary (search-only) ===\")\n",
    "if manifest_year.empty:\n",
    "    print(\"No videos matched — consider adjusting keywords or increasing PER_KEYWORD_PAGES.\")\n",
    "else:\n",
    "    print(\"Total videos:\", len(manifest_year))\n",
    "    display(manifest_year.groupby(\"region_group\")[\"video_id\"].count().rename(\"videos\").to_frame())\n",
    "\n",
    "manifest_year = manifest_year[manifest_year[\"video_comment_count\"].fillna(0).astype(int) > 0].copy()\n",
    "print(\"After filtering zero-comment videos:\", len(manifest_year))\n",
    "\n",
    "run_crawl_from_manifest(\n",
    "    manifest_df=manifest_year,\n",
    "    include_replies=False,\n",
    "    max_top_level_per_video=10000,           \n",
    "    out_basename=f\"climate_comments_{YEAR}_all\",\n",
    "    ignore_progress=True\n",
    ")\n",
    "\n",
    "_ = build_monthly_aggregates(out_basename=f\"climate_comments_{YEAR}_all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e48816-3496-4108-8f19-b1d2a1e67577",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
