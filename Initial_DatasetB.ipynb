{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7918f-8392-4d92-ae8c-5b58d2e0fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "LABELS = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
    "LABEL_NAMES = [\"negative\", \"neutral\", \"positive\"]  \n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")  \n",
    "\n",
    "\n",
    "class YouTubeCommentDataset(Dataset):\n",
    "    def __init__(self, comments, labels, tokenizer, max_len=128):\n",
    "        self.comments = comments\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        comment = self.comments[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            str(comment),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "    def forward(self, pred, target):\n",
    "        n_class = pred.size(1)\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, target.unsqueeze(1), 1)\n",
    "        smooth_one_hot = one_hot * (1 - self.smoothing) + self.smoothing / n_class\n",
    "        return F.kl_div(F.log_softmax(pred, 1), smooth_one_hot, reduction='batchmean')\n",
    "\n",
    "\n",
    "class BERTMultiKernelCNN(nn.Module):\n",
    "    def __init__(self, n_classes=3, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained('cardiffnlp/twitter-roberta-base-sentiment-latest')\n",
    "        self.bert.gradient_checkpointing_enable()\n",
    "\n",
    "        self.kernel_sizes = [2, 3, 4, 5]\n",
    "        self.num_filters = 128\n",
    "\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(in_channels=768, out_channels=self.num_filters, kernel_size=k),\n",
    "                nn.BatchNorm1d(self.num_filters),\n",
    "                nn.ReLU(),\n",
    "                nn.AdaptiveMaxPool1d(1)\n",
    "            ) for k in self.kernel_sizes\n",
    "        ])\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        total_filters = len(self.kernel_sizes) * self.num_filters\n",
    "        self.fc1 = nn.Linear(total_filters, 128)\n",
    "        self.fc2 = nn.Linear(128, n_classes)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for module in [self.fc1, self.fc2]:\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        x = self.bert(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  \n",
    "        x = x.permute(0, 2, 1)  \n",
    "        conv_outputs = [conv(x) for conv in self.convs]  \n",
    "        x = torch.cat(conv_outputs, dim=1).squeeze(2)  \n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a149d65-87d5-4840-b866-4df82251a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "CSV_PATH = \"Initial_DatasetB.csv\" \n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH, encoding=\"utf-8\", engine=\"python\")\n",
    "\n",
    "\n",
    "text_candidates  = [\"message\",\"comment\",\"comments\",\"text\",\"content\",\"body\",\"clean_text\",\"Comment\",\"Text\"]\n",
    "label_candidates = [\"sentiment\",\"label\",\"labels\",\"polarity\",\"sentimentlabel\",\"Sentiment\",\"Label\",\"Polarity\"]\n",
    "\n",
    "def pick_col(cands, cols):\n",
    "    cols_lower = {c.lower(): c for c in cols}\n",
    "    for c in cands:\n",
    "        if c.lower() in cols_lower:\n",
    "            return cols_lower[c.lower()]\n",
    "    raise ValueError(f\"Could not find any of {cands} in columns: {list(cols)}\")\n",
    "\n",
    "TEXT_COL  = pick_col(text_candidates,  df.columns)\n",
    "LABEL_COL = pick_col(label_candidates, df.columns)\n",
    "\n",
    "\n",
    "df = df[df[TEXT_COL].notnull()]\n",
    "df = df[df[TEXT_COL].astype(str).str.strip() != \"\"]\n",
    "\n",
    "\n",
    "raw = df[LABEL_COL]\n",
    "\n",
    "def normalize_label_series(s):\n",
    "\n",
    "    s_str = s.astype(str).str.strip().str.lower()\n",
    "\n",
    "\n",
    "    map_str = {\n",
    "        \"negative\": 0, \"neg\": 0, \"-1\": 0, \"0 (negative)\": 0,\n",
    "        \"neutral\":  1, \"neu\": 1, \"0\": 1, \"neutral/other\": 1,\n",
    "        \"positive\": 2, \"pos\": 2, \"1\": 2, \"2\": 2, \"0 (positive)\": 2\n",
    "    }\n",
    "\n",
    "\n",
    "    mapped = s_str.map(map_str)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if mapped.isna().any():\n",
    "\n",
    "        s_num = pd.to_numeric(s, errors=\"coerce\")\n",
    "        unique = sorted([int(x) for x in s_num.dropna().unique().tolist()])\n",
    "\n",
    "        if set(unique) == {-1, 0, 1}:\n",
    "            num_map = {-1: 0, 0: 1, 1: 2}\n",
    "            mapped  = s_num.map(num_map)\n",
    "        elif set(unique) == {0, 1, 2}:\n",
    "            num_map = {0: 0, 1: 1, 2: 2}\n",
    "            mapped  = s_num.map(num_map)\n",
    "        elif set(unique) == {0, 1}:\n",
    "   \n",
    "            num_map = {0: 0, 1: 2}\n",
    "            mapped  = s_num.map(num_map)\n",
    "        else:\n",
    "   \n",
    "            mapped = s_str.apply(\n",
    "                lambda t: 0 if \"neg\" in t else (2 if \"pos\" in t else (1 if \"neu\" in t else np.nan))\n",
    "            )\n",
    "\n",
    "    return mapped.astype(\"Int64\")  \n",
    "\n",
    "df[\"label\"] = normalize_label_series(raw)\n",
    "df = df[df[\"label\"].notnull()].copy()\n",
    "df[\"label\"] = df[\"label\"].astype(int)\n",
    "\n",
    "\n",
    "df = df.rename(columns={TEXT_COL: \"message\"})[[\"message\", \"label\"]]\n",
    "\n",
    "\n",
    "present = sorted(df[\"label\"].unique().tolist())\n",
    "print(\"Classes present (0=neg, 1=neu, 2=pos):\", present)\n",
    "\n",
    "\n",
    "if len(present) >= 2:\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, test_size=0.2, stratify=df['label'], random_state=42\n",
    "    )\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42\n",
    "    )\n",
    "else:\n",
    "\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "\n",
    "print(\"Data split complete (YouTube dataset):\")\n",
    "for name, d in [(\"train\", train_df), (\"val\", val_df), (\"test\", test_df)]:\n",
    "    print(name, d['label'].value_counts().sort_index().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22869ea9-e4a7-47cb-9125-b162e8c0a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "EPOCHS = 30      \n",
    "BATCH  = 64\n",
    "MAXLEN = 128\n",
    "\n",
    "\n",
    "NUM_CLASSES = len(sorted(train_df[\"label\"].unique().tolist()))\n",
    "assert NUM_CLASSES in (2, 3), f\"Unexpected number of classes: {NUM_CLASSES}\"\n",
    "TARGET_NAMES_FULL = [\"negative\", \"neutral\", \"positive\"]\n",
    "TARGET_NAMES_USED = TARGET_NAMES_FULL[:NUM_CLASSES] if NUM_CLASSES == 3 else [\"negative\", \"positive\"]\n",
    "\n",
    "\n",
    "train_dataset = YouTubeCommentDataset(\n",
    "    comments=train_df['message'].tolist(),\n",
    "    labels=train_df['label'].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAXLEN\n",
    ")\n",
    "val_dataset = YouTubeCommentDataset(\n",
    "    comments=val_df['message'].tolist(),\n",
    "    labels=val_df['label'].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAXLEN\n",
    ")\n",
    "test_dataset = YouTubeCommentDataset(\n",
    "    comments=test_df['message'].tolist(),\n",
    "    labels=test_df['label'].tolist(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAXLEN\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH, shuffle=True,  num_workers=0)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "model = BERTMultiKernelCNN(n_classes=NUM_CLASSES, dropout=0.4).to(device)\n",
    "\n",
    "\n",
    "for df_ in (train_df, val_df, test_df):\n",
    "    df_['label'] = df_['label'].astype(int)\n",
    "\n",
    "print(\"Train label counts:\", train_df['label'].value_counts().sort_index().to_dict())\n",
    "\n",
    "present_classes = sorted(train_df['label'].unique().tolist())\n",
    "present_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.array(present_classes),\n",
    "    y=train_df['label'].values\n",
    ")\n",
    "\n",
    "full_weights = np.ones(NUM_CLASSES, dtype=np.float32)\n",
    "for c, w in zip(present_classes, present_weights):\n",
    "    if c < NUM_CLASSES:  \n",
    "        full_weights[c] = float(w)\n",
    "class_weights = torch.tensor(full_weights, dtype=torch.float, device=device)\n",
    "print(\"Class weights (final):\", class_weights)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "\n",
    "bert_params, other_params = [], []\n",
    "for name, param in model.named_parameters():\n",
    "    (bert_params if 'bert' in name else other_params).append(param)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': bert_params,  'lr': 1e-5},\n",
    "    {'params': other_params, 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='max', patience=3, factor=0.5\n",
    ")\n",
    "\n",
    "best_val_acc = 0.0\n",
    "epochs_no_improve = 0\n",
    "early_stop_patience = 3\n",
    "best_model_state = None\n",
    "train_acc_list, val_acc_list = [], []\n",
    "\n",
    "print(\"Beginning training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with autocast():\n",
    "            outputs = model(input_ids, attention_mask)      \n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        all_preds.extend(outputs.argmax(dim=1).detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    train_acc = accuracy_score(all_labels, all_preds)\n",
    "    train_acc_list.append(train_acc)\n",
    "    print(f\"Train Loss: {total_loss:.4f} | Accuracy: {train_acc:.4f}\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    val_preds, val_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            val_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_acc = accuracy_score(val_labels, val_preds)\n",
    "    val_acc_list.append(val_acc)\n",
    "    print(f\"\\nValidation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "\n",
    "    present_val = sorted(np.unique(val_labels).tolist())\n",
    "    target_names_val = [TARGET_NAMES_FULL[i] for i in present_val]\n",
    "    print(classification_report(\n",
    "        val_labels, val_preds,\n",
    "        labels=present_val,\n",
    "        target_names=target_names_val,\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    ))\n",
    "\n",
    "    scheduler.step(val_acc)\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        epochs_no_improve = 0\n",
    "        best_model_state = deepcopy(model.state_dict())\n",
    "        print(\"New best model saved.\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        print(f\"No improvement for {epochs_no_improve} epochs.\")\n",
    "        if epochs_no_improve >= early_stop_patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "\n",
    "if best_model_state:\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(\"Best model loaded for final evaluation.\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "test_preds, test_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        test_preds.extend(outputs.argmax(dim=1).cpu().numpy())\n",
    "        test_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(\"\\nFinal Test Set Evaluation:\")\n",
    "present_test = sorted(np.unique(test_labels).tolist())\n",
    "target_names_test = [TARGET_NAMES_FULL[i] for i in present_test]\n",
    "print(classification_report(\n",
    "    test_labels, test_preds,\n",
    "    labels=present_test,\n",
    "    target_names=target_names_test,\n",
    "    digits=4,\n",
    "    zero_division=0\n",
    "))\n",
    "\n",
    "\n",
    "labels_for_cm = [0,1] if NUM_CLASSES == 2 else [0,1,2]\n",
    "names_for_cm  = [\"negative\",\"positive\"] if NUM_CLASSES == 2 else [\"negative\",\"neutral\",\"positive\"]\n",
    "cm = confusion_matrix(test_labels, test_preds, labels=labels_for_cm)\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d',\n",
    "            xticklabels=names_for_cm, yticklabels=names_for_cm)\n",
    "plt.title(\"Confusion Matrix (Test)\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "if train_acc_list:\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    plt.plot(train_acc_list, label='Train Acc', marker='o')\n",
    "    plt.plot(val_acc_list, label='Val Acc', marker='s')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training vs Validation Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Training complete. Best Validation Accuracy: {best_val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f559d-3411-4137-a5e5-edc645c7ebbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb85394-22e4-49b5-9ff2-15266808a9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
