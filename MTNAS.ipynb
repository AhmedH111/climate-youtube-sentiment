{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d7918f-8392-4d92-ae8c-5b58d2e0fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, time, json, gc, copy\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from collections import defaultdict  \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "\n",
    "BACKBONE_NAME   = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "MAX_LEN         = 128\n",
    "BATCH           = 32\n",
    "NUM_WORKERS     = 0\n",
    "FREEZE_BACKBONE = True     \n",
    "GRAD_ACC        = 1\n",
    "MIXED_PREC      = True\n",
    "\n",
    "\n",
    "LF_SUBSET_PER_TASK = 14000\n",
    "LF_EPOCHS          = 5\n",
    "LF_VAL_FRAC        = 0.1\n",
    "\n",
    "\n",
    "FINAL_EPOCHS = 5\n",
    "FINAL_LR     = 2e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Evolutionary search hyperparams\n",
    "POP_SIZE   = 10\n",
    "TOURN_SIZE = 5\n",
    "MUT_PRO = 1\n",
    "MUT_AFTER_TRANSFER = 1  \n",
    "CHILDREN_PER_TRANSFER = 12\n",
    "TOPK_REUSE = 4\n",
    "UNIT       = 1.1\n",
    "ALPHA      = 0.1\n",
    "GENERATIONS = 10\n",
    "\n",
    "# Tasks\n",
    "TASKS: List[Dict[str, Any]] = [\n",
    "    {\n",
    "        'name': 'twitter2',\n",
    "        'csv_path': 'training.1600000.processed.noemoticon.csv',  \n",
    "        'format': 'sent140_raw',\n",
    "        'text_col': 'message',\n",
    "        'label_col': 'label',     \n",
    "        'num_classes': 2,\n",
    "        'train_frac': 0.20,       \n",
    "        'lf_subset': 12000,\n",
    "    },\n",
    "    {\n",
    "        'name': 'yt_taskA',\n",
    "        'csv_path': 'taskA_youtube_raw.csv',  \n",
    "        'text_col': 'text',\n",
    "        'label_col': 'label',        \n",
    "        'num_classes': 3,\n",
    "        'train_frac': 1,\n",
    "        'lf_subset': 12000,\n",
    "    },\n",
    "    {\n",
    "        'name': 'yt_taskB',\n",
    "        'csv_path': 'taskB_youtube_raw.csv',  \n",
    "        'text_col': 'text',\n",
    "        'label_col': 'label',\n",
    "        'num_classes': 3,\n",
    "        'train_frac': 1,\n",
    "        'lf_subset': 12000,\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'name': 'twitter_research',\n",
    "        'csv_path': 'twitter_sentiment_data_ResearchPaper.csv',\n",
    "        'text_col': 'message',      \n",
    "        'label_col': 'label',       \n",
    "        'num_classes': 4,           \n",
    "        'train_frac': 1,\n",
    "        'lf_subset': 12000,\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "TOKENIZER = AutoTokenizer.from_pretrained(BACKBONE_NAME, normalization=True, use_fast=False)\n",
    "\n",
    "class Backbone(nn.Module):\n",
    "    def __init__(self, name: str):\n",
    "        super().__init__()\n",
    "        self.model = AutoModel.from_pretrained(name)\n",
    "        self.hidden = self.model.config.hidden_size\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        return out.last_hidden_state  # [B, T, H]\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts: List[str], labels: List[int], tokenizer: AutoTokenizer, max_len: int):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        enc = self.tokenizer(text, truncation=True, padding='max_length', max_length=self.max_len)\n",
    "        item = {k: torch.tensor(v, dtype=torch.long) for k, v in enc.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "# Head search space\n",
    "KERNEL_CHOICES   = [1, 3, 5, 7, 9, 11, 13, 15]\n",
    "FILTER_CHOICES   = [128, 192, 256, 320, 384]\n",
    "LAYERS_CHOICES   = [1, 2, 3, 4]\n",
    "BRANCH_CHOICES   = [1, 2, 3, 4, 5]\n",
    "DILATION_CHOICES = [1, 2, 4, 8, 16]\n",
    "\n",
    "DROPOUT_CHOICES  = [0.1, 0.2, 0.3, 0.5]\n",
    "\n",
    "\n",
    "POOLING_CHOICES  = ['max', 'avg', 'attn', 'gem', 'kmax']\n",
    "KMAX_CHOICES     = [1, 2, 3, 4]\n",
    "GEM_P_CHOICES    = [2.0, 3.0, 4.0]\n",
    "\n",
    "\n",
    "ACT_CHOICES      = ['gelu', 'relu', 'silu', 'mish']\n",
    "NORM_CHOICES     = ['bn', 'none', 'gn8', 'ln']\n",
    "\n",
    "\n",
    "SEP_CHOICES      = [False, True]\n",
    "GROUPS_CHOICES   = [1, 2, 4, 8]\n",
    "\n",
    "\n",
    "SE_RATIO_CHOICES = [0, 4, 8, 16]   \n",
    "\n",
    "\n",
    "RESIDUAL_CHOICES = [False, True]\n",
    "\n",
    "\n",
    "HEADSIZE_CAP = 6000   \n",
    "MIN_F_PER    = 64     \n",
    "\n",
    "def _feasible(filters: int, branches: int) -> bool:\n",
    "    return (filters * branches) <= HEADSIZE_CAP and (filters // max(1, branches)) >= MIN_F_PER\n",
    "\n",
    "SKIP_STATS = defaultdict(int)  \n",
    "\n",
    "def snapshot_skip_stats():\n",
    "    return dict(SKIP_STATS)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class HeadArch:\n",
    "    layers: int\n",
    "    filters: int                     \n",
    "    branches: int                    \n",
    "    kernels: tuple                   \n",
    "    dropout: float\n",
    "    pooling: str\n",
    "    \n",
    "    act: str = 'gelu'\n",
    "    norm: str = 'bn'\n",
    "    dilations: tuple = ()\n",
    "    \n",
    "    kmax_k: int = 1\n",
    "    gem_p: float = 3.0\n",
    "    \n",
    "    sep: bool = False\n",
    "    groups: int = 1\n",
    "    \n",
    "    se_ratio: int = 0\n",
    "    \n",
    "    residual: bool = False\n",
    "\n",
    "    def as_tuple(self):\n",
    "        \n",
    "        dils = tuple(self.dilations) if self.dilations else tuple([1]*self.branches)\n",
    "        return (\n",
    "            self.layers, self.filters, self.branches, tuple(self.kernels),\n",
    "            round(self.dropout, 2), self.pooling, self.act, self.norm, dils,\n",
    "            int(self.kmax_k), float(self.gem_p), bool(self.sep), int(self.groups),\n",
    "            int(self.se_ratio), bool(self.residual)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def random():\n",
    "        for attempts in range(1, 101):\n",
    "            b  = random.choice(BRANCH_CHOICES)\n",
    "            ks = tuple(sorted(random.sample(KERNEL_CHOICES, k=b)))\n",
    "            ds = tuple(random.choice(DILATION_CHOICES) for _ in range(b))\n",
    "            f  = random.choice(FILTER_CHOICES)\n",
    "            arch = HeadArch(\n",
    "                layers=random.choice(LAYERS_CHOICES),\n",
    "                filters=f,\n",
    "                branches=b,\n",
    "                kernels=ks,\n",
    "                dropout=random.choice(DROPOUT_CHOICES),\n",
    "                pooling=random.choice(POOLING_CHOICES),\n",
    "                act='gelu',\n",
    "                norm='bn',\n",
    "                dilations=ds,\n",
    "                kmax_k=random.choice(KMAX_CHOICES),\n",
    "                gem_p=random.choice(GEM_P_CHOICES),\n",
    "                sep=random.choice(SEP_CHOICES),\n",
    "                groups=random.choice(GROUPS_CHOICES),\n",
    "                se_ratio=random.choice(SE_RATIO_CHOICES),\n",
    "                residual=random.choice(RESIDUAL_CHOICES),\n",
    "            )\n",
    "            if _feasible(arch.filters, arch.branches):\n",
    "                SKIP_STATS['random_skips'] += (attempts - 1)\n",
    "                return arch\n",
    "        SKIP_STATS['random_skips'] += 100\n",
    "        \n",
    "        return HeadArch(layers=1, filters=128, branches=1, kernels=(3,), dropout=0.1,\n",
    "                        pooling='max', act='gelu', norm='bn', dilations=(1,))\n",
    "\n",
    "    def mutate(self):\n",
    "        fields = [\n",
    "            'layers', 'filters', 'branches', 'kernels', 'dropout', 'pooling',\n",
    "            'act', 'norm', 'dilations', 'kmax_k', 'gem_p', 'sep', 'groups',\n",
    "            'se_ratio', 'residual'\n",
    "        ]\n",
    "        for attempts in range(1, 51):\n",
    "            f = random.choice(fields)\n",
    "            d = asdict(self)\n",
    "            if f == 'layers':\n",
    "                d['layers'] = random.choice([c for c in LAYERS_CHOICES if c != self.layers])\n",
    "            elif f == 'filters':\n",
    "                d['filters'] = random.choice([c for c in FILTER_CHOICES if c != self.filters])\n",
    "            elif f == 'branches':\n",
    "                new_b = random.choice([c for c in BRANCH_CHOICES if c != self.branches])\n",
    "                d['branches'] = new_b\n",
    "                d['kernels'] = tuple(sorted(random.sample(KERNEL_CHOICES, k=new_b)))\n",
    "                d['dilations'] = tuple(random.choice(DILATION_CHOICES) for _ in range(new_b))\n",
    "            elif f == 'kernels':\n",
    "                d['kernels'] = tuple(sorted(random.sample(KERNEL_CHOICES, k=self.branches)))\n",
    "                if len(self.dilations) != self.branches:\n",
    "                    d['dilations'] = tuple(random.choice(DILATION_CHOICES) for _ in range(self.branches))\n",
    "            elif f == 'dropout':\n",
    "                d['dropout'] = random.choice([c for c in DROPOUT_CHOICES if c != self.dropout])\n",
    "            elif f == 'pooling':\n",
    "                d['pooling'] = random.choice([c for c in POOLING_CHOICES if c != self.pooling])\n",
    "            elif f == 'act':\n",
    "                d['act'] = random.choice([a for a in ACT_CHOICES if a != self.act])\n",
    "            elif f == 'norm':\n",
    "                d['norm'] = random.choice([n for n in NORM_CHOICES if n != self.norm])\n",
    "            elif f == 'dilations':\n",
    "                d['dilations'] = tuple(random.choice(DILATION_CHOICES) for _ in range(self.branches))\n",
    "            elif f == 'kmax_k':\n",
    "                d['kmax_k'] = random.choice([v for v in KMAX_CHOICES if v != self.kmax_k])\n",
    "            elif f == 'gem_p':\n",
    "                d['gem_p'] = random.choice([v for v in GEM_P_CHOICES if v != self.gem_p])\n",
    "            elif f == 'sep':\n",
    "                d['sep'] = not self.sep\n",
    "            elif f == 'groups':\n",
    "                d['groups'] = random.choice([g for g in GROUPS_CHOICES if g != self.groups])\n",
    "            elif f == 'se_ratio':\n",
    "                d['se_ratio'] = random.choice([r for r in SE_RATIO_CHOICES if r != self.se_ratio])\n",
    "            else:  # 'residual'\n",
    "                d['residual'] = not self.residual\n",
    "\n",
    "            cand = HeadArch(**d)\n",
    "            if _feasible(cand.filters, cand.branches):\n",
    "                SKIP_STATS['mutate_skips'] += (attempts - 1)\n",
    "                return cand\n",
    "        SKIP_STATS['mutate_skips'] += 50\n",
    "        return self  \n",
    "\n",
    "\n",
    "def _gn_groups(C: int) -> int:\n",
    "    for g in (8, 4, 2, 1):\n",
    "        if C % g == 0:\n",
    "            return g\n",
    "    return 1\n",
    "\n",
    "def get_act(name: str):\n",
    "    if name == 'relu': return nn.ReLU()\n",
    "    if name == 'silu': return nn.SiLU()\n",
    "    if name == 'mish': return nn.Mish()\n",
    "    return nn.GELU()  \n",
    "\n",
    "def get_norm(name: str, C: int):\n",
    "    if name == 'bn':   return nn.BatchNorm1d(C)\n",
    "    if name == 'gn8':  return nn.GroupNorm(_gn_groups(C), C)\n",
    "    if name == 'ln':   return nn.GroupNorm(1, C)  \n",
    "    return nn.Identity()\n",
    "\n",
    "\n",
    "class GeM(nn.Module):\n",
    "    def __init__(self, p=3.0, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.p = nn.Parameter(torch.tensor(float(p)))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):                 \n",
    "        x = x.clamp(min=self.eps).pow(self.p)\n",
    "        x = x.mean(dim=2)\n",
    "        return x.pow(1.0/self.p)\n",
    "\n",
    "\n",
    "class SE1d(nn.Module):\n",
    "    def __init__(self, C: int, r: int):\n",
    "        super().__init__()\n",
    "        m = max(1, C // r)\n",
    "        self.fc1 = nn.Linear(C, m)\n",
    "        self.fc2 = nn.Linear(m, C)\n",
    "    def forward(self, x):                 \n",
    "        s = x.mean(dim=2)                 \n",
    "        s = F.silu(self.fc1(s))\n",
    "        s = torch.sigmoid(self.fc2(s)).unsqueeze(-1)\n",
    "        return x * s\n",
    "\n",
    "\n",
    "class ResidualBranch(nn.Module):\n",
    "    def __init__(self, core: nn.Module, skip: nn.Module):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "        self.skip = skip  \n",
    "    def forward(self, x):\n",
    "        y = self.core(x)\n",
    "        s = x if isinstance(self.skip, nn.Identity) else self.skip(x)\n",
    "        return y + s\n",
    "\n",
    "\n",
    "class CNNHead(nn.Module):\n",
    "    def __init__(self, hidden: int, num_classes: int, arch: HeadArch):\n",
    "        super().__init__()\n",
    "        self.arch = arch\n",
    "        b = arch.branches\n",
    "        f_total = arch.filters\n",
    "        f_per = max(1, f_total // b)\n",
    "\n",
    "        \n",
    "        self.branches = nn.ModuleList()\n",
    "        for bi, k in enumerate(arch.kernels):\n",
    "            layers = []\n",
    "            in_ch = hidden\n",
    "            \n",
    "            d = arch.dilations[bi] if (hasattr(arch, 'dilations') and len(arch.dilations) > bi) else 1\n",
    "            pad = ((k - 1) * d) // 2\n",
    "\n",
    "            \n",
    "            use_res = getattr(arch, 'residual', False)\n",
    "            skip = None  \n",
    "\n",
    "            for li in range(arch.layers):\n",
    "                \n",
    "                if getattr(arch, 'sep', False):\n",
    "                    \n",
    "                    layers.append(nn.Conv1d(in_ch, in_ch, kernel_size=k, padding=pad, dilation=d, groups=in_ch))\n",
    "                    layers.append(get_act(arch.act))\n",
    "                    layers.append(get_norm(arch.norm, in_ch))\n",
    "                    layers.append(nn.Conv1d(in_ch, f_per, kernel_size=1))\n",
    "                else:\n",
    "                    \n",
    "                    g_ok = 1\n",
    "                    for g in sorted(GROUPS_CHOICES, reverse=True):\n",
    "                        if in_ch % g == 0 and f_per % g == 0:\n",
    "                            g_ok = g\n",
    "                            break\n",
    "                    layers.append(nn.Conv1d(in_ch, f_per, kernel_size=k, padding=pad, dilation=d, groups=g_ok))\n",
    "                layers.append(get_act(arch.act))\n",
    "                layers.append(get_norm(arch.norm, f_per))\n",
    "\n",
    "                \n",
    "                if getattr(arch, 'se_ratio', 0) > 0:\n",
    "                    layers.append(SE1d(f_per, arch.se_ratio))\n",
    "\n",
    "                in_ch = f_per\n",
    "                if use_res and skip is None:\n",
    "                    skip = nn.Identity() if hidden == f_per else nn.Conv1d(hidden, f_per, 1)\n",
    "\n",
    "            branch_core = nn.Sequential(*layers)\n",
    "            if use_res:\n",
    "                self.branches.append(ResidualBranch(branch_core, skip if skip is not None else nn.Identity()))\n",
    "            else:\n",
    "                self.branches.append(branch_core)\n",
    "\n",
    "        concat_ch = f_per * b\n",
    "        self.proj = None\n",
    "        if concat_ch != f_total:\n",
    "            self.proj = nn.Conv1d(concat_ch, f_total, kernel_size=1)\n",
    "\n",
    "        self.dropout = nn.Dropout(arch.dropout)\n",
    "        self.pooling = arch.pooling\n",
    "        if self.pooling == 'attn':\n",
    "            self.attn = nn.Linear(f_total, 1)\n",
    "        elif self.pooling == 'gem':\n",
    "            self.gem = GeM(p=arch.gem_p)\n",
    "\n",
    "        self.out = nn.Linear(f_total, num_classes)\n",
    "\n",
    "    def forward(self, last_hidden: torch.Tensor, attn_mask: torch.Tensor):\n",
    "        \n",
    "        x = last_hidden.transpose(1, 2).contiguous()  \n",
    "\n",
    "        \n",
    "        with torch.amp.autocast('cuda', enabled=False):\n",
    "            x = x.float()\n",
    "\n",
    "            feats_list = [branch(x) for branch in self.branches]   \n",
    "            feats = torch.cat(feats_list, dim=1)                   \n",
    "            if self.proj is not None:\n",
    "                feats = self.proj(feats)                           \n",
    "\n",
    "            if self.pooling == 'max':\n",
    "                x_out = torch.amax(feats, dim=2)                   \n",
    "            elif self.pooling == 'avg':\n",
    "                x_out = torch.mean(feats, dim=2)                   \n",
    "            elif self.pooling == 'gem':\n",
    "                x_out = self.gem(feats)                            \n",
    "            elif self.pooling == 'kmax':\n",
    "                mask = (attn_mask == 0)[:, None, :]                \n",
    "                feats_m = feats.masked_fill(mask, torch.finfo(feats.dtype).min)\n",
    "                k = max(1, int(getattr(self.arch, 'kmax_k', 1)))\n",
    "                vals, _ = torch.topk(feats_m, k, dim=2)            \n",
    "                x_out = vals.mean(dim=2)                           \n",
    "            else:  # 'attn'\n",
    "                feats_T = feats.transpose(1, 2)                    \n",
    "                logits = self.attn(feats_T)                        \n",
    "                mask = (attn_mask == 0).unsqueeze(-1)              \n",
    "                logits = logits.masked_fill(mask, torch.finfo(logits.dtype).min)\n",
    "                w = torch.softmax(logits, dim=1)                   \n",
    "                x_out = torch.sum(feats_T * w, dim=1)              \n",
    "\n",
    "            x_out = self.dropout(x_out)\n",
    "            return self.out(x_out)\n",
    "\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, backbone: Backbone, num_classes: int, head_arch: HeadArch):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = CNNHead(self.backbone.hidden, num_classes, head_arch)\n",
    "        self.backbone_frozen = False\n",
    "    def freeze_backbone(self, freeze: bool = True):\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = not freeze\n",
    "        self.backbone_frozen = freeze\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        if self.backbone_frozen:\n",
    "            with torch.no_grad():\n",
    "                last_hidden = self.backbone(input_ids, attention_mask)\n",
    "            last_hidden = last_hidden.detach()  \n",
    "        else:\n",
    "            last_hidden = self.backbone(input_ids, attention_mask)\n",
    "        return self.head(last_hidden, attention_mask)\n",
    "\n",
    "\n",
    "CROSSOVER_PRO   = 0.30\n",
    "MUT_AFTER_CROSS = 0.40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838e3116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#warm start\n",
    "import json as _json\n",
    "\n",
    "\n",
    "WARMSTART_WINNERS_PATH = globals().get('WARMSTART_WINNERS_PATH', None)\n",
    "WARMSTART_VARIANTS = globals().get('WARMSTART_VARIANTS', 8)\n",
    "WARMSTART_KEEP_WINNER = globals().get('WARMSTART_KEEP_WINNER', True)\n",
    "\n",
    "def _arch_to_dict(a: HeadArch):\n",
    "    return {'layers': a.layers, 'filters': a.filters, 'branches': a.branches,\n",
    "            'kernels': list(a.kernels), 'dropout': a.dropout, 'pooling': a.pooling}\n",
    "\n",
    "def _arch_from_dict(d):\n",
    "    return HeadArch(d['layers'], d['filters'], d['branches'], tuple(d['kernels']), d['dropout'], d['pooling'])\n",
    "\n",
    "def save_winners(best_per_task, path='winners_last.json'):\n",
    "    data = {name: _arch_to_dict(arch) for name, arch in best_per_task.items()}\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        _json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Saved winners to {path}\")\n",
    "\n",
    "def load_winners(path='winners_last.json'):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        data = _json.load(f)\n",
    "    return {name: _arch_from_dict(d) for name, d in data.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a149d65-87d5-4840-b866-4df82251a72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_task_dataframe(task: Dict[str, Any]) -> pd.DataFrame:\n",
    "    fmt = task.get('format', None)\n",
    "\n",
    "    \n",
    "    if fmt == 'sent140_raw':\n",
    "        \n",
    "        df = pd.read_csv(\n",
    "            task['csv_path'],\n",
    "            encoding=\"latin-1\",\n",
    "            header=None,\n",
    "            names=[\"polarity\", \"id\", \"date\", \"query\", \"user\", \"message\"]\n",
    "        )\n",
    "        df = df[df[\"polarity\"].isin([0, 4])].copy()\n",
    "        df[\"label\"] = (df[\"polarity\"] == 4).astype(int)  \n",
    "        df = df[df[\"message\"].notnull()].reset_index(drop=True)\n",
    "        return df[[\"message\", \"label\"]]\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(task['csv_path'], encoding='utf-8', engine='python')\n",
    "\n",
    "    \n",
    "    if task.get('name') == 'twitter_research':\n",
    "        \n",
    "        if 'sentiment' in df.columns and task.get('label_col', 'label') not in df.columns:\n",
    "            label_map = {1: 0, -1: 1, 0: 2, 2: 3}\n",
    "            df[task['label_col']] = df['sentiment'].map(label_map).astype(int)\n",
    "\n",
    "    if task.get('name') == 'youtube3':\n",
    "        \n",
    "        lbl = task.get('label_col', 'label')\n",
    "        if df[lbl].dtype == object:\n",
    "            df[lbl] = (\n",
    "                df[lbl].astype(str).str.lower()\n",
    "                  .map({'negative': 0, 'neutral': 1, 'positive': 2})\n",
    "                  .astype(int)\n",
    "            )\n",
    "\n",
    "    \n",
    "    assert task['text_col'] in df.columns and task['label_col'] in df.columns, \\\n",
    "        f\"Missing columns in {task['csv_path']}\"\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_lf_splits(df: pd.DataFrame, text_col: str, label_col: str,\n",
    "                   subset_n: int, val_frac: float) -> Tuple[List[str], List[int], List[str], List[int]]:\n",
    "    if subset_n is not None and subset_n < len(df):\n",
    "        df = df.sample(subset_n, random_state=SEED)\n",
    "    train_df, val_df = train_test_split(df, test_size=val_frac,\n",
    "                                        stratify=df[label_col], random_state=SEED)\n",
    "    Xtr = train_df[text_col].astype(str).tolist()\n",
    "    ytr = train_df[label_col].astype(int).tolist()\n",
    "    Xva = val_df[text_col].astype(str).tolist()\n",
    "    yva = val_df[label_col].astype(int).tolist()\n",
    "    return Xtr, ytr, Xva, yva\n",
    "\n",
    "def build_loader(texts, labels, batch=None, shuffle=False):\n",
    "    \n",
    "    if batch is None:\n",
    "        batch = BATCH\n",
    "    ds = TextDataset(texts, labels, TOKENIZER, MAX_LEN)\n",
    "    return DataLoader(ds, batch_size=batch, shuffle=shuffle,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29627394-8903-4ac8-bb9c-0fdb8774aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import numpy as np, time, math, hashlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.amp import autocast, GradScaler  \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def nas_reset_vram_peak():\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def _vram_peak_gb():\n",
    "    try:\n",
    "        if torch.cuda.is_available():\n",
    "            return float(torch.cuda.max_memory_reserved() / (1024**3))\n",
    "    except Exception:\n",
    "        pass\n",
    "    return float(\"nan\")\n",
    "\n",
    "@dataclass\n",
    "class ChildEval:\n",
    "    gen: int\n",
    "    task: str\n",
    "    child_id: str\n",
    "    op: str\n",
    "    arch_hash: str\n",
    "    head_desc: str\n",
    "    params_m: float\n",
    "    batch_used: int\n",
    "    vram_peak_gb: float\n",
    "    train_time_s: float\n",
    "    val_acc: float\n",
    "    val_loss: float\n",
    "    lr_head: float = None\n",
    "    lr_backbone: float = None\n",
    "    feasible: bool = True\n",
    "    infeasible_reason: str = \"\"\n",
    "    arch_vec: object = None  \n",
    "\n",
    "class GenLogger:\n",
    "    def __init__(self, topk=3, objective_key=\"val_acc\", maximize=True, stop_hint_k=4):\n",
    "        from collections import defaultdict\n",
    "        self.children = []\n",
    "        self.gen_to_idx = defaultdict(list)\n",
    "        self.best_so_far = None                \n",
    "        self.best_overall_by_hash = None       \n",
    "        self.objective_key = objective_key\n",
    "        self.maximize = maximize\n",
    "        self.clock = time.time()\n",
    "        self.no_improve_streak = 0\n",
    "        self.archive_hashes = set()\n",
    "        self.hash_to_vec = {}\n",
    "        self.topk = int(topk)\n",
    "        self.stop_hint_k = int(stop_hint_k)\n",
    "\n",
    "    def log_child(self, **kw):\n",
    "        \n",
    "        ce = ChildEval(\n",
    "            gen=kw.get(\"gen\"),\n",
    "            task=str(kw.get(\"task\")),\n",
    "            child_id=str(kw.get(\"child_id\")),\n",
    "            op=str(kw.get(\"op\")),\n",
    "            arch_hash=str(kw.get(\"arch_hash\")),\n",
    "            head_desc=str(kw.get(\"head_desc\") or \"\"),\n",
    "            params_m=float(kw.get(\"params_m\", 0.0)),\n",
    "            batch_used=int(kw.get(\"batch_used\", -1)),\n",
    "            vram_peak_gb=_vram_peak_gb(),\n",
    "            train_time_s=float(kw.get(\"train_time_s\", float(\"nan\"))),\n",
    "            val_acc=float(kw.get(\"val_acc\", float(\"nan\"))),\n",
    "            val_loss=float(kw.get(\"val_loss\", float(\"nan\"))),\n",
    "            lr_head=(None if kw.get(\"lr_head\") is None else float(kw.get(\"lr_head\"))),\n",
    "            lr_backbone=(None if kw.get(\"lr_backbone\") is None else float(kw.get(\"lr_backbone\"))),\n",
    "            feasible=bool(kw.get(\"feasible\", True)),\n",
    "            infeasible_reason=str(kw.get(\"infeasible_reason\", \"\")),\n",
    "            arch_vec=(None if kw.get(\"arch_vec\") is None else np.asarray(kw.get(\"arch_vec\"), dtype=float)),\n",
    "        )\n",
    "        idx = len(self.children)\n",
    "        self.children.append(ce)\n",
    "        self.gen_to_idx[ce.gen].append(idx)\n",
    "        if ce.arch_vec is not None and ce.arch_hash not in self.hash_to_vec:\n",
    "            self.hash_to_vec[ce.arch_hash] = ce.arch_vec\n",
    "\n",
    "    def end_gen_summary(self, gen:int, infeasible_counts=None, batch_usage=None, ooms_dict=None,\n",
    "                        survivors_ops_counter=None, new_best_ops_counter=None):\n",
    "        from collections import defaultdict\n",
    "        infeasible_counts = infeasible_counts or {}\n",
    "        batch_usage = batch_usage or {}\n",
    "        ooms_dict = ooms_dict or {}\n",
    "\n",
    "        idxs = self.gen_to_idx.get(gen, [])\n",
    "        feas = [self.children[i] for i in idxs if self.children[i].feasible]\n",
    "        infeas = [self.children[i] for i in idxs if not self.children[i].feasible]\n",
    "\n",
    "        \n",
    "        sc = np.array([c.val_acc for c in feas], dtype=float)\n",
    "        acc_mean = float(np.nanmean(sc)) if sc.size else float(\"nan\")\n",
    "        acc_std  = float(np.nanstd(sc))  if sc.size else float(\"nan\")\n",
    "        p25, p50, p75 = (np.percentile(sc, [25,50,75]).tolist() if sc.size else [float(\"nan\")]*3)\n",
    "\n",
    "        feas_sorted = sorted(feas, key=lambda c: getattr(c, self.objective_key), reverse=self.maximize)\n",
    "        top = feas_sorted[:min(self.topk, len(feas_sorted))]\n",
    "\n",
    "        improved = False\n",
    "        if top:\n",
    "            s0 = getattr(top[0], self.objective_key)\n",
    "            if self.best_so_far is None:\n",
    "                self.best_so_far = (s0, top[0], gen)\n",
    "                improved = True\n",
    "            else:\n",
    "                prev = self.best_so_far[0]\n",
    "                if (s0 > prev if self.maximize else s0 < prev):\n",
    "                    self.best_so_far = (s0, top[0], gen)\n",
    "                    improved = True\n",
    "        self.no_improve_streak = 0 if improved else (self.no_improve_streak + 1)\n",
    "\n",
    "        \n",
    "        from collections import defaultdict as _dd\n",
    "        by_hash = _dd(list)\n",
    "        for c in feas:\n",
    "            by_hash[c.arch_hash].append(c.val_acc)\n",
    "        best_hash, best_hash_mean = None, None\n",
    "        if by_hash:\n",
    "            for h, lst in by_hash.items():\n",
    "                m = float(np.mean(lst)) if lst else float(\"nan\")\n",
    "                if best_hash_mean is None or (m > best_hash_mean if self.maximize else m < best_hash_mean):\n",
    "                    best_hash, best_hash_mean = h, m\n",
    "            if best_hash is not None:\n",
    "                if self.best_overall_by_hash is None:\n",
    "                    self.best_overall_by_hash = (best_hash_mean, best_hash, gen)\n",
    "                else:\n",
    "                    prev_m, _, _ = self.best_overall_by_hash\n",
    "                    if (best_hash_mean > prev_m if self.maximize else best_hash_mean < prev_m):\n",
    "                        self.best_overall_by_hash = (best_hash_mean, best_hash, gen)\n",
    "\n",
    "        \n",
    "        new_hashes = {self.children[i].arch_hash for i in idxs}\n",
    "        seen_before = sum(1 for h in new_hashes if h in self.archive_hashes)\n",
    "        dup_rate = (seen_before / max(1, len(new_hashes))) * 100.0\n",
    "        self.archive_hashes |= new_hashes\n",
    "\n",
    "        novelty_med = float(\"nan\")\n",
    "        if self.hash_to_vec and len(self.hash_to_vec) > 1:\n",
    "            archive = {h:v for h,v in self.hash_to_vec.items()}\n",
    "            dist_list = []\n",
    "            for h in new_hashes:\n",
    "                v = archive.get(h, None)\n",
    "                if v is None: continue\n",
    "                nn = []\n",
    "                for hh, vv in archive.items():\n",
    "                    if hh == h or vv.shape != v.shape: continue\n",
    "                    nn.append(np.sum(np.abs(v - vv)))\n",
    "                if nn: dist_list.append(min(nn))\n",
    "            if dist_list: novelty_med = float(np.median(dist_list))\n",
    "\n",
    "        \n",
    "        vram_vals = [c.vram_peak_gb for c in feas if not math.isnan(c.vram_peak_gb)]\n",
    "        t_vals    = [c.train_time_s  for c in feas if not math.isnan(c.train_time_s)]\n",
    "        vmin = min(vram_vals) if vram_vals else float(\"nan\")\n",
    "        vmed = float(np.median(vram_vals)) if vram_vals else float(\"nan\")\n",
    "        vmax = max(vram_vals) if vram_vals else float(\"nan\")\n",
    "        tmed = float(np.median(t_vals)) if t_vals else float(\"nan\")\n",
    "        tp90 = float(np.percentile(t_vals, 90)) if t_vals else float(\"nan\")\n",
    "\n",
    "        \n",
    "        from collections import defaultdict as _d2\n",
    "        op_counts = _d2(int)\n",
    "        for c in feas: op_counts[c.op] += 1\n",
    "        reason_counts = _d2(int)\n",
    "        for c in infeas: reason_counts[c.infeasible_reason] += 1\n",
    "        top_reasons = sorted(reason_counts.items(), key=lambda kv: kv[1], reverse=True)[:3]\n",
    "\n",
    "        elapsed = time.time() - self.clock\n",
    "        self.clock = time.time()\n",
    "\n",
    "        \n",
    "        print(f\"[Gen {gen}] this_gen={len(idxs)}  feasible={len(feas)}  infeasible={len(infeas)}  \"\n",
    "              f\"dup_rate={dup_rate:.1f}%  novelty_med(L1)={novelty_med if not math.isnan(novelty_med) else 'N/A'}  \"\n",
    "              f\"no_improve_streak={self.no_improve_streak}  elapsed={elapsed:.1f}s\")\n",
    "\n",
    "        accept_rate = (len(feas) / max(1, len(idxs))) * 100.0\n",
    "        def _fmt(x): return f\"{x:.4f}\" if not math.isnan(x) else \"nan\"\n",
    "        print(f\"  Scores: accept={accept_rate:.1f}%  mean={_fmt(acc_mean)} ± {_fmt(acc_std)} | p25/50/75 = {_fmt(p25)}/{_fmt(p50)}/{_fmt(p75)}\")\n",
    "\n",
    "        if top:\n",
    "            print(f\"  Top-{self.topk} (by val_acc)\")\n",
    "            for r, ce in enumerate(top, 1):\n",
    "                print(f\"    #{r} {ce.val_acc:.4f}  task={ce.task}  op={ce.op}  head={ce.head_desc}  \"\n",
    "                      f\"params={ce.params_m:.2f}M  batch={ce.batch_used}  vram={ce.vram_peak_gb:.2f}GB  \"\n",
    "                      f\"time={ce.train_time_s:.1f}s  id={ce.child_id[:8]}  hash={ce.arch_hash[:8]}\")\n",
    "\n",
    "        if self.best_so_far:\n",
    "            best_s, best_ce, best_gen = self.best_so_far\n",
    "            print(f\"  Best-so-far child: {best_s:.4f} (task={best_ce.task}, head={best_ce.head_desc}, op={best_ce.op}, gen={best_gen}, id={best_ce.child_id[:8]})\"\n",
    "                  + (\"  improved\" if improved else \"\"))\n",
    "\n",
    "        if best_hash is not None:\n",
    "            tag = \"\"\n",
    "            if self.best_overall_by_hash and self.best_overall_by_hash[1] == best_hash and self.best_overall_by_hash[2] == gen:\n",
    "                tag = \"  improved overall-by-arch\"\n",
    "            print(f\"  Best arch (this gen, mean across tasks): {best_hash[:8]}  mean_acc={best_hash_mean:.4f}{tag}\")\n",
    "\n",
    "        if survivors_ops_counter:\n",
    "            print(\"  Survivors by operator:\", \", \".join(f\"{k}={v}\" for k,v in survivors_ops_counter.items()))\n",
    "        if new_best_ops_counter:\n",
    "            print(\"  New-best origins:\", \", \".join(f\"{k}={v}\" for k,v in new_best_ops_counter.items()))\n",
    "\n",
    "        if op_counts:\n",
    "            print(\"  Ops (feasible evals):\", \", \".join(f\"{k}={v}\" for k,v in op_counts.items()))\n",
    "        if infeasible_counts:\n",
    "            inf_str = \", \".join(f\"{k}={v}\" for k,v in infeasible_counts.items() if v)\n",
    "            if inf_str: print(\"  Infeasible resamples:\", inf_str)\n",
    "        if top_reasons:\n",
    "            print(\"  Top infeasible reasons:\", \", \".join(f\"{k}={v}\" for k,v in top_reasons))\n",
    "\n",
    "        if batch_usage: print(\"  BATCH_USAGE:\", dict(sorted(batch_usage.items())))\n",
    "        if ooms_dict:   print(\"  OOMs:\", ooms_dict)\n",
    "\n",
    "        print(f\"  VRAM reserved (GB) min/med/max = \"\n",
    "              f\"{('N/A' if math.isnan(vmin) else f'{vmin:.2f}')}/\"\n",
    "              f\"{('N/A' if math.isnan(vmed) else f'{vmed:.2f}')}/\"\n",
    "              f\"{('N/A' if math.isnan(vmax) else f'{vmax:.2f}')}  |  \"\n",
    "              f\"time/child med/p90 = \"\n",
    "              f\"{('N/A' if math.isnan(tmed) else f'{tmed:.1f}')}/\"\n",
    "              f\"{('N/A' if math.isnan(tp90) else f'{tp90:.1f}')}s\")\n",
    "\n",
    "        print(f\"  Archive: {len(self.archive_hashes)} unique architectures  |  \"\n",
    "              f\"Champion age: {0 if self.best_so_far is None else (gen - self.best_so_far[2])} gens  \"\n",
    "              f\"| Stop-hint when no_improve_streak ≥ {self.stop_hint_k}\")\n",
    "\n",
    "\n",
    "def head_summary(a) -> str:\n",
    "    try:\n",
    "        ks = getattr(a, \"kernels\", (getattr(a, \"kernel\", 3),))\n",
    "        ks = tuple(int(k) for k in ks)\n",
    "        return (f\"mkCNN[k={','.join(map(str, ks))}]+layers={getattr(a,'layers', '?')}\"\n",
    "                f\"+filters={getattr(a,'filters','?')}+branches={getattr(a,'branches','?')}\"\n",
    "                f\"+{getattr(a,'pooling','?')}+drop={getattr(a,'dropout','?')}\")\n",
    "    except Exception:\n",
    "        return str(a)\n",
    "\n",
    "def arch_hash(a) -> str:\n",
    "    try:\n",
    "        if hasattr(a, \"as_tuple\"):\n",
    "            t = a.as_tuple()\n",
    "        else:\n",
    "            t = (getattr(a,'layers',None), getattr(a,'filters',None), getattr(a,'branches',None),\n",
    "                 tuple(getattr(a,'kernels',(getattr(a,'kernel',3),))), getattr(a,'dropout',None), getattr(a,'pooling',None))\n",
    "        return hashlib.md5(str(t).encode(\"utf-8\")).hexdigest()\n",
    "    except Exception:\n",
    "        return hashlib.md5(str(a).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EvalResult:\n",
    "    acc: float\n",
    "    f1: float\n",
    "    loss: float\n",
    "\n",
    "@torch.no_grad()\n",
    "def quick_val(model: nn.Module, loader: DataLoader) -> EvalResult:\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    with torch.no_grad():                        \n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += float(loss.detach().cpu())\n",
    "            preds = logits.argmax(dim=-1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_labels.append(labels.detach().cpu())\n",
    "    if total == 0:\n",
    "        return EvalResult(0.0, 0.0, 0.0)\n",
    "    logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    acc = correct / total\n",
    "    f1 = f1_score(labels, logits.argmax(axis=1), average='macro')\n",
    "    return EvalResult(acc=acc, f1=f1, loss=total_loss / max(1, len(all_labels)))\n",
    "\n",
    "def evaluate_arch_on_task(arch: HeadArch, ts, epochs: int = LF_EPOCHS, freeze: bool = FREEZE_BACKBONE) -> float:\n",
    "    \"\"\"Low-fidelity train-once evaluator for an architecture on a given task state.\n",
    "    Returns validation accuracy.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_loader = build_loader(ts.Xtr, ts.ytr, shuffle=True)\n",
    "    val_loader   = build_loader(ts.Xva, ts.yva, shuffle=False)\n",
    "\n",
    "    \n",
    "    bb = Backbone(BACKBONE_NAME)\n",
    "    model = SentimentModel(bb, ts.num_classes, arch)\n",
    "    model.freeze_backbone(freeze)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    \n",
    "    nas_reset_vram_peak()\n",
    "\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-4, weight_decay=0.01)\n",
    "    scaler = GradScaler(enabled=MIXED_PREC)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for _ in range(max(1, epochs)):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with autocast('cuda', enabled=MIXED_PREC):\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "    \n",
    "    res = quick_val(model, val_loader)\n",
    "\n",
    "    \n",
    "    return float(res.acc)\n",
    "\n",
    "\n",
    "def encode_arch(a: HeadArch) -> List[int]:\n",
    "    kernel_idx = max([KERNEL_CHOICES.index(k) for k in a.kernels]) if len(a.kernels) else 0\n",
    "    return [\n",
    "        LAYERS_CHOICES.index(a.layers),\n",
    "        FILTER_CHOICES.index(a.filters),\n",
    "        BRANCH_CHOICES.index(a.branches),\n",
    "        kernel_idx,\n",
    "        DROPOUT_CHOICES.index(a.dropout),\n",
    "        POOLING_CHOICES.index(a.pooling),\n",
    "    ]\n",
    "\n",
    "\n",
    "def encode_arch_onehot(a: HeadArch) -> list:\n",
    "    v = []\n",
    "    def oh(choices, sel):\n",
    "        z = [0]*len(choices)\n",
    "        z[choices.index(sel)] = 1\n",
    "        return z\n",
    "    v += oh(LAYERS_CHOICES, a.layers)\n",
    "    v += oh(FILTER_CHOICES, a.filters)\n",
    "    v += oh(BRANCH_CHOICES, a.branches)\n",
    "    v += [1 if k in KERNEL_CHOICES and k in a.kernels else 0 for k in KERNEL_CHOICES]\n",
    "    v += oh(DROPOUT_CHOICES, a.dropout)\n",
    "    v += oh(POOLING_CHOICES, a.pooling)\n",
    "    return v\n",
    "\n",
    "def pop_diversity(pop: List[HeadArch]) -> float:\n",
    "    if len(pop) < 2:\n",
    "        return 0.0\n",
    "    enc = [encode_arch_onehot(a) for a in pop]\n",
    "    n = len(enc)\n",
    "    man = 0.0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            man += sum(abs(enc[i][k] - enc[j][k]) for k in range(len(enc[i])))\n",
    "    denom = (n*(n-1)/2) * (sum(enc[0]))\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return 0.25 * (man / denom)\n",
    "\n",
    "def _smoke_test():\n",
    "    \n",
    "    cfg = TASKS[0]\n",
    "    df = load_task_dataframe(cfg).sample(min(8, 32), random_state=SEED)\n",
    "    X = df[cfg['text_col']].tolist()\n",
    "    y = df[cfg['label_col']].astype(int).tolist()\n",
    "    loader = build_loader(X, y, batch=4, shuffle=False)\n",
    "\n",
    "    arch = HeadArch(layers=1, filters=128, branches=2, kernels=(3,5), dropout=0.2, pooling='attn')\n",
    "    model = SentimentModel(Backbone(BACKBONE_NAME), cfg['num_classes'], arch).to(DEVICE)\n",
    "    model.freeze_backbone(True)  \n",
    "\n",
    "    batch = next(iter(loader))\n",
    "    with torch.cuda.amp.autocast(enabled=MIXED_PREC):\n",
    "        logits = model(batch['input_ids'].to(DEVICE), batch['attention_mask'].to(DEVICE))\n",
    "    assert logits.shape[0] == len(batch['labels'])\n",
    "    print(\"Smoke test OK — logits:\", tuple(logits.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22869ea9-e4a7-47cb-9125-b162e8c0a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import random, math, time, gc\n",
    "import numpy as np\n",
    "import torch\n",
    "import math as _math\n",
    "\n",
    "\n",
    "def safe_eval(arch, ts, epochs, freeze):\n",
    "    # Ladder of batch sizes to try during search (override elsewhere if you want)\n",
    "    candidates = globals().get(\"BATCH_SEARCH_CANDIDATES\", [96, 80, 64, 48, 32, 24, 16, 12, 8])\n",
    "\n",
    "    globals().setdefault(\"BATCH_USAGE\", {})      \n",
    "    globals().setdefault(\"BATCH_OOM_COUNT\", {})  \n",
    "\n",
    "    \n",
    "    orig = globals().get(\"BATCH\", 16)\n",
    "    last_err = None\n",
    "\n",
    "    for b in candidates:\n",
    "        try:\n",
    "            globals()[\"BATCH\"] = b\n",
    "            out = evaluate_arch_on_task(arch, ts, epochs=epochs, freeze=freeze)\n",
    "            \n",
    "            globals()[\"BATCH_USAGE\"][b] = globals()[\"BATCH_USAGE\"].get(b, 0) + 1\n",
    "            globals()[\"LAST_SUCCESS_BATCH\"] = b  \n",
    "            globals()[\"BATCH\"] = orig\n",
    "            return out\n",
    "        except RuntimeError as e:\n",
    "            s = str(e)\n",
    "            if (\"CUDA out of memory\" in s) or (\"cudaError\" in s) or (\"cublas\" in s) or (\"cuDNN\" in s):\n",
    "                globals()[\"BATCH_OOM_COUNT\"][b] = globals()[\"BATCH_OOM_COUNT\"].get(b, 0) + 1\n",
    "                last_err = e\n",
    "                try:\n",
    "                    if torch.cuda.is_available():\n",
    "                        torch.cuda.empty_cache()\n",
    "                except Exception:\n",
    "                    pass\n",
    "                \n",
    "                continue\n",
    "            else:\n",
    "                \n",
    "                globals()[\"BATCH\"] = orig\n",
    "                raise\n",
    "    \n",
    "    globals()[\"BATCH\"] = orig\n",
    "    if last_err is not None:\n",
    "        raise last_err\n",
    "    \n",
    "    return evaluate_arch_on_task(arch, ts, epochs=epochs, freeze=freeze)\n",
    "\n",
    "\n",
    "def _onehot(a: 'HeadArch'):\n",
    "    try:\n",
    "        k = tuple(getattr(a, \"kernels\", [getattr(a, \"kernel\", 3)]))\n",
    "        br = getattr(a, \"branches\", 1)\n",
    "        ops = [(\"layers\", getattr(a, \"layers\", 2)),\n",
    "               (\"filters\", getattr(a, \"filters\", 64)),\n",
    "               (\"branches\", br),\n",
    "               (\"kernels\", k),\n",
    "               (\"dropout\", getattr(a, \"dropout\", 0.2)),\n",
    "               (\"pooling\", getattr(a, \"pooling\", \"attn\"))]\n",
    "        s = \"|\".join(f\"{k}={v}\" for k, v in ops)\n",
    "        rng = np.random.RandomState(abs(hash(s)) % (2**32))\n",
    "        vec = rng.rand(64)\n",
    "        idx = np.argpartition(vec, -8)[-8:]\n",
    "        out = np.zeros_like(vec)\n",
    "        out[idx] = 1\n",
    "        return out\n",
    "    except Exception:\n",
    "        return np.zeros(64)\n",
    "\n",
    "def pop_diversity(pop: List['HeadArch']) -> float:\n",
    "    if len(pop) < 2:\n",
    "        return 0.0\n",
    "    E = [_onehot(a) for a in pop]\n",
    "    n = len(E)\n",
    "    acc = 0.0\n",
    "    denom = (n * (n - 1)) / 2\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            acc += np.abs(E[i] - E[j]).sum()\n",
    "    norm = max(np.abs(E[0]).sum(), 1.0)\n",
    "    return 0.25 * (acc / max(denom, 1)) / norm\n",
    "\n",
    "def probi(unit, div_i, d_i):\n",
    "    z = unit * div_i + d_i\n",
    "    return 1.0 / (1.0 + _math.exp(-z))\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TaskState:\n",
    "    name: str\n",
    "    num_classes: int\n",
    "    Xtr: List[str]\n",
    "    ytr: List[int]\n",
    "    Xva: List[str]\n",
    "    yva: List[int]\n",
    "    population: List['HeadArch']\n",
    "    archive_best: List[Tuple['HeadArch', float]]\n",
    "\n",
    "\n",
    "PRERANK_TOPK = None\n",
    "\n",
    "def zero_cost_score(arch: 'HeadArch') -> float:\n",
    "    br = getattr(arch, \"branches\", 1)\n",
    "    kernels = getattr(arch, \"kernels\", [getattr(arch, \"kernel\", 3)])\n",
    "    layers = getattr(arch, \"layers\", 2)\n",
    "    width  = getattr(arch, \"filters\", 64)\n",
    "    variety = len(set(kernels))\n",
    "    param_proxy = layers * width * br\n",
    "    score = 1.0 + 0.3 * (1 if br == 2 else (2 if br == 3 else 0)) + 0.2 * variety - 0.0005 * param_proxy\n",
    "    return float(score)\n",
    "\n",
    "def _ensure_task_runtime_fields(ts):\n",
    "    if not hasattr(ts, 'nsf'): ts.nsf = 0\n",
    "    if not hasattr(ts, 'ntf'): ts.ntf = 0\n",
    "    if not hasattr(ts, 'd'):   ts.d   = 0.0\n",
    "    if not hasattr(ts, 'selected_set'): ts.selected_set = set()\n",
    "\n",
    "def tournament_select(pop, scores, k):\n",
    "    if not pop:\n",
    "        raise RuntimeError(\"Empty population\")\n",
    "    \n",
    "    cand = random.sample(pop, min(k, len(pop)))\n",
    "    return max(cand, key=lambda a: scores.get(a, -1e9))\n",
    "\n",
    "def search_multitask(tasks_cfg: List[Dict[str, Any]]):\n",
    "    \n",
    "    global GENLOG\n",
    "    if 'GENLOG' not in globals():\n",
    "        GENLOG = GenLogger(topk=4, objective_key=\"val_acc\", maximize=True, stop_hint_k=4)\n",
    "\n",
    "   \n",
    "    _snap = globals().get(\"snapshot_skip_stats\", None)\n",
    "    def _snapshot():\n",
    "        return _snap() if callable(_snap) else {}\n",
    "\n",
    "    \n",
    "    warm_winners: Dict[str, 'HeadArch'] = {}\n",
    "    try:\n",
    "        import os, json\n",
    "        _load_fn = globals().get(\"load_winners\", None)\n",
    "        ws_path = globals().get(\"WARMSTART_WINNERS_PATH\", None)\n",
    "        if ws_path and os.path.exists(ws_path):\n",
    "            if _load_fn is not None:\n",
    "                warm_winners = _load_fn(ws_path)\n",
    "            else:\n",
    "                \n",
    "                with open(ws_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    raw = json.load(f)\n",
    "                warm_winners = {\n",
    "                    k: HeadArch(v[\"layers\"], v[\"filters\"], v[\"branches\"],\n",
    "                                tuple(v[\"kernels\"]), v[\"dropout\"], v[\"pooling\"])\n",
    "                    for k, v in raw.items()\n",
    "                }\n",
    "            print(f\"[WarmStart] Loaded winners: {list(warm_winners.keys())}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WarmStart] Load skipped: {e}\")\n",
    "\n",
    "    \n",
    "    task_states: List[TaskState] = []\n",
    "    prep_t0 = time.time()\n",
    "    print(f\"[Init] config: tasks={len(tasks_cfg)}, POP_SIZE={globals().get('POP_SIZE','?')}, \"\n",
    "          f\"LF_EPOCHS={globals().get('LF_EPOCHS','?')}, subset={globals().get('LF_SUBSET_PER_TASK','?')}, \"\n",
    "          f\"freeze={globals().get('FREEZE_BACKBONE','?')}, batch={globals().get('BATCH','?')}\", flush=True)\n",
    "\n",
    "    s0 = _snapshot()  \n",
    "\n",
    "    for cfg in tasks_cfg:\n",
    "        tprep = time.time()\n",
    "        print(f\"[Init] {cfg['name']}: loading & building LF splits...\", flush=True)\n",
    "        df = load_task_dataframe(cfg)\n",
    "        Xtr, ytr, Xva, yva = make_lf_splits(\n",
    "            df, cfg['text_col'], cfg['label_col'], cfg.get('lf_subset', LF_SUBSET_PER_TASK), LF_VAL_FRAC\n",
    "        )\n",
    "        print(f\"[Init] {cfg['name']}: splits ready | Xtr={len(Xtr)} Xva={len(Xva)} (took {time.time()-tprep:.1f}s)\", flush=True)\n",
    "\n",
    "        \n",
    "        pop: List['HeadArch'] = []\n",
    "        used = set()\n",
    "        winner = warm_winners.get(cfg['name'])\n",
    "        if winner is not None:\n",
    "            \n",
    "            pop.append(winner)\n",
    "            used.add(winner.as_tuple())\n",
    "        \n",
    "        while len(pop) < POP_SIZE:\n",
    "            a = HeadArch.sample() if hasattr(HeadArch, \"sample\") else (HeadArch.random() if hasattr(HeadArch, \"random\") else random_arch())\n",
    "            tup = a.as_tuple()\n",
    "            if tup in used:\n",
    "                continue\n",
    "            pop.append(a)\n",
    "            used.add(tup)\n",
    "\n",
    "        ts = TaskState(\n",
    "            name=cfg['name'], num_classes=cfg['num_classes'],\n",
    "            Xtr=Xtr, ytr=ytr, Xva=Xva, yva=yva,\n",
    "            population=pop,\n",
    "            archive_best=[],\n",
    "        )\n",
    "        _ensure_task_runtime_fields(ts)\n",
    "        task_states.append(ts)\n",
    "        print(f\"[Init] {cfg['name']}: pop seeded (warm={'yes' if winner is not None else 'no'}, size={len(pop)})\", flush=True)\n",
    "\n",
    "    print(f\"[Init] prep-phase total {time.time()-prep_t0:.1f}s\")\n",
    "\n",
    "    s1 = _snapshot()  \n",
    "    if s1:\n",
    "        dr = s1.get('random_skips', 0) - s0.get('random_skips', 0)\n",
    "        dm = s1.get('mutate_skips', 0) - s0.get('mutate_skips', 0)\n",
    "        print(f\"[Init] infeasible re-samples during seeding: random={dr}, mutate={dm} \"\n",
    "              f\"(cumulative: random={s1.get('random_skips',0)}, mutate={s1.get('mutate_skips',0)})\")\n",
    "\n",
    "    \n",
    "    TOURN_SIZE = globals().get(\"TOURN_SIZE\", 3)\n",
    "    MUT_PRO = globals().get(\"MUT_PRO\", 0.8)\n",
    "    MUT_AFTER_TRANSFER = globals().get(\"MUT_AFTER_TRANSFER\", 0.9)\n",
    "\n",
    "    UNIT = globals().get(\"UNIT\", 1.1)\n",
    "    ALPHA = globals().get(\"ALPHA\", 0.1)\n",
    "\n",
    "    start_time = time.time()\n",
    "    for gen in range(1, GENERATIONS + 1):\n",
    "\n",
    "        \n",
    "        crossover_pro = 0.30   \n",
    "        lf_epochs = 1\n",
    "        children_per_transfer = 12\n",
    "        topk_reuse = 4\n",
    "\n",
    "        gen_t0 = time.time()\n",
    "\n",
    "        \n",
    "        globals()[\"BATCH_USAGE\"] = {}\n",
    "        globals()[\"BATCH_OOM_COUNT\"] = {}\n",
    "\n",
    "        \n",
    "        counts: Dict[str, int] = {ts.name: 0 for ts in task_states}      \n",
    "        sec:    Dict[str, float] = {ts.name: 0.0 for ts in task_states}  \n",
    "        all_children: List['HeadArch'] = []                               \n",
    "\n",
    "        \n",
    "        prev_best: Dict[str, float] = {}\n",
    "        for ts in task_states:\n",
    "            _ensure_task_runtime_fields(ts)\n",
    "            ts.nsf = 0\n",
    "            ts.ntf = 0\n",
    "            prev_best[ts.name] = max([acc for (a, acc) in ts.archive_best], default=-1e9)\n",
    "\n",
    "        \n",
    "        prev_stats = _snapshot()\n",
    "\n",
    "        \n",
    "        for idx, ts in enumerate(task_states):\n",
    "            block_t0 = time.time()\n",
    "\n",
    "            \n",
    "            scores: Dict['HeadArch', float] = {}\n",
    "            cand_pool = ts.population\n",
    "            if PRERANK_TOPK is not None and PRERANK_TOPK < len(ts.population):\n",
    "                # take top-K by zero-cost proxy\n",
    "                ranked = sorted(ts.population, key=zero_cost_score, reverse=True)\n",
    "                cand_pool = ranked[:PRERANK_TOPK]\n",
    "            for a in cand_pool:\n",
    "                if a not in scores:\n",
    "                    scores[a] = safe_eval(a, ts, epochs=1, freeze=FREEZE_BACKBONE)\n",
    "\n",
    "            \n",
    "            sum_ntf_other_prev = sum(getattr(other, \"ntf\", 0) for j, other in enumerate(task_states) if j != idx)\n",
    "            ts.d = ALPHA * ts.d + (getattr(ts, \"nsf\", 0) - sum_ntf_other_prev)\n",
    "            ts.nsf = 0\n",
    "            ts.ntf = 0\n",
    "\n",
    "            # Diversity & operator\n",
    "            div_i = pop_diversity(ts.population)\n",
    "            p = probi(UNIT, div_i, ts.d)\n",
    "            do_self = (random.random() < p)\n",
    "\n",
    "            # Candidate parents\n",
    "            candidates: List[Tuple[str, Any]] = []\n",
    "            if do_self:\n",
    "                parent = tournament_select(ts.population, scores, TOURN_SIZE)\n",
    "                candidates.append((\"self\", parent))\n",
    "            else:\n",
    "                # Algorithm 2 (Low-Fidelity Knowledge Extraction)\n",
    "                donor_pool = []\n",
    "                for j, other in enumerate(task_states):\n",
    "                    if j == idx:\n",
    "                        continue\n",
    "                    for (arch_j, acc_j) in other.archive_best:\n",
    "                        if arch_j.as_tuple() in ts.selected_set:\n",
    "                            continue\n",
    "                        donor_pool.append((j, arch_j, acc_j))\n",
    "                if not donor_pool:\n",
    "                    parent = tournament_select(ts.population, scores, TOURN_SIZE)\n",
    "                    candidates.append((\"self\", parent))\n",
    "                else:\n",
    "                   \n",
    "                    best_score = -1e9\n",
    "                    best = None\n",
    "                    for (j_src, arch_cand, _acc_src) in donor_pool:\n",
    "                        try:\n",
    "                            score_cand = safe_eval(arch_cand, ts, epochs=lf_epochs, freeze=FREEZE_BACKBONE)\n",
    "                        finally:\n",
    "                            if torch.cuda.is_available():\n",
    "                                torch.cuda.empty_cache()\n",
    "                        if score_cand > best_score:\n",
    "                            best_score = score_cand\n",
    "                            best = (j_src, arch_cand)\n",
    "                    if best is None:\n",
    "                        parent = tournament_select(ts.population, scores, TOURN_SIZE)\n",
    "                        candidates.append((\"self\", parent))\n",
    "                    else:\n",
    "                        donor_task_idx, donor_arch = best\n",
    "                        \n",
    "                        ts.selected_set.add(donor_arch.as_tuple())\n",
    "                        candidates = [(\"transfer\", (donor_task_idx, donor_arch))]\n",
    "\n",
    "            import copy as _copy\n",
    "            extra_children: List[Tuple[float, 'HeadArch']] = []  \n",
    "\n",
    "            if candidates[0][0] == \"self\":\n",
    "                parent = candidates[0][1]\n",
    "                if random.random() < MUT_PRO:\n",
    "                    child = parent.mutate()\n",
    "                    _op = \"mutate\"\n",
    "                else:\n",
    "                    child = _copy.deepcopy(parent)\n",
    "                    _op = \"self\"\n",
    "               \n",
    "                st = time.time()\n",
    "                acc_child = safe_eval(child, ts, epochs=lf_epochs, freeze=FREEZE_BACKBONE)\n",
    "                elapsed = time.time() - st\n",
    "                if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "\n",
    "                \n",
    "                try:\n",
    "                    GENLOG.log_child(\n",
    "                        gen=gen, task=ts.name,\n",
    "                        child_id=arch_hash(child),\n",
    "                        op=_op,\n",
    "                        arch_hash=arch_hash(child),\n",
    "                        head_desc=head_summary(child),\n",
    "                        params_m=0.0,  # keep LF light\n",
    "                        batch_used=int(globals().get(\"LAST_SUCCESS_BATCH\", -1)),\n",
    "                        train_time_s=elapsed,\n",
    "                        val_acc=float(acc_child),\n",
    "                        val_loss=float(\"nan\"),\n",
    "                        lr_head=None, lr_backbone=None,\n",
    "                        feasible=True,\n",
    "                        arch_vec=np.asarray(encode_arch_onehot(child), dtype=float)\n",
    "                    )\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                counts[ts.name] += 1\n",
    "                all_children.append(child)\n",
    "                child_acc = acc_child\n",
    "                kind = \"self\"\n",
    "\n",
    "            else:\n",
    "                donor_task_idx, donor_arch = candidates[0][1]\n",
    "                # Algorithm 3\n",
    "                tmp_children: List[Tuple[float, 'HeadArch']] = []\n",
    "                for _ in range(children_per_transfer):\n",
    "                    c = _copy.deepcopy(donor_arch)\n",
    "                    if random.random() < MUT_AFTER_TRANSFER:\n",
    "                        c = c.mutate()\n",
    "                    st = time.time()\n",
    "                    acc_c = safe_eval(c, ts, epochs=lf_epochs, freeze=FREEZE_BACKBONE)\n",
    "                    elapsed = time.time() - st\n",
    "                    tmp_children.append((acc_c, c))\n",
    "                    \n",
    "                    try:\n",
    "                        GENLOG.log_child(\n",
    "                            gen=gen, task=ts.name,\n",
    "                            child_id=arch_hash(c),\n",
    "                            op=\"xfer\",\n",
    "                            arch_hash=arch_hash(c),\n",
    "                            head_desc=head_summary(c),\n",
    "                            params_m=0.0,\n",
    "                            batch_used=int(globals().get(\"LAST_SUCCESS_BATCH\", -1)),\n",
    "                            train_time_s=elapsed,\n",
    "                            val_acc=float(acc_c),\n",
    "                            val_loss=float(\"nan\"),\n",
    "                            lr_head=None, lr_backbone=None,\n",
    "                            feasible=True,\n",
    "                            arch_vec=np.asarray(encode_arch_onehot(c), dtype=float)\n",
    "                        )\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                    if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "                tmp_children.sort(key=lambda x: x[0], reverse=True)\n",
    "                extra_children = tmp_children[:topk_reuse]\n",
    "                \n",
    "                child_acc_prefetch, child = extra_children[0]\n",
    "                kind = \"transfer\"\n",
    "                \n",
    "                ts.selected_set.add(donor_arch.as_tuple())\n",
    "                \n",
    "                counts[ts.name] += len(tmp_children)\n",
    "                all_children.extend([c for _, c in tmp_children])\n",
    "                \n",
    "                child_acc = safe_eval(child, ts, epochs=lf_epochs, freeze=FREEZE_BACKBONE)\n",
    "\n",
    "            \n",
    "            if extra_children:\n",
    "                best_i = max((scores[a] for a in ts.population), default=-1e9)\n",
    "                for acc_c, c_arch in extra_children:\n",
    "                    if acc_c > best_i:\n",
    "                        ts.archive_best.append((c_arch, acc_c))\n",
    "                        ts.ntf += 1\n",
    "\n",
    "            # Algorithm 1\n",
    "            best_i = max((scores[a] for a in ts.population), default=-1e9)\n",
    "            if child_acc > best_i:\n",
    "                ts.archive_best.append((child, child_acc))\n",
    "                if kind == \"self\":\n",
    "                    ts.nsf += 1\n",
    "                else:\n",
    "                    ts.ntf += 1\n",
    "\n",
    "            \n",
    "            if len(ts.population) > 0:\n",
    "                worst = min(ts.population, key=lambda a: scores.get(a, -1e9))\n",
    "                if child_acc > scores.get(worst, -1e9):\n",
    "                    # replace\n",
    "                    ts.population.remove(worst)\n",
    "                    ts.population.append(child)\n",
    "\n",
    "            sec[ts.name] += time.time() - block_t0\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        gen_elapsed = time.time() - gen_t0\n",
    "        kids_total = sum(counts.values())\n",
    "\n",
    "        \n",
    "        cur_stats = _snapshot()\n",
    "        dr = dm = 0\n",
    "        if cur_stats:\n",
    "            dr = cur_stats.get('random_skips', 0) - (prev_stats.get('random_skips', 0) if prev_stats else 0)\n",
    "            dm = cur_stats.get('mutate_skips', 0) - (prev_stats.get('mutate_skips', 0) if prev_stats else 0)\n",
    "            print(f\"[Gen {gen}] infeasible re-samples: random={dr}, mutate={dm} \"\n",
    "                  f\"(cumulative: random={cur_stats.get('random_skips',0)}, mutate={cur_stats.get('mutate_skips',0)})\")\n",
    "\n",
    "        print(f\"[Gen {gen}] elapsed {gen_elapsed:.1f}s  | children evals={kids_total}  | \"\n",
    "              f\"BATCH_USAGE={globals().get('BATCH_USAGE',{})}  | OOMs={globals().get('BATCH_OOM_COUNT',{})}\")\n",
    "\n",
    "        \n",
    "        try:\n",
    "            GENLOG.end_gen_summary(\n",
    "                gen=gen,\n",
    "                infeasible_counts={'random': dr, 'mutate': dm},\n",
    "                batch_usage=globals().get('BATCH_USAGE', {}),\n",
    "                ooms_dict=globals().get('BATCH_OOM_COUNT', {})\n",
    "                \n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    \n",
    "    results = {}\n",
    "    for ts in task_states:\n",
    "        if ts.archive_best:\n",
    "            best_arch, best_acc = max(ts.archive_best, key=lambda t: t[1])\n",
    "        else:\n",
    "            \n",
    "            scored = [(safe_eval(a, ts, epochs=1, freeze=FREEZE_BACKBONE), a) for a in ts.population]\n",
    "            best_acc, best_arch = max(scored, key=lambda t: t[0])\n",
    "        print(f\"[Result] Task {ts.name}: best arch = {best_arch} | acc={best_acc:.4f}\")\n",
    "        results[ts.name] = best_arch\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f559d-3411-4137-a5e5-edc645c7ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_train_task(cfg: Dict[str, Any], head_arch: HeadArch, epochs=FINAL_EPOCHS, lr=FINAL_LR):\n",
    "    df = load_task_dataframe(cfg)\n",
    "\n",
    "    \n",
    "    train_df, val_df = train_test_split(\n",
    "        df, test_size=0.1, stratify=df[cfg['label_col']], random_state=SEED\n",
    "    )\n",
    "\n",
    "    \n",
    "    frac = cfg.get('train_frac', None)\n",
    "    if frac is not None and 0 < frac < 1.0:\n",
    "        sss = StratifiedShuffleSplit(n_splits=1, train_size=frac, random_state=SEED)\n",
    "        idx_keep, _ = next(sss.split(train_df, train_df[cfg['label_col']]))\n",
    "        original_n = len(train_df)\n",
    "        train_df = train_df.iloc[idx_keep].reset_index(drop=True)\n",
    "        print(f\"[{cfg['name']}] Subsampled TRAIN to {len(train_df)} of {original_n} ({frac*100:.1f}%).\")\n",
    "\n",
    "    Xtr = train_df[cfg['text_col']].astype(str).tolist()\n",
    "    ytr = train_df[cfg['label_col']].astype(int).tolist()\n",
    "    Xva = val_df[cfg['text_col']].astype(str).tolist()\n",
    "    yva = val_df[cfg['label_col']].astype(int).tolist()\n",
    "\n",
    "    train_loader = build_loader(Xtr, ytr, shuffle=True)\n",
    "    val_loader   = build_loader(Xva, yva)\n",
    "\n",
    "    model = SentimentModel(Backbone(BACKBONE_NAME), cfg['num_classes'], head_arch)\n",
    "    \n",
    "    model.freeze_backbone(False if not FREEZE_BACKBONE else True)\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=MIXED_PREC)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_acc, best_path = 0.0, f\"best_{cfg['name']}.pt\"\n",
    "    for ep in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        pbar = tqdm(train_loader, desc=f\"Train {cfg['name']} ep{ep}\")\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(DEVICE)\n",
    "            attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "            labels = batch['labels'].to(DEVICE)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=MIXED_PREC):\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            pbar.set_postfix({'loss': float(loss)})\n",
    "\n",
    "        \n",
    "        @torch.no_grad()\n",
    "        def _val_step():\n",
    "            return quick_val(model, val_loader)\n",
    "        res = _val_step()\n",
    "        scheduler.step(res.acc)\n",
    "        print(f\"[Val] ep{ep} acc={res.acc:.4f} f1={res.f1:.4f}\")\n",
    "        if res.acc > best_acc:\n",
    "            best_acc = res.acc\n",
    "            torch.save({'arch': asdict(head_arch),\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'backbone': BACKBONE_NAME},\n",
    "                       best_path)\n",
    "            print(f\"Saved: {best_path} (acc={best_acc:.4f})\")\n",
    "    return best_acc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb85394-22e4-49b5-9ff2-15266808a9d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Training loop\n",
    "BATCH_SEARCH_CANDIDATES = [32, 24, 16, 12, 8]\n",
    "\n",
    "\n",
    "#Warm start\n",
    "WARMSTART_WINNERS_PATH = \"winners_last.json\"\n",
    "WARMSTART_VARIANTS = 8\n",
    "WARMSTART_KEEP_WINNER = True\n",
    "\n",
    "\n",
    "FREEZE_BACKBONE = True\n",
    "\n",
    "print(\"Starting search...\")\n",
    "best_arches = search_multitask(TASKS)\n",
    "\n",
    "print(\"\\nBest heads found per task:\")\n",
    "for name, arch in best_arches.items():\n",
    "    print(f\"{name}: {arch}\")\n",
    "\n",
    "\n",
    "FREEZE_BACKBONE = False\n",
    "\n",
    "\n",
    "PER_TASK_EPOCHS = {\n",
    "    'twitter2': 4,         \n",
    "    'yt_taskA': 10,        \n",
    "    'yt_taskB': 25,        \n",
    "    'twitter_research': 25 \n",
    "}\n",
    "\n",
    "print(\"\\nStarting short full training per task...\")\n",
    "for cfg in TASKS:\n",
    "    ep = PER_TASK_EPOCHS.get(cfg['name'], 3)\n",
    "    acc = full_train_task(cfg, best_arches[cfg['name']], epochs=ep, lr=2e-5)\n",
    "    print(f\"{cfg['name']} final acc ({ep} ep): {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691ed260-7bb2-4fab-84de-26b16d3e8787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f8082f-fa48-4108-9f4a-da573d39ab35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
