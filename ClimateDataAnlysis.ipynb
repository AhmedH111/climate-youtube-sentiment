{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea69b0f0-dcd5-4294-a1df-d1a1006ac3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "DATA_DIR = Path(r\"C:\\Users\\Admin\\Downloads\\Papers we care about\\yt_climate_data\\excel\")\n",
    "\n",
    "files = sorted(DATA_DIR.glob(\"climate_comments_*_all_clean_labeled.xlsx\"))\n",
    "\n",
    "print(\"Found\", len(files), \"labeled files:\")\n",
    "for f in files:\n",
    "    print(\" -\", f.name)\n",
    "\n",
    "all_dfs = []\n",
    "for f in files:\n",
    "    \n",
    "    year = None\n",
    "    for token in f.stem.split(\"_\"):\n",
    "        if token.isdigit() and len(token) == 4:\n",
    "            year = int(token)\n",
    "            break\n",
    "\n",
    "    df_year = pd.read_excel(f)\n",
    "\n",
    "    \n",
    "    df_year[\"year\"] = year\n",
    "\n",
    "    all_dfs.append(df_year)\n",
    "\n",
    "full_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(\"Combined shape:\", full_df.shape)\n",
    "\n",
    "\n",
    "print(full_df.columns.tolist())\n",
    "full_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4ff88-5a9b-4be8-9fb2-df834c9b9e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "full_df[\"sentiment_label\"] = full_df[\"sentiment_label\"].str.lower().str.strip()\n",
    "\n",
    "\n",
    "full_df = full_df[full_df[\"comment_text\"].notna()].copy()\n",
    "\n",
    "\n",
    "full_df = full_df[full_df[\"region_group\"].notna()].copy()\n",
    "\n",
    "full_df[\"region_group\"].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c293b72-31d8-4f4d-94c0-4f4584b556c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_THRESHOLD = 0.75\n",
    "\n",
    "hc_df = full_df[full_df[\"sentiment_confidence\"] >= CONF_THRESHOLD].copy()\n",
    "\n",
    "print(\"All rows:\", len(full_df))\n",
    "print(\"High-confidence rows:\", len(hc_df))\n",
    "print(\"Retention %:\",\n",
    "      round(len(hc_df) / max(len(full_df),1) * 100, 2), \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f64a1b-89c6-467b-92fb-14c7b63aebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_breakdown(df):\n",
    "    \n",
    "    counts = (\n",
    "        df\n",
    "        .groupby([\"region_group\", \"sentiment_label\"])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    totals = (\n",
    "        counts.groupby(\"region_group\")[\"count\"]\n",
    "        .sum()\n",
    "        .reset_index(name=\"region_total\")\n",
    "    )\n",
    "\n",
    "    \n",
    "    counts = counts.merge(totals, on=\"region_group\", how=\"left\")\n",
    "\n",
    "    \n",
    "    counts[\"pct_within_region\"] = counts[\"count\"] / counts[\"region_total\"] * 100.0\n",
    "\n",
    "    \n",
    "    sentiment_order = [\"negative\", \"neutral\", \"positive\"]\n",
    "    counts[\"sentiment_label\"] = pd.Categorical(\n",
    "        counts[\"sentiment_label\"].str.lower().str.strip(),\n",
    "        categories=sentiment_order,\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    counts = counts.sort_values([\"region_group\", \"sentiment_label\"]).reset_index(drop=True)\n",
    "\n",
    "    return counts[[\n",
    "        \"region_group\",\n",
    "        \"sentiment_label\",\n",
    "        \"count\",\n",
    "        \"pct_within_region\"\n",
    "    ]]\n",
    "\n",
    "sent_overall_full = sentiment_breakdown(full_df)\n",
    "sent_overall_hc   = sentiment_breakdown(hc_df)\n",
    "\n",
    "print(\"=== Overall sentiment distribution (all rows) ===\")\n",
    "display(sent_overall_full)\n",
    "\n",
    "print(\"=== Overall sentiment distribution (high-confidence only) ===\")\n",
    "display(sent_overall_hc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ca9c7-317d-4f65-9e85-dff0bb150542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2d080b-0c3e-4e5a-b8ab-5acbd11fbb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearly_neg_trend(df):\n",
    "    tmp = df.copy()\n",
    "    tmp[\"is_negative\"] = (tmp[\"sentiment_label\"] == \"negative\").astype(int)\n",
    "\n",
    "    trend = (\n",
    "        tmp\n",
    "        .groupby([\"year\", \"region_group\"])[\"is_negative\"]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={\"is_negative\": \"neg_rate\"})\n",
    "    )\n",
    "\n",
    "    \n",
    "    trend[\"neg_rate_pct\"] = trend[\"neg_rate\"] * 100.0\n",
    "\n",
    "    return trend.sort_values([\"region_group\", \"year\"])\n",
    "\n",
    "neg_trend_full = yearly_neg_trend(full_df)\n",
    "neg_trend_hc   = yearly_neg_trend(hc_df)\n",
    "\n",
    "print(\"=== Negativity over time (all rows) ===\")\n",
    "display(neg_trend_full.head(30))\n",
    "\n",
    "print(\"=== Negativity over time (high-confidence) ===\")\n",
    "display(neg_trend_hc.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0221a-19b3-4bff-b2ad-11eb3e394de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_yearly_total(df):\n",
    "    \"\"\"Return yearly total negativity rate (%), computed over all comments (not averaging region means).\"\"\"\n",
    "    tmp = df.copy()\n",
    "    tmp[\"is_negative\"] = (tmp[\"sentiment_label\"] == \"negative\").astype(int)\n",
    "    total = (\n",
    "        tmp.groupby(\"year\")[\"is_negative\"]\n",
    "           .mean()\n",
    "           .reset_index()\n",
    "           .rename(columns={\"is_negative\": \"Total\"})\n",
    "    )\n",
    "    total[\"Total\"] = total[\"Total\"] * 100.0\n",
    "    return total\n",
    "\n",
    "def to_pivot_with_total(trend_df, base_df):\n",
    "    \"\"\"\n",
    "    Convert your yearly_neg_trend() output (year, region_group, neg_rate_pct)\n",
    "    into a pivot with columns ['EU','US','Total'] where 'Total' is computed from base_df.\n",
    "    \"\"\"\n",
    "    pivot = (trend_df\n",
    "             .pivot(index=\"year\", columns=\"region_group\", values=\"neg_rate_pct\")\n",
    "             .rename_axis(None, axis=1)\n",
    "             .reset_index())\n",
    "    total = compute_yearly_total(base_df)\n",
    "    pivot = pivot.merge(total, on=\"year\", how=\"left\")\n",
    "    \n",
    "    cols = [\"year\"] + [c for c in [\"US\", \"EU\"] if c in pivot.columns] + [\"Total\"]\n",
    "    pivot = pivot[cols]\n",
    "    return pivot\n",
    "\n",
    "def plot_neg_trends(pivot, title=\"Negativity over time (% negative comments)\"):\n",
    "    plt.figure(figsize=(10.5, 5.5))\n",
    "    \n",
    "    for col in [\"US\", \"EU\", \"Total\"]:\n",
    "        if col in pivot.columns:\n",
    "            plt.plot(pivot[\"year\"], pivot[col], marker=\"o\", linewidth=2, label=col)\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"% Negative\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pivot_full = to_pivot_with_total(neg_trend_full, full_df)\n",
    "plot_neg_trends(pivot_full, title=\"Negativity over time\")\n",
    "\n",
    "\n",
    "pivot_hc = to_pivot_with_total(neg_trend_hc, hc_df)\n",
    "plot_neg_trends(pivot_hc, title=\"Negativity over time — High-confidence (75%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb9e88-1a02-4ec8-95cf-039b0f6878e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def yearly_counts(df):\n",
    "    return (df.groupby(\"year\").size()\n",
    "              .reset_index(name=\"count\")\n",
    "              .sort_values(\"year\"))\n",
    "\n",
    "def yearly_counts_by_region(df):\n",
    "    return (df.groupby([\"year\", \"region_group\"]).size()\n",
    "              .reset_index(name=\"count\")\n",
    "              .pivot(index=\"year\", columns=\"region_group\", values=\"count\")\n",
    "              .reset_index()\n",
    "              .sort_values(\"year\"))\n",
    "\n",
    "\n",
    "counts_total = yearly_counts(full_df)\n",
    "\n",
    "plt.figure(figsize=(10.5, 4.8))\n",
    "plt.bar(counts_total[\"year\"], counts_total[\"count\"])\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Total comments\")\n",
    "plt.title(\"Comment activity over time — total per year\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "counts_region = yearly_counts_by_region(full_df)\n",
    "\n",
    "plt.figure(figsize=(10.5, 4.8))\n",
    "for col in [\"US\", \"EU\"]:\n",
    "    if col in counts_region.columns:\n",
    "        plt.plot(counts_region[\"year\"], counts_region[col], marker=\"o\", linewidth=2, label=col)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Comments per year\")\n",
    "plt.title(\"Comment activity over time — US vs EU\")\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.35)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc05cc2-c8ef-4958-a6bb-64ccb49f075e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "FORCE_US_CHANNELS = {\"pbs newshour\"}  \n",
    "\n",
    "def _pick_channel_col(df):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for key in [\"channel_name\", \"channel_title\", \"channel\"]:\n",
    "        if key in cols:\n",
    "            return cols[key]\n",
    "    raise KeyError(\"Expected a channel column: channel_name / channel_title / channel\")\n",
    "\n",
    "def _wilson_ci(neg, N, z=1.96):\n",
    "    neg = pd.Series(neg, dtype=float)\n",
    "    N = pd.Series(N, dtype=float)\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        phat = np.where(N > 0, neg / N, np.nan)\n",
    "        denom = 1 + (z**2) / N\n",
    "        centre = (phat + (z**2)/(2*N)) / denom\n",
    "        hw = z * np.sqrt((phat*(1-phat)/N) + (z**2)/(4*N**2)) / denom\n",
    "    low = np.clip(centre - hw, 0, 1); high = np.clip(centre + hw, 0, 1)\n",
    "    return centre, low, high\n",
    "\n",
    "def _canon_text(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str)\n",
    "    try:\n",
    "        s = s.str.normalize(\"NFKC\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = (s.str.replace(\"\\u200b\", \"\", regex=False)   \n",
    "           .str.replace(\"\\ufeff\", \"\", regex=False)  \n",
    "           .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "           .str.strip())\n",
    "    return s\n",
    "\n",
    "def _canon_region(s: pd.Series) -> pd.Series:\n",
    "    s = _canon_text(s).str.upper()\n",
    "    region_map = {\n",
    "        \"USA\": \"US\", \"U.S.\": \"US\", \"UNITED STATES\": \"US\", \"US \": \"US\",\n",
    "        \"EUROPE\": \"EU\", \"EUROPEAN UNION\": \"EU\", \"E.U.\": \"EU\", \"EU \": \"EU\",\n",
    "    }\n",
    "    s = s.replace(region_map)\n",
    "    s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n",
    "    return s\n",
    "\n",
    "def _ensure_channel_stats_full():\n",
    "    if \"channel_stats_full\" in globals():\n",
    "        return channel_stats_full, None\n",
    "    if \"full_df\" not in globals():\n",
    "        raise RuntimeError(\"I couldn't find `channel_stats_full` or `full_df` in memory.\")\n",
    "\n",
    "    CH = _pick_channel_col(full_df)\n",
    "    tmp = full_df.copy()\n",
    "\n",
    "    \n",
    "    tmp[CH] = _canon_text(tmp[CH])\n",
    "    tmp[\"region_group\"] = _canon_region(tmp[\"region_group\"])\n",
    "\n",
    "\n",
    "    key = tmp[CH].str.lower()\n",
    "    canonical_name = {}\n",
    "    for k, g in tmp.groupby(key):\n",
    "        canonical_name[k] = Counter(g[CH].tolist()).most_common(1)[0][0]\n",
    "    tmp[\"channel_name\"] = key.map(canonical_name)\n",
    "\n",
    "\n",
    "    def _mode_region(series):\n",
    "        c = Counter(series.dropna())\n",
    "        if not c:\n",
    "            return np.nan\n",
    "        mc = c.most_common()\n",
    "        top = mc[0][1]\n",
    "        tied = [r for r, n in mc if n == top]\n",
    "        return \"US\" if \"US\" in tied else tied[0]\n",
    "\n",
    "    region_mode = (\n",
    "        tmp.groupby(\"channel_name\")[\"region_group\"]\n",
    "           .agg(_mode_region).rename(\"channel_region\").reset_index()\n",
    "    )\n",
    "    tmp = tmp.merge(region_mode, on=\"channel_name\", how=\"left\")\n",
    "\n",
    "\n",
    "    mask_force = tmp[\"channel_name\"].str.casefold().isin(FORCE_US_CHANNELS)\n",
    "    tmp.loc[mask_force, \"channel_region\"] = \"US\"\n",
    "\n",
    "\n",
    "    lab = tmp[\"sentiment_label\"].astype(str).str.lower().str.strip()\n",
    "    num = pd.to_numeric(tmp[\"sentiment_label\"], errors=\"coerce\")\n",
    "    tmp[\"is_negative\"] = np.where(lab.str.startswith(\"neg\") | (num == 0), 1, 0)\n",
    "\n",
    "\n",
    "    g = (tmp.groupby([\"channel_name\", \"channel_region\"], dropna=False)[\"is_negative\"]\n",
    "           .agg(neg=\"sum\", N=\"count\").reset_index()\n",
    "           .rename(columns={\"channel_region\": \"region_group\"}))\n",
    "\n",
    "\n",
    "    centre, low, high = _wilson_ci(g[\"neg\"], g[\"N\"])\n",
    "    g[\"neg_share\"] = g[\"neg\"] / g[\"N\"]\n",
    "    g[\"neg_low\"] = low\n",
    "    g[\"neg_high\"] = high\n",
    "\n",
    "\n",
    "    g = g.sort_values([\"region_group\", \"channel_name\"]).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    us = g[g[\"region_group\"] == \"US\"][\"channel_name\"].unique().tolist()\n",
    "    eu = g[g[\"region_group\"] == \"EU\"][\"channel_name\"].unique().tolist()\n",
    "    print(f\"[Diagnostic] US channels ({len(us)}): {sorted(us)}\")\n",
    "    print(f\"[Diagnostic] EU channels ({len(eu)}): {sorted(eu)}\")\n",
    "    if \"PBS NewsHour\" not in us:\n",
    "\n",
    "        present = tmp.loc[tmp[\"channel_name\"].str.casefold() == \"pbs newshour\", [\"channel_name\",\"region_group\",\"channel_region\"]]\n",
    "        print(\"\\n[Diagnostic] Rows seen for PBS NewsHour after cleaning:\")\n",
    "        print(present.drop_duplicates())\n",
    "\n",
    "    return g, \"built\"\n",
    "\n",
    "channel_stats_full, _built = _ensure_channel_stats_full()\n",
    "\n",
    "def plot_negativity_bars(stats_df, region, top_n=None, sort_by=\"neg_share\", save=True):\n",
    "    sub = stats_df[stats_df[\"region_group\"] == region].copy()\n",
    "    if sub.empty:\n",
    "        print(f\"[{region}] No channels found.\"); return\n",
    "    sub = sub.sort_values(sort_by, ascending=False)\n",
    "    if top_n is not None:\n",
    "        sub = sub.head(top_n)\n",
    "    sub = sub.iloc[::-1]\n",
    "\n",
    "    y = np.arange(len(sub))\n",
    "    x = sub[\"neg_share\"].values\n",
    "    xerr = np.vstack([x - sub[\"neg_low\"].values, sub[\"neg_high\"].values - x])\n",
    "\n",
    "    fig_h = 0.48 * len(sub) + 1.8\n",
    "    plt.figure(figsize=(10.5, fig_h))\n",
    "    plt.barh(y, x, xerr=xerr, capsize=3, alpha=0.85)\n",
    "    plt.yticks(y, sub[\"channel_name\"])\n",
    "    plt.xlabel(\"Negativity share\")\n",
    "    plt.title(f\"Channel negativity (%) — {region}\")\n",
    "    plt.xlim(0, 1)\n",
    "\n",
    "    for i, (val, n) in enumerate(zip(x, sub[\"N\"].values)):\n",
    "        pct = f\"{val*100:.1f}%\"\n",
    "        label = f\"{pct}  (N={int(n):,})\"\n",
    "        xpos = max(val, 0.005)\n",
    "        plt.text(xpos + 0.01, i, label, va=\"center\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        fname = f\"ch5_533_negativity_bars_{region}.png\"\n",
    "        plt.savefig(fname, dpi=200, bbox_inches=\"tight\")\n",
    "        print(f\"Saved: {fname}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_negativity_bars(channel_stats_full, \"EU\", top_n=None)\n",
    "plot_negativity_bars(channel_stats_full, \"US\", top_n=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bec7e7-d2cd-43e4-a7ac-2bfd9b40526d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb4cf7f-5220-472c-bb77-e64b683ac423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
